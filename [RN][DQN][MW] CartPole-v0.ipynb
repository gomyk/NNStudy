{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMbDDDoxhrdTiFIXNnYPAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRN%5D%5BDQN%5D%5BMW%5D%20CartPole-v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "be57d659-3c4f-4908-b00a-85a42704b340"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(1400,900),)\n",
        "display.start()\n",
        "\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwNNaGeeyMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "7d872497-42e8-4a8f-cf4d-fab47cc8f13f"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.9\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.01\n",
        "MEMORY_SIZE = 50000\n",
        "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
        "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
        "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
        "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
        "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
        "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber 손실 계산\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f3b3ba4b-06fe-41cf-c3f3-ea1166f8ee59"
      },
      "source": [
        "num_episodes = 500\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # 새로운 상태 관찰\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        state = next_state\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            output.clear()\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}