{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0YNMEOSi1KUkhmyPSJi3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20LunarLanderContinuous-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "3d6edaef-1505-4b56-e66c-347838b8f2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 66%\r\rReading package lists... 66%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 88%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"LunarLanderContinuous-v2\")\n",
        "#env._max_episode_steps = 1600\n",
        "#env.seed(10)\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_512 = 512\n",
        "HIDDEN_256 = 256\n",
        "HIDDEN_128 = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, outputs)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear2.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = F.relu(self.bn256(self.linear2(x)))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.linear3 = nn.Linear(action_size, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, 1)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "        self.bn128 = nn.BatchNorm1d(HIDDEN_128)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear3.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        #x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        y = self.linear3(action)\n",
        "    \n",
        "        x = F.relu(torch.add(x, y))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "8cc9c42e-c96a-4ff2-e09d-97358e35fb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwW1mlhkHt5lZZv4fUG7PTk1WpgsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.90\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 2000\n",
        "WEIGHT_DECAY = 0.001\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.0001\n",
        "CRITIC_LR = 0.0004\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 1000\n",
        "TAU = 0.001\n",
        "ou_noise_theta = 0.4\n",
        "ou_noise_sigma = 0.2\n",
        "\n",
        "RECORD_INTERVAL = 100\n",
        "\n",
        "seed = random.seed(0)\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = env.action_space.shape[0]\n",
        "n_obvs = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "#actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "#actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "#critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "#critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR, weight_decay=WEIGHT_DECAY)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = np.random.uniform(-1,1,n_actions)\n",
        "    else:\n",
        "        actor.eval()\n",
        "        with torch.no_grad():\n",
        "          selected_action = actor(\n",
        "              state.to(device)\n",
        "           )[0].detach().cpu().numpy()\n",
        "          \n",
        "    _noise = noise.sample()\n",
        "    actor.train()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvU9RgzCYsW",
        "colab_type": "text"
      },
      "source": [
        "##Normalizer for obv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0, 0\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + (GAMMA * next_value * masks)\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.mse_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    \n",
        "    # train actor\n",
        "\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "39bafaef-ab5c-4cc3-96cd-14468c1bcaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "batch_count = 0\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        \n",
        "        \n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(total_reward)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Total reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward))\n",
        "            #print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    #if i_episode % TARGET_UPDATE == 0:\n",
        "    #   actor_target.load_state_dict(actor.state_dict())\n",
        "    #   critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 131 step , 0.00 Actor Loss, 0.79 Critic Loss,  0.85 Threshold , 15.15 Top reward, -235.92 Total reward\n",
            "1 episode , 102 step , 1.70 Actor Loss, 43.21 Critic Loss,  0.81 Threshold , 6.77 Top reward, -317.53 Total reward\n",
            "2 episode , 92 step , 2.59 Actor Loss, 23.55 Critic Loss,  0.77 Threshold , 2.51 Top reward, -339.01 Total reward\n",
            "3 episode , 110 step , 2.85 Actor Loss, 10.42 Critic Loss,  0.73 Threshold , 1.88 Top reward, -568.41 Total reward\n",
            "4 episode , 85 step , 3.21 Actor Loss, 24.32 Critic Loss,  0.71 Threshold , 1.50 Top reward, -332.38 Total reward\n",
            "5 episode , 95 step , 3.82 Actor Loss, 31.46 Critic Loss,  0.67 Threshold , 0.23 Top reward, -655.40 Total reward\n",
            "6 episode , 65 step , 4.22 Actor Loss, 37.50 Critic Loss,  0.66 Threshold , 18.67 Top reward, -211.60 Total reward\n",
            "7 episode , 57 step , 4.46 Actor Loss, 33.32 Critic Loss,  0.64 Threshold , -0.10 Top reward, -342.69 Total reward\n",
            "8 episode , 63 step , 4.59 Actor Loss, 38.47 Critic Loss,  0.62 Threshold , 2.81 Top reward, -309.00 Total reward\n",
            "9 episode , 81 step , 4.86 Actor Loss, 43.05 Critic Loss,  0.60 Threshold , 0.68 Top reward, -486.69 Total reward\n",
            "10 episode , 67 step , 5.06 Actor Loss, 41.54 Critic Loss,  0.58 Threshold , 24.65 Top reward, -211.25 Total reward\n",
            "11 episode , 80 step , 5.36 Actor Loss, 41.64 Critic Loss,  0.56 Threshold , 0.85 Top reward, -538.99 Total reward\n",
            "12 episode , 86 step , 5.58 Actor Loss, 40.33 Critic Loss,  0.54 Threshold , 2.79 Top reward, -329.73 Total reward\n",
            "13 episode , 73 step , 5.71 Actor Loss, 46.15 Critic Loss,  0.52 Threshold , 2.17 Top reward, -437.58 Total reward\n",
            "14 episode , 73 step , 6.12 Actor Loss, 43.54 Critic Loss,  0.50 Threshold , 1.40 Top reward, -448.05 Total reward\n",
            "15 episode , 87 step , 6.50 Actor Loss, 41.79 Critic Loss,  0.48 Threshold , 112.36 Top reward, -552.56 Total reward\n",
            "16 episode , 85 step , 6.71 Actor Loss, 49.77 Critic Loss,  0.47 Threshold , 1.26 Top reward, -423.42 Total reward\n",
            "17 episode , 102 step , 6.99 Actor Loss, 42.67 Critic Loss,  0.44 Threshold , -0.17 Top reward, -439.79 Total reward\n",
            "18 episode , 114 step , 7.09 Actor Loss, 41.58 Critic Loss,  0.42 Threshold , 2.53 Top reward, -303.98 Total reward\n",
            "19 episode , 93 step , 7.31 Actor Loss, 33.51 Critic Loss,  0.41 Threshold , 3.44 Top reward, -299.44 Total reward\n",
            "20 episode , 131 step , 7.32 Actor Loss, 34.67 Critic Loss,  0.38 Threshold , 2.27 Top reward, -300.53 Total reward\n",
            "21 episode , 77 step , 7.42 Actor Loss, 31.22 Critic Loss,  0.37 Threshold , 4.64 Top reward, -126.92 Total reward\n",
            "22 episode , 158 step , 7.46 Actor Loss, 29.10 Critic Loss,  0.35 Threshold , 2.18 Top reward, -509.43 Total reward\n",
            "23 episode , 202 step , 7.78 Actor Loss, 24.60 Critic Loss,  0.32 Threshold , 2.64 Top reward, -427.23 Total reward\n",
            "24 episode , 257 step , 8.06 Actor Loss, 25.86 Critic Loss,  0.29 Threshold , 2.32 Top reward, -617.19 Total reward\n",
            "25 episode , 131 step , 8.43 Actor Loss, 28.14 Critic Loss,  0.27 Threshold , 100.96 Top reward, -298.18 Total reward\n",
            "26 episode , 110 step , 8.73 Actor Loss, 27.60 Critic Loss,  0.26 Threshold , 4.47 Top reward, -339.99 Total reward\n",
            "27 episode , 115 step , 8.84 Actor Loss, 24.92 Critic Loss,  0.25 Threshold , 3.41 Top reward, -362.64 Total reward\n",
            "28 episode , 218 step , 9.15 Actor Loss, 21.17 Critic Loss,  0.23 Threshold , 2.59 Top reward, -267.94 Total reward\n",
            "29 episode , 299 step , 9.45 Actor Loss, 23.67 Critic Loss,  0.20 Threshold , 3.20 Top reward, -444.85 Total reward\n",
            "30 episode , 333 step , 9.80 Actor Loss, 21.70 Critic Loss,  0.18 Threshold , 4.71 Top reward, -281.08 Total reward\n",
            "31 episode , 410 step , 10.05 Actor Loss, 17.62 Critic Loss,  0.16 Threshold , 3.41 Top reward, -510.92 Total reward\n",
            "32 episode , 456 step , 10.44 Actor Loss, 12.11 Critic Loss,  0.13 Threshold , 2.92 Top reward, -242.95 Total reward\n",
            "33 episode , 329 step , 10.72 Actor Loss, 14.26 Critic Loss,  0.12 Threshold , 4.14 Top reward, -192.08 Total reward\n",
            "34 episode , 263 step , 10.73 Actor Loss, 13.34 Critic Loss,  0.11 Threshold , 3.47 Top reward, -176.09 Total reward\n",
            "35 episode , 427 step , 10.83 Actor Loss, 11.06 Critic Loss,  0.10 Threshold , 2.84 Top reward, -170.56 Total reward\n",
            "36 episode , 306 step , 10.86 Actor Loss, 13.12 Critic Loss,  0.09 Threshold , 4.03 Top reward, -264.58 Total reward\n",
            "37 episode , 344 step , 11.07 Actor Loss, 13.29 Critic Loss,  0.09 Threshold , 3.31 Top reward, -180.49 Total reward\n",
            "38 episode , 387 step , 11.08 Actor Loss, 11.12 Critic Loss,  0.08 Threshold , 3.47 Top reward, -152.93 Total reward\n",
            "39 episode , 405 step , 11.23 Actor Loss, 10.72 Critic Loss,  0.07 Threshold , 2.07 Top reward, -238.96 Total reward\n",
            "40 episode , 446 step , 11.33 Actor Loss, 12.00 Critic Loss,  0.07 Threshold , 2.74 Top reward, -200.67 Total reward\n",
            "41 episode , 542 step , 11.26 Actor Loss, 10.41 Critic Loss,  0.06 Threshold , 2.21 Top reward, -288.25 Total reward\n",
            "42 episode , 654 step , 11.34 Actor Loss, 9.66 Critic Loss,  0.06 Threshold , 2.09 Top reward, -256.71 Total reward\n",
            "43 episode , 400 step , 11.30 Actor Loss, 10.64 Critic Loss,  0.06 Threshold , 3.52 Top reward, -255.02 Total reward\n",
            "44 episode , 442 step , 11.24 Actor Loss, 10.36 Critic Loss,  0.06 Threshold , 4.06 Top reward, -173.14 Total reward\n",
            "45 episode , 704 step , 11.14 Actor Loss, 10.46 Critic Loss,  0.05 Threshold , 2.82 Top reward, -265.75 Total reward\n",
            "46 episode , 307 step , 10.82 Actor Loss, 11.56 Critic Loss,  0.05 Threshold , 9.24 Top reward, -82.86 Total reward\n",
            "47 episode , 1000 step , 10.04 Actor Loss, 11.46 Critic Loss,  0.05 Threshold , 3.03 Top reward, -189.97 Total reward\n",
            "48 episode , 280 step , 9.09 Actor Loss, 8.94 Critic Loss,  0.05 Threshold , 27.62 Top reward, -113.90 Total reward\n",
            "49 episode , 281 step , 8.83 Actor Loss, 9.00 Critic Loss,  0.05 Threshold , 42.31 Top reward, -115.44 Total reward\n",
            "50 episode , 429 step , 8.75 Actor Loss, 6.95 Critic Loss,  0.05 Threshold , 13.82 Top reward, -138.55 Total reward\n",
            "51 episode , 579 step , 8.37 Actor Loss, 8.66 Critic Loss,  0.05 Threshold , 5.95 Top reward, -372.13 Total reward\n",
            "52 episode , 469 step , 8.25 Actor Loss, 7.57 Critic Loss,  0.05 Threshold , 35.24 Top reward, -215.32 Total reward\n",
            "53 episode , 690 step , 8.00 Actor Loss, 8.09 Critic Loss,  0.05 Threshold , 3.91 Top reward, -212.66 Total reward\n",
            "54 episode , 1000 step , 7.31 Actor Loss, 6.48 Critic Loss,  0.05 Threshold , 3.00 Top reward, -154.09 Total reward\n",
            "55 episode , 1000 step , 6.59 Actor Loss, 5.73 Critic Loss,  0.05 Threshold , 4.58 Top reward, -94.63 Total reward\n",
            "56 episode , 645 step , 5.90 Actor Loss, 4.89 Critic Loss,  0.05 Threshold , 4.22 Top reward, -220.14 Total reward\n",
            "57 episode , 1000 step , 5.46 Actor Loss, 4.51 Critic Loss,  0.05 Threshold , 3.61 Top reward, -144.39 Total reward\n",
            "58 episode , 1000 step , 5.02 Actor Loss, 4.60 Critic Loss,  0.05 Threshold , 3.54 Top reward, -93.04 Total reward\n",
            "59 episode , 199 step , 4.70 Actor Loss, 4.27 Critic Loss,  0.05 Threshold , 3.99 Top reward, -132.71 Total reward\n",
            "60 episode , 1000 step , 4.23 Actor Loss, 4.65 Critic Loss,  0.05 Threshold , 2.99 Top reward, -126.40 Total reward\n",
            "61 episode , 1000 step , 3.81 Actor Loss, 3.99 Critic Loss,  0.05 Threshold , 4.04 Top reward, -143.45 Total reward\n",
            "62 episode , 1000 step , 3.54 Actor Loss, 3.67 Critic Loss,  0.05 Threshold , 3.80 Top reward, -130.30 Total reward\n",
            "63 episode , 106 step , 3.35 Actor Loss, 4.40 Critic Loss,  0.05 Threshold , 2.21 Top reward, -161.96 Total reward\n",
            "64 episode , 234 step , 3.47 Actor Loss, 3.70 Critic Loss,  0.05 Threshold , 2.32 Top reward, -160.25 Total reward\n",
            "65 episode , 1000 step , 3.18 Actor Loss, 4.03 Critic Loss,  0.05 Threshold , 3.64 Top reward, -83.22 Total reward\n",
            "66 episode , 931 step , 2.47 Actor Loss, 2.73 Critic Loss,  0.05 Threshold , 4.46 Top reward, -276.50 Total reward\n",
            "67 episode , 1000 step , 2.20 Actor Loss, 3.14 Critic Loss,  0.05 Threshold , 4.01 Top reward, -115.43 Total reward\n",
            "68 episode , 1000 step , 2.13 Actor Loss, 3.06 Critic Loss,  0.05 Threshold , 4.55 Top reward, -118.63 Total reward\n",
            "69 episode , 1000 step , 2.21 Actor Loss, 2.09 Critic Loss,  0.05 Threshold , 3.52 Top reward, -154.93 Total reward\n",
            "70 episode , 1000 step , 1.95 Actor Loss, 1.92 Critic Loss,  0.05 Threshold , 3.86 Top reward, -116.58 Total reward\n",
            "71 episode , 1000 step , 1.39 Actor Loss, 1.53 Critic Loss,  0.05 Threshold , 3.92 Top reward, -81.80 Total reward\n",
            "72 episode , 1000 step , 1.06 Actor Loss, 1.47 Critic Loss,  0.05 Threshold , 3.81 Top reward, -85.81 Total reward\n",
            "73 episode , 1000 step , 0.83 Actor Loss, 1.67 Critic Loss,  0.05 Threshold , 3.14 Top reward, -113.66 Total reward\n",
            "74 episode , 1000 step , 0.76 Actor Loss, 1.18 Critic Loss,  0.05 Threshold , 3.98 Top reward, -91.04 Total reward\n",
            "75 episode , 1000 step , 0.80 Actor Loss, 1.34 Critic Loss,  0.05 Threshold , 3.43 Top reward, -63.44 Total reward\n",
            "76 episode , 1000 step , 0.78 Actor Loss, 1.18 Critic Loss,  0.05 Threshold , 3.75 Top reward, -129.04 Total reward\n",
            "77 episode , 1000 step , 0.91 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , 3.84 Top reward, -116.56 Total reward\n",
            "78 episode , 1000 step , 0.82 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , 3.74 Top reward, -83.84 Total reward\n",
            "79 episode , 1000 step , 0.29 Actor Loss, 0.62 Critic Loss,  0.05 Threshold , 3.85 Top reward, -94.22 Total reward\n",
            "80 episode , 1000 step , 0.26 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , 3.63 Top reward, -142.29 Total reward\n",
            "81 episode , 1000 step , 0.29 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 4.43 Top reward, -120.86 Total reward\n",
            "82 episode , 1000 step , 0.09 Actor Loss, 0.55 Critic Loss,  0.05 Threshold , 4.56 Top reward, -94.01 Total reward\n",
            "83 episode , 1000 step , 0.13 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , 5.53 Top reward, -57.48 Total reward\n",
            "84 episode , 1000 step , -0.03 Actor Loss, 0.53 Critic Loss,  0.05 Threshold , 3.66 Top reward, -96.58 Total reward\n",
            "85 episode , 1000 step , -0.23 Actor Loss, 0.52 Critic Loss,  0.05 Threshold , 3.85 Top reward, -91.30 Total reward\n",
            "86 episode , 1000 step , -0.39 Actor Loss, 0.51 Critic Loss,  0.05 Threshold , 4.65 Top reward, -101.82 Total reward\n",
            "87 episode , 1000 step , -0.47 Actor Loss, 0.52 Critic Loss,  0.05 Threshold , 4.89 Top reward, -71.13 Total reward\n",
            "88 episode , 1000 step , -0.91 Actor Loss, 0.50 Critic Loss,  0.05 Threshold , 4.33 Top reward, -80.56 Total reward\n",
            "89 episode , 1000 step , -0.74 Actor Loss, 0.47 Critic Loss,  0.05 Threshold , 3.99 Top reward, -162.34 Total reward\n",
            "90 episode , 1000 step , -0.76 Actor Loss, 0.49 Critic Loss,  0.05 Threshold , 4.56 Top reward, -86.84 Total reward\n",
            "91 episode , 1000 step , -1.06 Actor Loss, 0.45 Critic Loss,  0.05 Threshold , 3.86 Top reward, -124.71 Total reward\n",
            "92 episode , 1000 step , -1.21 Actor Loss, 0.44 Critic Loss,  0.05 Threshold , 3.51 Top reward, -106.55 Total reward\n",
            "93 episode , 1000 step , -1.47 Actor Loss, 0.42 Critic Loss,  0.05 Threshold , 3.89 Top reward, -124.70 Total reward\n",
            "94 episode , 1000 step , -1.34 Actor Loss, 0.40 Critic Loss,  0.05 Threshold , 3.20 Top reward, -104.05 Total reward\n",
            "95 episode , 1000 step , -1.25 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 4.36 Top reward, -78.37 Total reward\n",
            "96 episode , 1000 step , -1.29 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , 2.99 Top reward, -123.39 Total reward\n",
            "97 episode , 1000 step , -1.29 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 3.61 Top reward, -49.10 Total reward\n",
            "98 episode , 1000 step , -1.08 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , 5.38 Top reward, -44.51 Total reward\n",
            "99 episode , 1000 step , -1.44 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 4.07 Top reward, -87.03 Total reward\n",
            "100 episode , 1000 step , -1.36 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , 6.28 Top reward, -30.59 Total reward\n",
            "101 episode , 1000 step , -0.98 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 4.28 Top reward, -67.45 Total reward\n",
            "102 episode , 1000 step , -0.57 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 6.62 Top reward, 1.12 Total reward\n",
            "103 episode , 1000 step , -0.39 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 4.68 Top reward, -117.96 Total reward\n",
            "104 episode , 1000 step , -0.52 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 5.05 Top reward, -58.50 Total reward\n",
            "105 episode , 1000 step , -0.84 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 4.84 Top reward, -54.01 Total reward\n",
            "106 episode , 1000 step , -1.23 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 4.82 Top reward, -84.76 Total reward\n",
            "107 episode , 1000 step , -1.52 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 5.25 Top reward, -59.76 Total reward\n",
            "108 episode , 1000 step , -1.52 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 6.66 Top reward, -58.09 Total reward\n",
            "109 episode , 1000 step , -1.63 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 4.96 Top reward, -77.63 Total reward\n",
            "110 episode , 1000 step , -1.83 Actor Loss, 0.58 Critic Loss,  0.05 Threshold , 14.33 Top reward, 60.09 Total reward\n",
            "111 episode , 570 step , -1.67 Actor Loss, 1.00 Critic Loss,  0.05 Threshold , 100.00 Top reward, 223.66 Total reward\n",
            "112 episode , 622 step , -1.70 Actor Loss, 1.22 Critic Loss,  0.05 Threshold , 100.00 Top reward, 214.58 Total reward\n",
            "113 episode , 1000 step , -1.80 Actor Loss, 3.44 Critic Loss,  0.05 Threshold , 6.74 Top reward, -75.08 Total reward\n",
            "114 episode , 1000 step , -2.05 Actor Loss, 3.32 Critic Loss,  0.05 Threshold , 4.63 Top reward, -54.78 Total reward\n",
            "115 episode , 1000 step , -2.11 Actor Loss, 3.41 Critic Loss,  0.05 Threshold , 6.61 Top reward, -64.97 Total reward\n",
            "116 episode , 709 step , -2.21 Actor Loss, 2.12 Critic Loss,  0.05 Threshold , 100.00 Top reward, 227.60 Total reward\n",
            "117 episode , 583 step , -2.14 Actor Loss, 3.14 Critic Loss,  0.05 Threshold , 11.14 Top reward, -29.23 Total reward\n",
            "118 episode , 118 step , -1.91 Actor Loss, 7.20 Critic Loss,  0.05 Threshold , 22.12 Top reward, 14.23 Total reward\n",
            "119 episode , 142 step , -1.72 Actor Loss, 8.26 Critic Loss,  0.05 Threshold , 10.24 Top reward, -200.64 Total reward\n",
            "120 episode , 443 step , -1.50 Actor Loss, 8.26 Critic Loss,  0.05 Threshold , 6.56 Top reward, -513.25 Total reward\n",
            "121 episode , 140 step , -1.31 Actor Loss, 3.32 Critic Loss,  0.05 Threshold , 6.22 Top reward, -439.99 Total reward\n",
            "122 episode , 140 step , -1.24 Actor Loss, 16.52 Critic Loss,  0.05 Threshold , 8.09 Top reward, -827.75 Total reward\n",
            "123 episode , 223 step , -1.18 Actor Loss, 15.91 Critic Loss,  0.05 Threshold , 8.46 Top reward, -657.39 Total reward\n",
            "124 episode , 147 step , -1.09 Actor Loss, 22.72 Critic Loss,  0.05 Threshold , 7.97 Top reward, -499.44 Total reward\n",
            "125 episode , 1000 step , -1.05 Actor Loss, 18.60 Critic Loss,  0.05 Threshold , 7.51 Top reward, -296.71 Total reward\n",
            "126 episode , 234 step , -1.27 Actor Loss, 9.80 Critic Loss,  0.05 Threshold , 10.66 Top reward, -409.69 Total reward\n",
            "127 episode , 355 step , -1.19 Actor Loss, 16.24 Critic Loss,  0.05 Threshold , 13.04 Top reward, -213.32 Total reward\n",
            "128 episode , 1000 step , -1.18 Actor Loss, 13.09 Critic Loss,  0.05 Threshold , 4.41 Top reward, -141.91 Total reward\n",
            "129 episode , 1000 step , -0.66 Actor Loss, 10.85 Critic Loss,  0.05 Threshold , 4.73 Top reward, -154.35 Total reward\n",
            "130 episode , 1000 step , -0.30 Actor Loss, 8.89 Critic Loss,  0.05 Threshold , 5.63 Top reward, -99.23 Total reward\n",
            "131 episode , 1000 step , -0.16 Actor Loss, 6.22 Critic Loss,  0.05 Threshold , 4.05 Top reward, -123.75 Total reward\n",
            "132 episode , 1000 step , -0.25 Actor Loss, 5.64 Critic Loss,  0.05 Threshold , 3.64 Top reward, -94.16 Total reward\n",
            "133 episode , 1000 step , -0.14 Actor Loss, 4.99 Critic Loss,  0.05 Threshold , 3.30 Top reward, -70.36 Total reward\n",
            "134 episode , 1000 step , 0.35 Actor Loss, 3.01 Critic Loss,  0.05 Threshold , 5.73 Top reward, -65.99 Total reward\n",
            "135 episode , 1000 step , 0.49 Actor Loss, 2.50 Critic Loss,  0.05 Threshold , 4.24 Top reward, -109.64 Total reward\n",
            "136 episode , 1000 step , -1.79 Actor Loss, 1.88 Critic Loss,  0.05 Threshold , 4.20 Top reward, -137.13 Total reward\n",
            "137 episode , 1000 step , -3.36 Actor Loss, 1.33 Critic Loss,  0.05 Threshold , 4.55 Top reward, -79.89 Total reward\n",
            "138 episode , 94 step , -5.87 Actor Loss, 0.85 Critic Loss,  0.05 Threshold , 6.74 Top reward, -107.13 Total reward\n",
            "139 episode , 62 step , -5.85 Actor Loss, 4.44 Critic Loss,  0.05 Threshold , 7.74 Top reward, -120.41 Total reward\n",
            "140 episode , 79 step , -5.85 Actor Loss, 1.95 Critic Loss,  0.05 Threshold , 7.42 Top reward, -141.38 Total reward\n",
            "141 episode , 80 step , -5.72 Actor Loss, 6.87 Critic Loss,  0.05 Threshold , 7.89 Top reward, -133.51 Total reward\n",
            "142 episode , 60 step , -5.72 Actor Loss, 8.11 Critic Loss,  0.05 Threshold , 13.30 Top reward, -111.33 Total reward\n",
            "143 episode , 74 step , -5.70 Actor Loss, 3.66 Critic Loss,  0.05 Threshold , 7.36 Top reward, -145.00 Total reward\n",
            "144 episode , 77 step , -5.69 Actor Loss, 6.02 Critic Loss,  0.05 Threshold , 17.81 Top reward, -129.67 Total reward\n",
            "145 episode , 54 step , -5.84 Actor Loss, 2.02 Critic Loss,  0.05 Threshold , 7.25 Top reward, -130.38 Total reward\n",
            "146 episode , 73 step , -5.77 Actor Loss, 3.57 Critic Loss,  0.05 Threshold , 8.21 Top reward, -143.20 Total reward\n",
            "147 episode , 85 step , -5.81 Actor Loss, 6.21 Critic Loss,  0.05 Threshold , 6.78 Top reward, -135.78 Total reward\n",
            "148 episode , 92 step , -5.70 Actor Loss, 7.83 Critic Loss,  0.05 Threshold , 6.38 Top reward, -152.56 Total reward\n",
            "149 episode , 93 step , -5.81 Actor Loss, 7.61 Critic Loss,  0.05 Threshold , 17.57 Top reward, -111.88 Total reward\n",
            "150 episode , 71 step , -5.92 Actor Loss, 8.80 Critic Loss,  0.05 Threshold , 17.70 Top reward, -134.44 Total reward\n",
            "151 episode , 1000 step , -6.16 Actor Loss, 8.06 Critic Loss,  0.05 Threshold , 4.63 Top reward, -160.06 Total reward\n",
            "152 episode , 1000 step , -6.63 Actor Loss, 7.47 Critic Loss,  0.05 Threshold , 4.65 Top reward, -78.78 Total reward\n",
            "153 episode , 1000 step , -7.10 Actor Loss, 7.68 Critic Loss,  0.05 Threshold , 3.58 Top reward, -114.98 Total reward\n",
            "154 episode , 96 step , -7.58 Actor Loss, 6.78 Critic Loss,  0.05 Threshold , 7.20 Top reward, -120.03 Total reward\n",
            "155 episode , 1000 step , -7.46 Actor Loss, 6.74 Critic Loss,  0.05 Threshold , 5.58 Top reward, -110.83 Total reward\n",
            "156 episode , 1000 step , -7.42 Actor Loss, 5.56 Critic Loss,  0.05 Threshold , 4.78 Top reward, -87.14 Total reward\n",
            "157 episode , 1000 step , -7.33 Actor Loss, 4.62 Critic Loss,  0.05 Threshold , 4.28 Top reward, -137.73 Total reward\n",
            "158 episode , 1000 step , -7.27 Actor Loss, 3.73 Critic Loss,  0.05 Threshold , 4.96 Top reward, -106.75 Total reward\n",
            "159 episode , 1000 step , -6.95 Actor Loss, 2.95 Critic Loss,  0.05 Threshold , 4.89 Top reward, -133.15 Total reward\n",
            "160 episode , 1000 step , -6.77 Actor Loss, 2.40 Critic Loss,  0.05 Threshold , 4.50 Top reward, -112.81 Total reward\n",
            "161 episode , 1000 step , -7.58 Actor Loss, 1.72 Critic Loss,  0.05 Threshold , 4.02 Top reward, -143.38 Total reward\n",
            "162 episode , 661 step , -8.58 Actor Loss, 1.12 Critic Loss,  0.05 Threshold , 14.70 Top reward, -256.83 Total reward\n",
            "163 episode , 350 step , -7.77 Actor Loss, 1.91 Critic Loss,  0.05 Threshold , 4.68 Top reward, -104.26 Total reward\n",
            "164 episode , 481 step , -7.81 Actor Loss, 2.93 Critic Loss,  0.05 Threshold , 4.52 Top reward, -160.89 Total reward\n",
            "165 episode , 366 step , -8.25 Actor Loss, 3.07 Critic Loss,  0.05 Threshold , 4.79 Top reward, -138.78 Total reward\n",
            "166 episode , 541 step , -8.25 Actor Loss, 4.66 Critic Loss,  0.05 Threshold , 4.35 Top reward, -165.45 Total reward\n",
            "167 episode , 295 step , -8.30 Actor Loss, 4.32 Critic Loss,  0.05 Threshold , 4.15 Top reward, -111.91 Total reward\n",
            "168 episode , 1000 step , -8.79 Actor Loss, 7.16 Critic Loss,  0.05 Threshold , 5.70 Top reward, -179.99 Total reward\n",
            "169 episode , 1000 step , -9.00 Actor Loss, 4.68 Critic Loss,  0.05 Threshold , 4.45 Top reward, -161.14 Total reward\n",
            "170 episode , 1000 step , -9.01 Actor Loss, 5.19 Critic Loss,  0.05 Threshold , 4.65 Top reward, -111.55 Total reward\n",
            "171 episode , 1000 step , -9.03 Actor Loss, 3.35 Critic Loss,  0.05 Threshold , 4.59 Top reward, -127.83 Total reward\n",
            "172 episode , 365 step , -8.91 Actor Loss, 5.76 Critic Loss,  0.05 Threshold , 8.50 Top reward, -97.55 Total reward\n",
            "173 episode , 1000 step , -9.03 Actor Loss, 5.07 Critic Loss,  0.05 Threshold , 5.02 Top reward, -69.63 Total reward\n",
            "174 episode , 1000 step , -9.14 Actor Loss, 4.62 Critic Loss,  0.05 Threshold , 4.36 Top reward, -120.77 Total reward\n",
            "175 episode , 139 step , -8.78 Actor Loss, 3.74 Critic Loss,  0.05 Threshold , 6.56 Top reward, -123.87 Total reward\n",
            "176 episode , 1000 step , -8.28 Actor Loss, 4.86 Critic Loss,  0.05 Threshold , 3.77 Top reward, -128.02 Total reward\n",
            "177 episode , 1000 step , -8.46 Actor Loss, 4.56 Critic Loss,  0.05 Threshold , 3.32 Top reward, -47.62 Total reward\n",
            "178 episode , 1000 step , -8.65 Actor Loss, 2.64 Critic Loss,  0.05 Threshold , 4.15 Top reward, -45.35 Total reward\n",
            "179 episode , 1000 step , -9.01 Actor Loss, 1.83 Critic Loss,  0.05 Threshold , 4.96 Top reward, -51.34 Total reward\n",
            "180 episode , 1000 step , -9.03 Actor Loss, 1.41 Critic Loss,  0.05 Threshold , 5.40 Top reward, -73.69 Total reward\n",
            "181 episode , 1000 step , -9.14 Actor Loss, 1.58 Critic Loss,  0.05 Threshold , 5.49 Top reward, -37.71 Total reward\n",
            "182 episode , 1000 step , -9.05 Actor Loss, 1.23 Critic Loss,  0.05 Threshold , 6.15 Top reward, -52.01 Total reward\n",
            "183 episode , 1000 step , -9.38 Actor Loss, 1.07 Critic Loss,  0.05 Threshold , 5.02 Top reward, -84.51 Total reward\n",
            "184 episode , 1000 step , -9.83 Actor Loss, 0.68 Critic Loss,  0.05 Threshold , 5.57 Top reward, -22.26 Total reward\n",
            "185 episode , 1000 step , -9.99 Actor Loss, 0.69 Critic Loss,  0.05 Threshold , 5.77 Top reward, -58.40 Total reward\n",
            "186 episode , 90 step , -11.01 Actor Loss, 0.59 Critic Loss,  0.05 Threshold , 19.23 Top reward, -25.35 Total reward\n",
            "187 episode , 97 step , -10.93 Actor Loss, 1.46 Critic Loss,  0.05 Threshold , 10.93 Top reward, -24.89 Total reward\n",
            "188 episode , 58 step , -11.04 Actor Loss, 1.15 Critic Loss,  0.05 Threshold , 11.80 Top reward, -56.33 Total reward\n",
            "189 episode , 74 step , -11.01 Actor Loss, 3.71 Critic Loss,  0.05 Threshold , 8.87 Top reward, -73.07 Total reward\n",
            "190 episode , 81 step , -10.96 Actor Loss, 5.54 Critic Loss,  0.05 Threshold , 11.29 Top reward, -64.59 Total reward\n",
            "191 episode , 63 step , -11.02 Actor Loss, 5.46 Critic Loss,  0.05 Threshold , 19.82 Top reward, -51.83 Total reward\n",
            "192 episode , 127 step , -11.27 Actor Loss, 3.29 Critic Loss,  0.05 Threshold , 13.51 Top reward, -35.05 Total reward\n",
            "193 episode , 84 step , -11.25 Actor Loss, 3.50 Critic Loss,  0.05 Threshold , 12.89 Top reward, -70.41 Total reward\n",
            "194 episode , 66 step , -11.31 Actor Loss, 4.67 Critic Loss,  0.05 Threshold , 18.97 Top reward, -97.75 Total reward\n",
            "195 episode , 1000 step , -11.31 Actor Loss, 3.57 Critic Loss,  0.05 Threshold , 5.46 Top reward, -62.60 Total reward\n",
            "196 episode , 1000 step , -10.76 Actor Loss, 3.70 Critic Loss,  0.05 Threshold , 4.94 Top reward, -108.11 Total reward\n",
            "197 episode , 1000 step , -10.20 Actor Loss, 3.46 Critic Loss,  0.05 Threshold , 4.45 Top reward, -59.21 Total reward\n",
            "198 episode , 1000 step , -9.72 Actor Loss, 2.55 Critic Loss,  0.05 Threshold , 5.57 Top reward, -69.95 Total reward\n",
            "199 episode , 1000 step , -9.27 Actor Loss, 1.99 Critic Loss,  0.05 Threshold , 4.99 Top reward, -81.85 Total reward\n",
            "200 episode , 1000 step , -8.95 Actor Loss, 1.83 Critic Loss,  0.05 Threshold , 4.08 Top reward, -91.95 Total reward\n",
            "201 episode , 1000 step , -8.58 Actor Loss, 1.72 Critic Loss,  0.05 Threshold , 5.97 Top reward, -77.43 Total reward\n",
            "202 episode , 1000 step , -8.52 Actor Loss, 1.70 Critic Loss,  0.05 Threshold , 8.02 Top reward, -30.77 Total reward\n",
            "203 episode , 1000 step , -8.62 Actor Loss, 1.62 Critic Loss,  0.05 Threshold , 3.48 Top reward, -59.83 Total reward\n",
            "204 episode , 1000 step , -8.81 Actor Loss, 1.63 Critic Loss,  0.05 Threshold , 4.33 Top reward, -69.13 Total reward\n",
            "205 episode , 1000 step , -10.23 Actor Loss, 0.94 Critic Loss,  0.05 Threshold , 4.09 Top reward, -88.56 Total reward\n",
            "206 episode , 1000 step , -10.78 Actor Loss, 0.73 Critic Loss,  0.05 Threshold , 5.80 Top reward, -150.41 Total reward\n",
            "207 episode , 88 step , -11.43 Actor Loss, 0.71 Critic Loss,  0.05 Threshold , 18.07 Top reward, -28.93 Total reward\n",
            "208 episode , 69 step , -11.00 Actor Loss, 1.63 Critic Loss,  0.05 Threshold , 12.32 Top reward, -37.69 Total reward\n",
            "209 episode , 104 step , -10.85 Actor Loss, 1.52 Critic Loss,  0.05 Threshold , 19.59 Top reward, 14.11 Total reward\n",
            "210 episode , 201 step , -10.74 Actor Loss, 1.82 Critic Loss,  0.05 Threshold , 5.11 Top reward, -149.77 Total reward\n",
            "211 episode , 1000 step , -11.09 Actor Loss, 2.95 Critic Loss,  0.05 Threshold , 4.86 Top reward, -87.73 Total reward\n",
            "212 episode , 1000 step , -11.81 Actor Loss, 3.07 Critic Loss,  0.05 Threshold , 4.49 Top reward, -14.36 Total reward\n",
            "213 episode , 1000 step , -12.31 Actor Loss, 2.36 Critic Loss,  0.05 Threshold , 3.94 Top reward, -85.38 Total reward\n",
            "214 episode , 1000 step , -12.81 Actor Loss, 2.33 Critic Loss,  0.05 Threshold , 4.52 Top reward, -80.89 Total reward\n",
            "215 episode , 949 step , -13.50 Actor Loss, 2.56 Critic Loss,  0.05 Threshold , 4.67 Top reward, -215.10 Total reward\n",
            "216 episode , 1000 step , -14.03 Actor Loss, 2.82 Critic Loss,  0.05 Threshold , 5.05 Top reward, -48.42 Total reward\n",
            "217 episode , 1000 step , -14.47 Actor Loss, 3.05 Critic Loss,  0.05 Threshold , 4.42 Top reward, -27.48 Total reward\n",
            "218 episode , 162 step , -14.69 Actor Loss, 2.53 Critic Loss,  0.05 Threshold , 2.50 Top reward, -89.90 Total reward\n",
            "219 episode , 238 step , -14.85 Actor Loss, 3.66 Critic Loss,  0.05 Threshold , 4.37 Top reward, -144.08 Total reward\n",
            "220 episode , 306 step , -15.15 Actor Loss, 5.34 Critic Loss,  0.05 Threshold , 4.19 Top reward, -139.50 Total reward\n",
            "221 episode , 1000 step , -15.25 Actor Loss, 5.31 Critic Loss,  0.05 Threshold , 5.55 Top reward, -40.03 Total reward\n",
            "222 episode , 264 step , -15.08 Actor Loss, 6.67 Critic Loss,  0.05 Threshold , 5.27 Top reward, -157.71 Total reward\n",
            "223 episode , 189 step , -15.15 Actor Loss, 5.62 Critic Loss,  0.05 Threshold , 4.52 Top reward, -107.65 Total reward\n",
            "224 episode , 1000 step , -15.13 Actor Loss, 6.54 Critic Loss,  0.05 Threshold , 4.86 Top reward, -63.14 Total reward\n",
            "225 episode , 224 step , -15.11 Actor Loss, 4.67 Critic Loss,  0.05 Threshold , 5.20 Top reward, -104.87 Total reward\n",
            "226 episode , 70 step , -14.96 Actor Loss, 5.28 Critic Loss,  0.05 Threshold , 18.63 Top reward, -33.80 Total reward\n",
            "227 episode , 1000 step , -14.87 Actor Loss, 6.17 Critic Loss,  0.05 Threshold , 5.04 Top reward, -93.32 Total reward\n",
            "228 episode , 1000 step , -14.66 Actor Loss, 4.98 Critic Loss,  0.05 Threshold , 4.73 Top reward, -70.24 Total reward\n",
            "229 episode , 1000 step , -14.36 Actor Loss, 4.47 Critic Loss,  0.05 Threshold , 4.13 Top reward, -59.55 Total reward\n",
            "230 episode , 1000 step , -14.17 Actor Loss, 3.90 Critic Loss,  0.05 Threshold , 4.70 Top reward, -77.11 Total reward\n",
            "231 episode , 1000 step , -14.00 Actor Loss, 3.76 Critic Loss,  0.05 Threshold , 4.62 Top reward, -70.53 Total reward\n",
            "232 episode , 1000 step , -13.88 Actor Loss, 3.59 Critic Loss,  0.05 Threshold , 5.56 Top reward, -39.18 Total reward\n",
            "233 episode , 1000 step , -13.64 Actor Loss, 2.86 Critic Loss,  0.05 Threshold , 7.21 Top reward, -42.65 Total reward\n",
            "234 episode , 1000 step , -13.98 Actor Loss, 1.99 Critic Loss,  0.05 Threshold , 6.76 Top reward, -11.57 Total reward\n",
            "235 episode , 1000 step , -13.71 Actor Loss, 1.62 Critic Loss,  0.05 Threshold , 6.79 Top reward, -31.72 Total reward\n",
            "236 episode , 1000 step , -13.69 Actor Loss, 1.05 Critic Loss,  0.05 Threshold , 6.04 Top reward, -28.80 Total reward\n",
            "237 episode , 1000 step , -14.13 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , 5.88 Top reward, -24.45 Total reward\n",
            "238 episode , 1000 step , -13.90 Actor Loss, 0.52 Critic Loss,  0.05 Threshold , 5.07 Top reward, -110.03 Total reward\n",
            "239 episode , 1000 step , -13.88 Actor Loss, 0.55 Critic Loss,  0.05 Threshold , 5.47 Top reward, -35.40 Total reward\n",
            "240 episode , 1000 step , -13.80 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 6.64 Top reward, -4.49 Total reward\n",
            "241 episode , 1000 step , -13.77 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 5.97 Top reward, -22.03 Total reward\n",
            "242 episode , 1000 step , -13.75 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , 5.35 Top reward, -34.17 Total reward\n",
            "243 episode , 1000 step , -14.06 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , 4.97 Top reward, -50.22 Total reward\n",
            "244 episode , 1000 step , -14.07 Actor Loss, 0.59 Critic Loss,  0.05 Threshold , 6.81 Top reward, 9.86 Total reward\n",
            "245 episode , 1000 step , -14.53 Actor Loss, 0.61 Critic Loss,  0.05 Threshold , 4.65 Top reward, -50.43 Total reward\n",
            "246 episode , 1000 step , -14.70 Actor Loss, 0.59 Critic Loss,  0.05 Threshold , 4.96 Top reward, -54.70 Total reward\n",
            "247 episode , 1000 step , -14.78 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , 4.93 Top reward, -13.99 Total reward\n",
            "248 episode , 1000 step , -14.65 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , 5.98 Top reward, -90.94 Total reward\n",
            "249 episode , 1000 step , -14.31 Actor Loss, 0.63 Critic Loss,  0.05 Threshold , 6.48 Top reward, -7.64 Total reward\n",
            "250 episode , 1000 step , -14.19 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 5.37 Top reward, -25.64 Total reward\n",
            "251 episode , 1000 step , -14.19 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 5.11 Top reward, -18.63 Total reward\n",
            "252 episode , 1000 step , -14.15 Actor Loss, 0.57 Critic Loss,  0.05 Threshold , 5.04 Top reward, -24.41 Total reward\n",
            "253 episode , 1000 step , -14.28 Actor Loss, 0.53 Critic Loss,  0.05 Threshold , 5.29 Top reward, -29.58 Total reward\n",
            "254 episode , 1000 step , -14.50 Actor Loss, 0.53 Critic Loss,  0.05 Threshold , 5.16 Top reward, -8.75 Total reward\n",
            "255 episode , 1000 step , -14.36 Actor Loss, 0.52 Critic Loss,  0.05 Threshold , 5.18 Top reward, -44.31 Total reward\n",
            "256 episode , 1000 step , -13.82 Actor Loss, 0.50 Critic Loss,  0.05 Threshold , 5.00 Top reward, -12.72 Total reward\n",
            "257 episode , 1000 step , -13.40 Actor Loss, 0.48 Critic Loss,  0.05 Threshold , 4.67 Top reward, -5.22 Total reward\n",
            "258 episode , 1000 step , -13.68 Actor Loss, 0.44 Critic Loss,  0.05 Threshold , 6.39 Top reward, 7.90 Total reward\n",
            "259 episode , 1000 step , -14.32 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , 5.35 Top reward, -5.58 Total reward\n",
            "260 episode , 1000 step , -14.70 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 5.65 Top reward, -13.09 Total reward\n",
            "261 episode , 1000 step , -14.57 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , 6.10 Top reward, 10.92 Total reward\n",
            "262 episode , 1000 step , -14.36 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 5.15 Top reward, 8.25 Total reward\n",
            "263 episode , 1000 step , -14.37 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 4.54 Top reward, -27.15 Total reward\n",
            "264 episode , 1000 step , -14.45 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 5.96 Top reward, 2.66 Total reward\n",
            "265 episode , 1000 step , -14.59 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 5.41 Top reward, -53.70 Total reward\n",
            "266 episode , 1000 step , -14.72 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 4.86 Top reward, 9.32 Total reward\n",
            "267 episode , 1000 step , -14.76 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 5.76 Top reward, 19.91 Total reward\n",
            "268 episode , 1000 step , -14.65 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 4.95 Top reward, -0.65 Total reward\n",
            "269 episode , 1000 step , -14.92 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , 4.48 Top reward, -16.93 Total reward\n",
            "270 episode , 1000 step , -15.06 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 5.41 Top reward, -29.92 Total reward\n",
            "271 episode , 1000 step , -15.18 Actor Loss, 0.49 Critic Loss,  0.05 Threshold , 14.42 Top reward, -55.05 Total reward\n",
            "272 episode , 1000 step , -15.40 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , 6.21 Top reward, -60.82 Total reward\n",
            "273 episode , 970 step , -15.28 Actor Loss, 0.49 Critic Loss,  0.05 Threshold , 100.00 Top reward, 94.79 Total reward\n",
            "274 episode , 1000 step , -15.50 Actor Loss, 1.68 Critic Loss,  0.05 Threshold , 19.46 Top reward, 123.72 Total reward\n",
            "275 episode , 1000 step , -15.87 Actor Loss, 1.17 Critic Loss,  0.05 Threshold , 5.85 Top reward, -44.43 Total reward\n",
            "276 episode , 839 step , -15.98 Actor Loss, 1.58 Critic Loss,  0.05 Threshold , 100.00 Top reward, 136.26 Total reward\n",
            "277 episode , 1000 step , -15.95 Actor Loss, 3.17 Critic Loss,  0.05 Threshold , 16.03 Top reward, -4.46 Total reward\n",
            "278 episode , 1000 step , -16.30 Actor Loss, 2.38 Critic Loss,  0.05 Threshold , 5.16 Top reward, -34.25 Total reward\n",
            "279 episode , 1000 step , -16.47 Actor Loss, 3.35 Critic Loss,  0.05 Threshold , 5.11 Top reward, -56.21 Total reward\n",
            "280 episode , 1000 step , -16.29 Actor Loss, 1.89 Critic Loss,  0.05 Threshold , 6.12 Top reward, -50.77 Total reward\n",
            "281 episode , 1000 step , -16.45 Actor Loss, 2.88 Critic Loss,  0.05 Threshold , 5.96 Top reward, -10.16 Total reward\n",
            "282 episode , 792 step , -16.45 Actor Loss, 2.43 Critic Loss,  0.05 Threshold , 100.00 Top reward, 188.94 Total reward\n",
            "283 episode , 1000 step , -16.67 Actor Loss, 3.54 Critic Loss,  0.05 Threshold , 6.25 Top reward, 5.99 Total reward\n",
            "284 episode , 1000 step , -16.62 Actor Loss, 2.77 Critic Loss,  0.05 Threshold , 13.54 Top reward, 83.98 Total reward\n",
            "285 episode , 97 step , -16.44 Actor Loss, 1.27 Critic Loss,  0.05 Threshold , 21.60 Top reward, 23.45 Total reward\n",
            "286 episode , 253 step , -16.92 Actor Loss, 6.17 Critic Loss,  0.05 Threshold , 100.00 Top reward, 268.57 Total reward\n",
            "287 episode , 146 step , -16.63 Actor Loss, 5.47 Critic Loss,  0.05 Threshold , 15.57 Top reward, 29.65 Total reward\n",
            "288 episode , 74 step , -16.64 Actor Loss, 8.74 Critic Loss,  0.05 Threshold , 5.96 Top reward, -145.25 Total reward\n",
            "289 episode , 84 step , -16.59 Actor Loss, 5.10 Critic Loss,  0.05 Threshold , 6.63 Top reward, -180.91 Total reward\n",
            "290 episode , 369 step , -16.71 Actor Loss, 14.95 Critic Loss,  0.05 Threshold , 100.00 Top reward, 239.99 Total reward\n",
            "291 episode , 1000 step , -17.05 Actor Loss, 9.91 Critic Loss,  0.05 Threshold , 5.14 Top reward, -79.75 Total reward\n",
            "292 episode , 1000 step , -16.89 Actor Loss, 7.53 Critic Loss,  0.05 Threshold , 10.25 Top reward, -2.94 Total reward\n",
            "293 episode , 1000 step , -16.37 Actor Loss, 6.43 Critic Loss,  0.05 Threshold , 4.17 Top reward, -9.96 Total reward\n",
            "294 episode , 1000 step , -15.89 Actor Loss, 5.91 Critic Loss,  0.05 Threshold , 4.57 Top reward, -115.28 Total reward\n",
            "295 episode , 1000 step , -15.65 Actor Loss, 6.28 Critic Loss,  0.05 Threshold , 4.25 Top reward, -87.64 Total reward\n",
            "296 episode , 1000 step , -16.05 Actor Loss, 5.88 Critic Loss,  0.05 Threshold , 3.93 Top reward, -100.14 Total reward\n",
            "297 episode , 1000 step , -16.58 Actor Loss, 4.82 Critic Loss,  0.05 Threshold , 4.29 Top reward, -38.72 Total reward\n",
            "298 episode , 1000 step , -16.52 Actor Loss, 3.70 Critic Loss,  0.05 Threshold , 3.50 Top reward, -130.27 Total reward\n",
            "299 episode , 132 step , -16.73 Actor Loss, 4.51 Critic Loss,  0.05 Threshold , 4.12 Top reward, -106.91 Total reward\n",
            "300 episode , 1000 step , -16.76 Actor Loss, 5.47 Critic Loss,  0.05 Threshold , 5.17 Top reward, -43.84 Total reward\n",
            "301 episode , 1000 step , -16.85 Actor Loss, 3.85 Critic Loss,  0.05 Threshold , 4.66 Top reward, -41.29 Total reward\n",
            "302 episode , 1000 step , -17.56 Actor Loss, 1.26 Critic Loss,  0.05 Threshold , 4.54 Top reward, -24.69 Total reward\n",
            "303 episode , 1000 step , -17.76 Actor Loss, 1.81 Critic Loss,  0.05 Threshold , 18.74 Top reward, -10.93 Total reward\n",
            "304 episode , 1000 step , -17.85 Actor Loss, 1.81 Critic Loss,  0.05 Threshold , 12.61 Top reward, 20.63 Total reward\n",
            "305 episode , 1000 step , -17.99 Actor Loss, 1.48 Critic Loss,  0.05 Threshold , 12.09 Top reward, 13.75 Total reward\n",
            "306 episode , 1000 step , -18.66 Actor Loss, 1.17 Critic Loss,  0.05 Threshold , 24.54 Top reward, 68.85 Total reward\n",
            "307 episode , 1000 step , -18.97 Actor Loss, 1.30 Critic Loss,  0.05 Threshold , 14.91 Top reward, 6.41 Total reward\n",
            "308 episode , 707 step , -19.37 Actor Loss, 1.17 Critic Loss,  0.05 Threshold , 100.00 Top reward, 184.87 Total reward\n",
            "309 episode , 1000 step , -19.89 Actor Loss, 2.48 Critic Loss,  0.05 Threshold , 5.30 Top reward, -10.67 Total reward\n",
            "310 episode , 1000 step , -20.48 Actor Loss, 2.19 Critic Loss,  0.05 Threshold , 13.86 Top reward, 6.46 Total reward\n",
            "311 episode , 614 step , -20.84 Actor Loss, 1.27 Critic Loss,  0.05 Threshold , 100.00 Top reward, 199.79 Total reward\n",
            "312 episode , 1000 step , -21.18 Actor Loss, 1.99 Critic Loss,  0.05 Threshold , 22.89 Top reward, 27.75 Total reward\n",
            "313 episode , 633 step , -21.29 Actor Loss, 3.18 Critic Loss,  0.05 Threshold , 100.00 Top reward, 201.95 Total reward\n",
            "314 episode , 610 step , -21.65 Actor Loss, 4.52 Critic Loss,  0.05 Threshold , 100.00 Top reward, 217.24 Total reward\n",
            "315 episode , 525 step , -21.76 Actor Loss, 4.33 Critic Loss,  0.05 Threshold , 100.00 Top reward, 283.40 Total reward\n",
            "316 episode , 1000 step , -21.78 Actor Loss, 5.88 Critic Loss,  0.05 Threshold , 11.95 Top reward, 72.27 Total reward\n",
            "317 episode , 977 step , -21.73 Actor Loss, 5.60 Critic Loss,  0.05 Threshold , 100.00 Top reward, 195.76 Total reward\n",
            "318 episode , 1000 step , -22.07 Actor Loss, 6.29 Critic Loss,  0.05 Threshold , 13.91 Top reward, 75.25 Total reward\n",
            "319 episode , 1000 step , -22.02 Actor Loss, 5.46 Critic Loss,  0.05 Threshold , 5.64 Top reward, -3.62 Total reward\n",
            "320 episode , 1000 step , -22.28 Actor Loss, 7.15 Critic Loss,  0.05 Threshold , 22.68 Top reward, 169.70 Total reward\n",
            "321 episode , 1000 step , -22.81 Actor Loss, 5.64 Critic Loss,  0.05 Threshold , 13.88 Top reward, 57.86 Total reward\n",
            "322 episode , 1000 step , -23.00 Actor Loss, 4.79 Critic Loss,  0.05 Threshold , 22.47 Top reward, 133.88 Total reward\n",
            "323 episode , 116 step , -23.16 Actor Loss, 7.19 Critic Loss,  0.05 Threshold , 17.72 Top reward, 12.31 Total reward\n",
            "324 episode , 1000 step , -23.45 Actor Loss, 5.72 Critic Loss,  0.05 Threshold , 5.66 Top reward, -71.93 Total reward\n",
            "325 episode , 1000 step , -23.87 Actor Loss, 4.56 Critic Loss,  0.05 Threshold , 15.73 Top reward, 58.81 Total reward\n",
            "326 episode , 1000 step , -24.13 Actor Loss, 3.41 Critic Loss,  0.05 Threshold , 4.65 Top reward, -55.09 Total reward\n",
            "327 episode , 1000 step , -24.81 Actor Loss, 1.90 Critic Loss,  0.05 Threshold , 4.90 Top reward, -37.33 Total reward\n",
            "328 episode , 1000 step , -25.25 Actor Loss, 1.65 Critic Loss,  0.05 Threshold , 4.56 Top reward, -56.30 Total reward\n",
            "329 episode , 1000 step , -25.13 Actor Loss, 0.76 Critic Loss,  0.05 Threshold , 12.76 Top reward, 29.57 Total reward\n",
            "330 episode , 1000 step , -25.02 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , 4.25 Top reward, -49.30 Total reward\n",
            "331 episode , 949 step , -24.98 Actor Loss, 0.66 Critic Loss,  0.05 Threshold , 100.00 Top reward, 184.23 Total reward\n",
            "332 episode , 1000 step , -24.98 Actor Loss, 1.42 Critic Loss,  0.05 Threshold , 4.41 Top reward, -63.10 Total reward\n",
            "333 episode , 418 step , -25.27 Actor Loss, 1.21 Critic Loss,  0.05 Threshold , 100.00 Top reward, 227.71 Total reward\n",
            "334 episode , 84 step , -25.46 Actor Loss, 3.98 Critic Loss,  0.05 Threshold , 23.07 Top reward, -10.51 Total reward\n",
            "335 episode , 1000 step , -25.72 Actor Loss, 3.76 Critic Loss,  0.05 Threshold , 22.62 Top reward, 133.01 Total reward\n",
            "336 episode , 75 step , -25.60 Actor Loss, 3.87 Critic Loss,  0.05 Threshold , 17.34 Top reward, -28.61 Total reward\n",
            "337 episode , 1000 step , -25.63 Actor Loss, 4.15 Critic Loss,  0.05 Threshold , 22.22 Top reward, 156.08 Total reward\n",
            "338 episode , 1000 step , -26.05 Actor Loss, 3.51 Critic Loss,  0.05 Threshold , 11.26 Top reward, 2.75 Total reward\n",
            "339 episode , 611 step , -25.94 Actor Loss, 3.94 Critic Loss,  0.05 Threshold , 100.00 Top reward, 271.00 Total reward\n",
            "340 episode , 1000 step , -25.87 Actor Loss, 3.79 Critic Loss,  0.05 Threshold , 5.50 Top reward, -53.03 Total reward\n",
            "341 episode , 1000 step , -25.77 Actor Loss, 4.29 Critic Loss,  0.05 Threshold , 5.06 Top reward, -42.78 Total reward\n",
            "342 episode , 1000 step , -26.53 Actor Loss, 4.96 Critic Loss,  0.05 Threshold , 5.10 Top reward, -58.99 Total reward\n",
            "343 episode , 1000 step , -26.71 Actor Loss, 5.63 Critic Loss,  0.05 Threshold , 3.56 Top reward, -71.58 Total reward\n",
            "344 episode , 1000 step , -26.64 Actor Loss, 5.29 Critic Loss,  0.05 Threshold , 4.09 Top reward, -29.26 Total reward\n",
            "345 episode , 1000 step , -26.54 Actor Loss, 4.01 Critic Loss,  0.05 Threshold , 3.57 Top reward, -70.51 Total reward\n",
            "346 episode , 913 step , -25.97 Actor Loss, 3.30 Critic Loss,  0.05 Threshold , 100.00 Top reward, 169.85 Total reward\n",
            "347 episode , 80 step , -25.62 Actor Loss, 4.60 Critic Loss,  0.05 Threshold , 23.11 Top reward, 6.23 Total reward\n",
            "348 episode , 1000 step , -25.51 Actor Loss, 3.26 Critic Loss,  0.05 Threshold , 12.36 Top reward, 32.32 Total reward\n",
            "349 episode , 119 step , -25.65 Actor Loss, 1.65 Critic Loss,  0.05 Threshold , 12.19 Top reward, 12.99 Total reward\n",
            "350 episode , 1000 step , -25.36 Actor Loss, 3.82 Critic Loss,  0.05 Threshold , 4.16 Top reward, -17.96 Total reward\n",
            "351 episode , 81 step , -25.57 Actor Loss, 4.40 Critic Loss,  0.05 Threshold , 16.91 Top reward, 28.86 Total reward\n",
            "352 episode , 1000 step , -25.51 Actor Loss, 4.42 Critic Loss,  0.05 Threshold , 19.97 Top reward, 102.38 Total reward\n",
            "353 episode , 302 step , -25.77 Actor Loss, 3.11 Critic Loss,  0.05 Threshold , 100.00 Top reward, 240.49 Total reward\n",
            "354 episode , 68 step , -25.69 Actor Loss, 7.85 Critic Loss,  0.05 Threshold , 14.83 Top reward, -8.42 Total reward\n",
            "355 episode , 317 step , -25.84 Actor Loss, 4.61 Critic Loss,  0.05 Threshold , 100.00 Top reward, 236.04 Total reward\n",
            "356 episode , 1000 step , -25.73 Actor Loss, 5.06 Critic Loss,  0.05 Threshold , 12.76 Top reward, 62.81 Total reward\n",
            "357 episode , 1000 step , -25.47 Actor Loss, 5.12 Critic Loss,  0.05 Threshold , 23.13 Top reward, 70.41 Total reward\n",
            "358 episode , 714 step , -25.49 Actor Loss, 4.40 Critic Loss,  0.05 Threshold , 100.00 Top reward, 136.25 Total reward\n",
            "359 episode , 1000 step , -25.52 Actor Loss, 5.94 Critic Loss,  0.05 Threshold , 11.29 Top reward, 52.89 Total reward\n",
            "360 episode , 476 step , -25.46 Actor Loss, 4.85 Critic Loss,  0.05 Threshold , 100.00 Top reward, 270.39 Total reward\n",
            "361 episode , 139 step , -25.11 Actor Loss, 5.82 Critic Loss,  0.05 Threshold , 10.77 Top reward, -26.87 Total reward\n",
            "362 episode , 1000 step , -24.69 Actor Loss, 6.75 Critic Loss,  0.05 Threshold , 22.84 Top reward, 122.32 Total reward\n",
            "363 episode , 784 step , -24.68 Actor Loss, 7.00 Critic Loss,  0.05 Threshold , 100.00 Top reward, 204.27 Total reward\n",
            "364 episode , 1000 step , -24.88 Actor Loss, 6.90 Critic Loss,  0.05 Threshold , 4.87 Top reward, -103.23 Total reward\n",
            "365 episode , 1000 step , -25.46 Actor Loss, 5.29 Critic Loss,  0.05 Threshold , 3.88 Top reward, -112.68 Total reward\n",
            "366 episode , 365 step , -26.37 Actor Loss, 5.58 Critic Loss,  0.05 Threshold , 100.00 Top reward, 264.91 Total reward\n",
            "367 episode , 281 step , -26.46 Actor Loss, 5.25 Critic Loss,  0.05 Threshold , 100.00 Top reward, 266.12 Total reward\n",
            "368 episode , 458 step , -26.62 Actor Loss, 6.21 Critic Loss,  0.05 Threshold , 100.00 Top reward, 197.73 Total reward\n",
            "369 episode , 1000 step , -26.44 Actor Loss, 6.93 Critic Loss,  0.05 Threshold , 13.15 Top reward, 116.50 Total reward\n",
            "370 episode , 662 step , -27.03 Actor Loss, 6.50 Critic Loss,  0.05 Threshold , 12.65 Top reward, -108.47 Total reward\n",
            "371 episode , 505 step , -27.64 Actor Loss, 8.59 Critic Loss,  0.05 Threshold , 100.00 Top reward, 241.49 Total reward\n",
            "372 episode , 188 step , -28.07 Actor Loss, 12.18 Critic Loss,  0.05 Threshold , 12.74 Top reward, 21.88 Total reward\n",
            "373 episode , 522 step , -28.53 Actor Loss, 7.62 Critic Loss,  0.05 Threshold , 100.00 Top reward, 252.12 Total reward\n",
            "374 episode , 525 step , -28.84 Actor Loss, 12.47 Critic Loss,  0.05 Threshold , 100.00 Top reward, 255.81 Total reward\n",
            "375 episode , 521 step , -28.66 Actor Loss, 12.37 Critic Loss,  0.05 Threshold , 100.00 Top reward, 251.56 Total reward\n",
            "376 episode , 1000 step , -29.57 Actor Loss, 10.41 Critic Loss,  0.05 Threshold , 3.41 Top reward, -92.84 Total reward\n",
            "377 episode , 655 step , -30.33 Actor Loss, 8.01 Critic Loss,  0.05 Threshold , 100.00 Top reward, 225.43 Total reward\n",
            "378 episode , 1000 step , -31.08 Actor Loss, 10.03 Critic Loss,  0.05 Threshold , 5.10 Top reward, -66.81 Total reward\n",
            "379 episode , 984 step , -30.99 Actor Loss, 7.29 Critic Loss,  0.05 Threshold , 100.00 Top reward, 93.87 Total reward\n",
            "380 episode , 75 step , -30.59 Actor Loss, 6.09 Critic Loss,  0.05 Threshold , 18.93 Top reward, 48.47 Total reward\n",
            "381 episode , 378 step , -30.45 Actor Loss, 9.88 Critic Loss,  0.05 Threshold , 100.00 Top reward, 217.76 Total reward\n",
            "382 episode , 534 step , -30.58 Actor Loss, 9.35 Critic Loss,  0.05 Threshold , 100.00 Top reward, 171.81 Total reward\n",
            "383 episode , 453 step , -30.49 Actor Loss, 11.78 Critic Loss,  0.05 Threshold , 100.00 Top reward, 171.91 Total reward\n",
            "384 episode , 1000 step , -30.69 Actor Loss, 9.87 Critic Loss,  0.05 Threshold , 24.18 Top reward, 78.21 Total reward\n",
            "385 episode , 396 step , -31.22 Actor Loss, 7.54 Critic Loss,  0.05 Threshold , 100.00 Top reward, 198.45 Total reward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lg-4834irdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}