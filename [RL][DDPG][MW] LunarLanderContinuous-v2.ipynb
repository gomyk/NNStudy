{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1lA8LZfF660+Nbrf0YXv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20LunarLanderContinuous-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "f02178c5-80cd-4cf5-ae36-829801b4fc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 66%\r\rReading package lists... 66%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 90%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"LunarLanderContinuous-v2\")\n",
        "env._max_episode_steps = 800\n",
        "#env.seed(10)\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_512 = 512\n",
        "HIDDEN_256 = 256\n",
        "HIDDEN_128 = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, outputs)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "\n",
        "        #self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear2.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = F.relu(self.bn256(self.linear2(x)))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.linear3 = nn.Linear(action_size, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, 1)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "        self.bn128 = nn.BatchNorm1d(HIDDEN_128)\n",
        "\n",
        "        #self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear3.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        #x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        y = self.linear3(action)\n",
        "    \n",
        "        x = F.relu(torch.add(x, y))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "5cd87e03-0320-497f-d411-323134167f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.90\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2000\n",
        "WEIGHT_DECAY = 0.01\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.001\n",
        "CRITIC_LR = 0.004\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 500\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 0.4\n",
        "ou_noise_sigma = 0.2\n",
        "\n",
        "RECORD_INTERVAL = 100\n",
        "\n",
        "#seed = random.seed(0)\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = env.action_space.shape[0]\n",
        "n_obvs = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "#actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "#actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "#critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "#critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR, weight_decay=WEIGHT_DECAY)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def getEpsTheshold(step_done):\n",
        "      return EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * step_done / EPS_DECAY)\n",
        "\n",
        "def select_action(state):\n",
        "    #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold  = getEpsTheshold(steps_done)\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = np.random.uniform(-1,1,n_actions)\n",
        "    else:\n",
        "        actor.eval()\n",
        "        with torch.no_grad():\n",
        "          selected_action = actor(\n",
        "              state.to(device)\n",
        "           )[0].detach().cpu().numpy()\n",
        "          \n",
        "    _noise = noise.sample()*eps_threshold\n",
        "    if steps_done%100 ==0:\n",
        "      print(_noise)\n",
        "    actor.train()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvU9RgzCYsW",
        "colab_type": "text"
      },
      "source": [
        "##Normalizer for obv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0, 0\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + (GAMMA * next_value * masks)\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.mse_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    \n",
        "    # train actor\n",
        "\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "8ffcc500-c5b8-4f3f-810e-9cf825d604cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "batch_count = 0\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        \n",
        "        \n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(total_reward)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Total reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward))\n",
        "            #print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    #if i_episode % TARGET_UPDATE == 0:\n",
        "    #   actor_target.load_state_dict(actor.state_dict())\n",
        "    #   critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 95 step , 0.00 Actor Loss, 0.00 Critic Loss,  0.86 Threshold , 4.77 Top reward, -297.58 Total reward\n",
            "[0.28663376 0.25230191]\n",
            "[0.27868515 0.23292597]\n",
            "1 episode , 125 step , 2.30 Actor Loss, 4.28 Critic Loss,  0.81 Threshold , 1.99 Top reward, -531.96 Total reward\n",
            "2 episode , 63 step , 3.73 Actor Loss, 33.21 Critic Loss,  0.78 Threshold , 40.59 Top reward, -76.68 Total reward\n",
            "[0.13769049 0.20188193]\n",
            "[0.18653166 0.15588927]\n",
            "3 episode , 152 step , 2.73 Actor Loss, 24.93 Critic Loss,  0.73 Threshold , 14.14 Top reward, -36.68 Total reward\n",
            "[0.21207432 0.10643377]\n",
            "4 episode , 117 step , 2.83 Actor Loss, 22.76 Critic Loss,  0.69 Threshold , 11.65 Top reward, -213.89 Total reward\n",
            "[0.20677565 0.22479885]\n",
            "5 episode , 97 step , 2.70 Actor Loss, 22.50 Critic Loss,  0.65 Threshold , 5.53 Top reward, -97.09 Total reward\n",
            "[0.21672472 0.09972543]\n",
            "[0.11252225 0.0674175 ]\n",
            "6 episode , 198 step , 2.31 Actor Loss, 11.24 Critic Loss,  0.59 Threshold , 35.12 Top reward, -138.40 Total reward\n",
            "[0.10523704 0.18002474]\n",
            "[0.20810787 0.19359621]\n",
            "7 episode , 212 step , 2.38 Actor Loss, 14.28 Critic Loss,  0.53 Threshold , 4.30 Top reward, -310.05 Total reward\n",
            "[0.16052714 0.0508363 ]\n",
            "[0.15585474 0.15723124]\n",
            "[0.14310651 0.10366297]\n",
            "8 episode , 324 step , 1.70 Actor Loss, 10.32 Critic Loss,  0.46 Threshold , 13.83 Top reward, -247.61 Total reward\n",
            "[0.13542444 0.07659208]\n",
            "[0.12770939 0.08424992]\n",
            "[0.07077136 0.13102641]\n",
            "[0.07360955 0.13278335]\n",
            "[0.06918784 0.1109787 ]\n",
            "[0.09143514 0.15870173]\n",
            "[0.06760349 0.09431002]\n",
            "9 episode , 617 step , 0.25 Actor Loss, 8.82 Critic Loss,  0.34 Threshold , 20.78 Top reward, -249.82 Total reward\n",
            "[0.11811081 0.07919062]\n",
            "[0.1000056  0.06458419]\n",
            "[0.09445806 0.05713169]\n",
            "[0.08386874 0.02959044]\n",
            "[0.05706647 0.06781685]\n",
            "[0.0548917  0.03068256]\n",
            "10 episode , 638 step , -2.10 Actor Loss, 6.74 Critic Loss,  0.25 Threshold , 6.30 Top reward, -236.85 Total reward\n",
            "[0.09183847 0.08611049]\n",
            "[0.02407608 0.07489081]\n",
            "[0.05737568 0.07507933]\n",
            "[0.06854555 0.04557827]\n",
            "11 episode , 439 step , -3.71 Actor Loss, 5.19 Critic Loss,  0.20 Threshold , 16.00 Top reward, -261.18 Total reward\n",
            "[0.02578066 0.07102646]\n",
            "[0.05564321 0.0582873 ]\n",
            "[0.03835002 0.0477165 ]\n",
            "[0.04400763 0.04808236]\n",
            "[0.06088494 0.04235484]\n",
            "[0.03955917 0.02202817]\n",
            "[0.0373024 0.0561347]\n",
            "[0.04182088 0.02595576]\n",
            "12 episode , 800 step , -5.40 Actor Loss, 5.21 Critic Loss,  0.14 Threshold , 4.22 Top reward, -81.34 Total reward\n",
            "[0.03730917 0.0432213 ]\n",
            "[0.02888322 0.0242825 ]\n",
            "[0.01914666 0.0281521 ]\n",
            "[0.0304503  0.02617671]\n",
            "[0.03070706 0.02877376]\n",
            "[0.02814787 0.02337843]\n",
            "[0.02417725 0.03901343]\n",
            "13 episode , 626 step , -7.36 Actor Loss, 4.20 Critic Loss,  0.10 Threshold , 4.78 Top reward, -165.84 Total reward\n",
            "[0.02710458 0.03324953]\n",
            "[0.033116   0.03287083]\n",
            "[0.01173119 0.00884719]\n",
            "[0.02172053 0.01960325]\n",
            "[0.02084186 0.01275189]\n",
            "[0.01826506 0.0064575 ]\n",
            "[0.02089993 0.02287182]\n",
            "[0.01728262 0.01649016]\n",
            "14 episode , 800 step , -9.12 Actor Loss, 5.48 Critic Loss,  0.07 Threshold , 3.91 Top reward, -95.34 Total reward\n",
            "[0.01905495 0.02366993]\n",
            "[0.01584616 0.01538271]\n",
            "[0.02171328 0.01037137]\n",
            "[0.01428715 0.01537138]\n",
            "[0.01609432 0.01637901]\n",
            "[0.01017775 0.01862354]\n",
            "[0.01424961 0.00912931]\n",
            "[0.00747419 0.01883317]\n",
            "15 episode , 800 step , -10.46 Actor Loss, 5.29 Critic Loss,  0.05 Threshold , 4.52 Top reward, -83.63 Total reward\n",
            "[0.01265384 0.00968473]\n",
            "[0.01083246 0.01034836]\n",
            "[0.01057363 0.00909975]\n",
            "[0.01160335 0.01400629]\n",
            "[0.00962534 0.01435374]\n",
            "[0.00550211 0.01088187]\n",
            "[0.00705245 0.0086756 ]\n",
            "[0.00867571 0.00855479]\n",
            "16 episode , 800 step , -11.70 Actor Loss, 4.76 Critic Loss,  0.04 Threshold , 4.45 Top reward, -41.06 Total reward\n",
            "[0.01184801 0.01409007]\n",
            "[0.00844071 0.01017076]\n",
            "[0.0040492  0.00938565]\n",
            "[0.00714134 0.00650944]\n",
            "[0.00808072 0.00723436]\n",
            "[0.00852877 0.01013962]\n",
            "[0.00792011 0.00666079]\n",
            "[0.00729654 0.00747431]\n",
            "17 episode , 800 step , -12.78 Actor Loss, 5.67 Critic Loss,  0.03 Threshold , 4.38 Top reward, -89.03 Total reward\n",
            "[0.0052581  0.00527483]\n",
            "[0.00522771 0.00868156]\n",
            "[0.00698275 0.00635371]\n",
            "[0.00473961 0.0037454 ]\n",
            "[0.00460367 0.00820432]\n",
            "[0.00612829 0.00566876]\n",
            "[0.00485995 0.00239104]\n",
            "[0.00735712 0.00407503]\n",
            "18 episode , 800 step , -13.95 Actor Loss, 4.84 Critic Loss,  0.02 Threshold , 4.19 Top reward, -35.00 Total reward\n",
            "[0.00596929 0.00781708]\n",
            "[0.00263738 0.00486676]\n",
            "[0.0044625  0.00305048]\n",
            "[0.00233116 0.00419468]\n",
            "[0.00480101 0.00548426]\n",
            "[0.00682253 0.00351538]\n",
            "[0.00417806 0.00426302]\n",
            "[0.00314082 0.00480513]\n",
            "19 episode , 800 step , -15.19 Actor Loss, 4.97 Critic Loss,  0.02 Threshold , 4.23 Top reward, -90.81 Total reward\n",
            "[0.00382552 0.00414085]\n",
            "[0.00645418 0.0034581 ]\n",
            "[0.00413168 0.00395347]\n",
            "[0.00552263 0.0056785 ]\n",
            "[0.0033417  0.00216532]\n",
            "[0.00649424 0.0052837 ]\n",
            "[0.00558801 0.00558484]\n",
            "[0.00550922 0.00480635]\n",
            "20 episode , 800 step , -16.08 Actor Loss, 4.77 Critic Loss,  0.02 Threshold , 4.94 Top reward, -33.13 Total reward\n",
            "[0.00425758 0.00263335]\n",
            "[0.0026103  0.00203375]\n",
            "[0.00487991 0.00343473]\n",
            "[0.00302955 0.0024997 ]\n",
            "[0.00168986 0.00462142]\n",
            "[0.00468318 0.00486379]\n",
            "[0.00371882 0.0048142 ]\n",
            "[0.00431128 0.0029882 ]\n",
            "21 episode , 800 step , -17.77 Actor Loss, 3.07 Critic Loss,  0.01 Threshold , 6.42 Top reward, -25.38 Total reward\n",
            "[0.00248473 0.00456351]\n",
            "[0.00248682 0.00258063]\n",
            "[0.00293784 0.00301858]\n",
            "[0.00243498 0.00215224]\n",
            "[0.00363575 0.00406466]\n",
            "[0.00147601 0.00172583]\n",
            "[0.00300797 0.00180682]\n",
            "[0.00335968 0.00393936]\n",
            "22 episode , 800 step , -19.49 Actor Loss, 2.96 Critic Loss,  0.01 Threshold , 5.08 Top reward, -31.96 Total reward\n",
            "[0.00350278 0.00458851]\n",
            "[0.00330862 0.0043691 ]\n",
            "[0.00345517 0.00250928]\n",
            "[0.00224528 0.00319971]\n",
            "[0.00471706 0.00397902]\n",
            "[0.003251   0.00356536]\n",
            "[0.00465414 0.00226413]\n",
            "[0.00281946 0.00344858]\n",
            "23 episode , 800 step , -20.73 Actor Loss, 2.47 Critic Loss,  0.01 Threshold , 4.48 Top reward, -34.24 Total reward\n",
            "[0.00231579 0.0035303 ]\n",
            "[0.00222104 0.00104779]\n",
            "[0.00368628 0.00074481]\n",
            "[0.0025013  0.00309242]\n",
            "[0.00436143 0.00315571]\n",
            "[0.00380383 0.00392927]\n",
            "[0.00360892 0.00135247]\n",
            "[0.00276343 0.00231736]\n",
            "24 episode , 800 step , -21.47 Actor Loss, 1.42 Critic Loss,  0.01 Threshold , 4.99 Top reward, -28.66 Total reward\n",
            "[0.00310208 0.00416917]\n",
            "[0.00197753 0.00199557]\n",
            "[0.00219355 0.00364053]\n",
            "[0.00300346 0.00261191]\n",
            "[0.00309519 0.00269782]\n",
            "[0.00236533 0.00197288]\n",
            "[0.00218286 0.00212824]\n",
            "[0.0022777  0.00117533]\n",
            "25 episode , 800 step , -21.78 Actor Loss, 1.47 Critic Loss,  0.01 Threshold , 5.97 Top reward, 20.88 Total reward\n",
            "[0.00245049 0.00205144]\n",
            "[0.00380247 0.00175072]\n",
            "[0.00232984 0.00299412]\n",
            "[0.00307056 0.00218736]\n",
            "[0.00402009 0.00250663]\n",
            "[0.00303121 0.00282429]\n",
            "[0.00284321 0.00114955]\n",
            "[0.00127673 0.00366353]\n",
            "26 episode , 800 step , -22.03 Actor Loss, 1.59 Critic Loss,  0.01 Threshold , 4.71 Top reward, -37.51 Total reward\n",
            "[0.00277054 0.00328787]\n",
            "[0.00276125 0.00293208]\n",
            "[0.00197869 0.00325385]\n",
            "[0.00340477 0.00165505]\n",
            "[0.00226749 0.0018946 ]\n",
            "[0.00300108 0.00186522]\n",
            "[0.00111015 0.00315009]\n",
            "[0.00354772 0.00284284]\n",
            "27 episode , 800 step , -22.43 Actor Loss, 1.14 Critic Loss,  0.01 Threshold , 5.51 Top reward, -54.46 Total reward\n",
            "[0.00448102 0.00106815]\n",
            "[0.00317611 0.00161507]\n",
            "[0.00243928 0.00370662]\n",
            "[0.00381159 0.00212575]\n",
            "[0.0016889  0.00361031]\n",
            "[0.00186044 0.00343827]\n",
            "[0.00255238 0.00406007]\n",
            "[0.00220145 0.00232801]\n",
            "28 episode , 800 step , -22.49 Actor Loss, 1.29 Critic Loss,  0.01 Threshold , 5.11 Top reward, 11.61 Total reward\n",
            "[0.00330081 0.00220654]\n",
            "[0.00260753 0.0023271 ]\n",
            "[0.00378024 0.00151332]\n",
            "[0.00205014 0.00236757]\n",
            "[0.0025067  0.00297349]\n",
            "[0.00367013 0.00283754]\n",
            "[0.00204106 0.00333962]\n",
            "[0.00324992 0.00409681]\n",
            "29 episode , 800 step , -22.72 Actor Loss, 1.31 Critic Loss,  0.01 Threshold , 5.04 Top reward, -40.01 Total reward\n",
            "[0.00331207 0.00408802]\n",
            "[0.00203823 0.00230567]\n",
            "[0.00186354 0.00267171]\n",
            "[0.0026734  0.00219569]\n",
            "[0.00384485 0.00273978]\n",
            "[0.00195175 0.00329624]\n",
            "[0.00264347 0.00277993]\n",
            "[0.00211059 0.00187166]\n",
            "30 episode , 800 step , -22.66 Actor Loss, 1.16 Critic Loss,  0.01 Threshold , 5.36 Top reward, -21.25 Total reward\n",
            "[0.00359946 0.00262163]\n",
            "[0.00056573 0.00260073]\n",
            "[0.00295891 0.0020244 ]\n",
            "[0.0019546  0.00256798]\n",
            "31 episode , 404 step , -22.09 Actor Loss, 1.25 Critic Loss,  0.01 Threshold , 100.00 Top reward, 232.96 Total reward\n",
            "[0.00129845 0.00193113]\n",
            "[0.00269912 0.00355173]\n",
            "[0.00274967 0.00127764]\n",
            "[0.00348454 0.00150216]\n",
            "[0.00116815 0.00233302]\n",
            "[0.00319323 0.00262163]\n",
            "[0.00248344 0.00395622]\n",
            "[0.003386   0.00286814]\n",
            "32 episode , 800 step , -21.85 Actor Loss, 1.83 Critic Loss,  0.01 Threshold , 5.51 Top reward, 10.10 Total reward\n",
            "[0.00168415 0.00317727]\n",
            "[0.00269952 0.00216936]\n",
            "[0.00191992 0.00337413]\n",
            "[0.00218103 0.00170902]\n",
            "[0.00205399 0.00197981]\n",
            "[0.00108723 0.00120832]\n",
            "[0.00237092 0.002557  ]\n",
            "[0.00156579 0.00296218]\n",
            "33 episode , 800 step , -21.38 Actor Loss, 1.90 Critic Loss,  0.01 Threshold , 5.81 Top reward, 29.42 Total reward\n",
            "[0.00326757 0.00286143]\n",
            "[0.00362321 0.00101429]\n",
            "[0.00287602 0.00298663]\n",
            "[0.00170673 0.00151909]\n",
            "[0.00371479 0.00097977]\n",
            "[0.00219661 0.00199792]\n",
            "[0.00258466 0.00213723]\n",
            "[0.00246549 0.00201306]\n",
            "34 episode , 800 step , -21.18 Actor Loss, 1.96 Critic Loss,  0.01 Threshold , 11.82 Top reward, 60.23 Total reward\n",
            "[0.00354949 0.00224838]\n",
            "35 episode , 123 step , -21.20 Actor Loss, 2.59 Critic Loss,  0.01 Threshold , 20.11 Top reward, 23.13 Total reward\n",
            "[0.00192768 0.00322545]\n",
            "36 episode , 155 step , -21.10 Actor Loss, 4.73 Critic Loss,  0.01 Threshold , 4.82 Top reward, -502.38 Total reward\n",
            "[0.00246384 0.00216406]\n",
            "37 episode , 89 step , -20.80 Actor Loss, 5.47 Critic Loss,  0.01 Threshold , 1.49 Top reward, -808.35 Total reward\n",
            "[0.00301121 0.00221678]\n",
            "[0.00214665 0.00359617]\n",
            "[0.00229299 0.00324803]\n",
            "[0.00248626 0.00363119]\n",
            "[0.00287214 0.00318135]\n",
            "[0.00306575 0.00184994]\n",
            "[0.00277527 0.00248509]\n",
            "38 episode , 638 step , -20.45 Actor Loss, 6.26 Critic Loss,  0.01 Threshold , 100.00 Top reward, 97.65 Total reward\n",
            "[0.00369116 0.00289066]\n",
            "39 episode , 136 step , -20.63 Actor Loss, 4.34 Critic Loss,  0.01 Threshold , 8.31 Top reward, -420.46 Total reward\n",
            "[0.00231825 0.00235367]\n",
            "[0.00273956 0.00201626]\n",
            "40 episode , 221 step , -20.68 Actor Loss, 8.21 Critic Loss,  0.01 Threshold , 11.55 Top reward, -253.50 Total reward\n",
            "[0.00221427 0.00239196]\n",
            "[0.00254457 0.00198868]\n",
            "[0.00298043 0.0018379 ]\n",
            "[0.00372536 0.00238325]\n",
            "[0.00149248 0.00314159]\n",
            "[0.00157295 0.00357033]\n",
            "[0.00198417 0.00241488]\n",
            "[0.00263356 0.00293708]\n",
            "41 episode , 800 step , -20.04 Actor Loss, 8.60 Critic Loss,  0.01 Threshold , 5.12 Top reward, -93.60 Total reward\n",
            "[0.00243016 0.00152988]\n",
            "[0.0016876  0.00157606]\n",
            "[0.00358595 0.00192843]\n",
            "[0.00361084 0.00225173]\n",
            "[0.00441663 0.00306428]\n",
            "42 episode , 451 step , -20.20 Actor Loss, 5.99 Critic Loss,  0.01 Threshold , 100.00 Top reward, 174.29 Total reward\n",
            "[0.00272254 0.00221848]\n",
            "[0.00357659 0.00289466]\n",
            "[0.00181832 0.00151509]\n",
            "[0.00219816 0.00344426]\n",
            "[0.00146946 0.00260223]\n",
            "[0.00274787 0.00373621]\n",
            "[0.00272356 0.00183115]\n",
            "[0.00357174 0.00307315]\n",
            "43 episode , 800 step , -20.54 Actor Loss, 6.00 Critic Loss,  0.01 Threshold , 6.08 Top reward, 13.22 Total reward\n",
            "[0.00217855 0.00089009]\n",
            "[0.00162252 0.00346554]\n",
            "[0.00347149 0.00286033]\n",
            "[0.0027651  0.00179231]\n",
            "[0.00102277 0.00199149]\n",
            "[0.00351378 0.00183229]\n",
            "[0.00186278 0.00172546]\n",
            "[0.00254182 0.00211677]\n",
            "44 episode , 800 step , -20.62 Actor Loss, 6.67 Critic Loss,  0.01 Threshold , 11.96 Top reward, 23.56 Total reward\n",
            "[0.00193167 0.00131913]\n",
            "[0.00394897 0.00234857]\n",
            "[0.00290923 0.00140549]\n",
            "[0.00129295 0.00341535]\n",
            "[0.00289558 0.00156919]\n",
            "[0.00302125 0.00163334]\n",
            "[0.00229851 0.00131084]\n",
            "[0.00316781 0.0030196 ]\n",
            "45 episode , 800 step , -21.20 Actor Loss, 6.23 Critic Loss,  0.01 Threshold , 4.84 Top reward, -3.56 Total reward\n",
            "[0.00343059 0.00256249]\n",
            "[0.00260248 0.00342469]\n",
            "[0.00287338 0.00241013]\n",
            "[0.00162667 0.00338366]\n",
            "46 episode , 423 step , -21.52 Actor Loss, 5.82 Critic Loss,  0.01 Threshold , 5.31 Top reward, -89.71 Total reward\n",
            "[0.00300399 0.00230551]\n",
            "[0.00196978 0.00240534]\n",
            "[0.00257999 0.00153237]\n",
            "[0.00283139 0.00312133]\n",
            "[0.00259225 0.00273604]\n",
            "[0.00335127 0.00290694]\n",
            "47 episode , 608 step , -21.31 Actor Loss, 8.23 Critic Loss,  0.01 Threshold , 100.00 Top reward, 137.14 Total reward\n",
            "[0.00169292 0.00267999]\n",
            "[0.00286721 0.00179151]\n",
            "[0.00356704 0.00316039]\n",
            "[0.002999   0.00270083]\n",
            "[0.00194573 0.00219221]\n",
            "[0.00360979 0.00329888]\n",
            "[0.00287025 0.00355641]\n",
            "[0.00269009 0.00240045]\n",
            "48 episode , 800 step , -22.07 Actor Loss, 7.99 Critic Loss,  0.01 Threshold , 6.37 Top reward, 9.23 Total reward\n",
            "[0.00155204 0.00222146]\n",
            "[0.00277678 0.00381779]\n",
            "49 episode , 151 step , -22.60 Actor Loss, 9.38 Critic Loss,  0.01 Threshold , 3.14 Top reward, -101.89 Total reward\n",
            "[0.00115972 0.00249214]\n",
            "[0.00254081 0.00135705]\n",
            "[0.00336702 0.00273324]\n",
            "[0.0024874  0.00228916]\n",
            "[0.0036365  0.00310368]\n",
            "[0.00177514 0.00349827]\n",
            "[0.00165243 0.00261976]\n",
            "[0.00234561 0.0036151 ]\n",
            "50 episode , 800 step , -23.28 Actor Loss, 9.54 Critic Loss,  0.01 Threshold , 4.54 Top reward, -9.58 Total reward\n",
            "[0.00265275 0.00172787]\n",
            "[0.0026974  0.00185723]\n",
            "[0.00314094 0.00167   ]\n",
            "[0.00321786 0.00342037]\n",
            "[0.00264582 0.00096177]\n",
            "[0.00272419 0.00247564]\n",
            "[0.00304861 0.00255633]\n",
            "51 episode , 756 step , -23.61 Actor Loss, 9.27 Critic Loss,  0.01 Threshold , 5.62 Top reward, -162.88 Total reward\n",
            "[0.00246885 0.00295366]\n",
            "[0.00237631 0.0030457 ]\n",
            "[0.00151851 0.00204286]\n",
            "[0.00162322 0.00255131]\n",
            "[0.00110151 0.00135708]\n",
            "[0.00321884 0.00261435]\n",
            "52 episode , 550 step , -24.41 Actor Loss, 9.56 Critic Loss,  0.01 Threshold , 10.86 Top reward, -104.86 Total reward\n",
            "[0.00193266 0.00334444]\n",
            "[0.00221609 0.00214738]\n",
            "[0.00349408 0.00264933]\n",
            "[0.00248778 0.00151436]\n",
            "[0.00258999 0.00146487]\n",
            "53 episode , 516 step , -24.85 Actor Loss, 10.65 Critic Loss,  0.01 Threshold , 100.00 Top reward, 135.32 Total reward\n",
            "[0.00308138 0.00329044]\n",
            "[0.00292486 0.00092995]\n",
            "[0.00196376 0.00301344]\n",
            "[0.00263763 0.00293674]\n",
            "[0.00366972 0.0025441 ]\n",
            "54 episode , 567 step , -25.54 Actor Loss, 11.34 Critic Loss,  0.01 Threshold , 29.52 Top reward, -181.90 Total reward\n",
            "[0.0028559  0.00321895]\n",
            "[0.00367577 0.00208422]\n",
            "[0.00203083 0.0023711 ]\n",
            "[0.00241175 0.0008019 ]\n",
            "[0.00382061 0.00388728]\n",
            "55 episode , 478 step , -28.18 Actor Loss, 10.30 Critic Loss,  0.01 Threshold , 100.00 Top reward, 143.42 Total reward\n",
            "[0.00235653 0.00310476]\n",
            "[0.00348241 0.00321857]\n",
            "[0.0027208  0.00238428]\n",
            "[0.00214583 0.00322766]\n",
            "56 episode , 340 step , -30.18 Actor Loss, 12.09 Critic Loss,  0.01 Threshold , 100.00 Top reward, 162.63 Total reward\n",
            "[0.00251751 0.00259564]\n",
            "[0.00320909 0.0009813 ]\n",
            "[0.00325203 0.00188793]\n",
            "[0.00294292 0.00288758]\n",
            "57 episode , 416 step , -31.91 Actor Loss, 12.25 Critic Loss,  0.01 Threshold , 100.00 Top reward, 174.68 Total reward\n",
            "[0.00144633 0.00176883]\n",
            "[0.00269168 0.00300902]\n",
            "58 episode , 230 step , -33.92 Actor Loss, 8.68 Critic Loss,  0.01 Threshold , 100.00 Top reward, 216.51 Total reward\n",
            "[0.00345355 0.00355914]\n",
            "59 episode , 81 step , -34.10 Actor Loss, 12.47 Critic Loss,  0.01 Threshold , 13.95 Top reward, -110.29 Total reward\n",
            "[0.00313493 0.00290039]\n",
            "60 episode , 112 step , -34.33 Actor Loss, 16.71 Critic Loss,  0.01 Threshold , 4.33 Top reward, -104.82 Total reward\n",
            "[0.00198101 0.00134718]\n",
            "[0.00208071 0.00135209]\n",
            "[0.00203338 0.00322898]\n",
            "[0.0033078  0.00354953]\n",
            "[0.00308275 0.00168471]\n",
            "[0.00324382 0.00247617]\n",
            "[0.0012783 0.002168 ]\n",
            "[0.00200221 0.00314828]\n",
            "61 episode , 800 step , -35.31 Actor Loss, 12.83 Critic Loss,  0.01 Threshold , 3.53 Top reward, -47.34 Total reward\n",
            "[0.00146414 0.00249303]\n",
            "62 episode , 65 step , -36.01 Actor Loss, 8.31 Critic Loss,  0.01 Threshold , 28.14 Top reward, -228.44 Total reward\n",
            "[0.00326115 0.00223494]\n",
            "[0.00270863 0.00185256]\n",
            "[0.0014797  0.00226421]\n",
            "[0.00186426 0.0024525 ]\n",
            "[0.00223321 0.00252996]\n",
            "[0.00328744 0.00229063]\n",
            "[0.00283397 0.00202409]\n",
            "[0.00325795 0.00146419]\n",
            "63 episode , 800 step , -35.89 Actor Loss, 13.32 Critic Loss,  0.01 Threshold , 3.71 Top reward, -48.81 Total reward\n",
            "[0.00224494 0.00267024]\n",
            "[0.00271451 0.0007271 ]\n",
            "[0.00246955 0.00339336]\n",
            "[0.00185857 0.00314688]\n",
            "64 episode , 483 step , -36.16 Actor Loss, 13.48 Critic Loss,  0.01 Threshold , 5.23 Top reward, -131.73 Total reward\n",
            "[0.0033714 0.0012881]\n",
            "[0.00172357 0.00164371]\n",
            "[0.00275026 0.00244601]\n",
            "[0.00132126 0.00346092]\n",
            "[0.00147111 0.00312501]\n",
            "[0.00344554 0.00184744]\n",
            "[0.00146442 0.0020186 ]\n",
            "[0.00147797 0.00370876]\n",
            "65 episode , 800 step , -36.07 Actor Loss, 10.01 Critic Loss,  0.01 Threshold , 5.12 Top reward, 9.61 Total reward\n",
            "[0.00271251 0.00303445]\n",
            "[0.00235022 0.00125969]\n",
            "[0.00230147 0.0036355 ]\n",
            "[0.00277437 0.0025967 ]\n",
            "[0.00175166 0.00190625]\n",
            "[0.00295927 0.00209178]\n",
            "[0.00200901 0.00337159]\n",
            "[0.00197967 0.00385203]\n",
            "66 episode , 800 step , -36.38 Actor Loss, 11.50 Critic Loss,  0.01 Threshold , 25.37 Top reward, 8.74 Total reward\n",
            "[0.00225749 0.00221291]\n",
            "67 episode , 86 step , -36.57 Actor Loss, 9.08 Critic Loss,  0.01 Threshold , 1.96 Top reward, -310.07 Total reward\n",
            "[0.00160159 0.00213014]\n",
            "[0.00347567 0.00347505]\n",
            "[0.00120307 0.00176437]\n",
            "[0.00182318 0.00231061]\n",
            "[0.00285728 0.00146109]\n",
            "[0.00223308 0.00234183]\n",
            "[0.00229378 0.0027574 ]\n",
            "[0.00151616 0.00163816]\n",
            "68 episode , 800 step , -35.15 Actor Loss, 9.85 Critic Loss,  0.01 Threshold , 6.23 Top reward, -10.31 Total reward\n",
            "[0.00281986 0.00294021]\n",
            "[0.0018269  0.00340389]\n",
            "[0.00258106 0.00415426]\n",
            "[0.00233532 0.00249947]\n",
            "[0.00219212 0.00367245]\n",
            "[0.00340729 0.00227425]\n",
            "[0.00106682 0.00262001]\n",
            "[0.00312287 0.00258286]\n",
            "69 episode , 800 step , -34.70 Actor Loss, 9.42 Critic Loss,  0.01 Threshold , 4.13 Top reward, -56.13 Total reward\n",
            "[0.00307962 0.00107164]\n",
            "[0.00291364 0.00213194]\n",
            "[0.00253892 0.00182534]\n",
            "[0.00233375 0.00236239]\n",
            "[0.00362976 0.00254507]\n",
            "[0.00174363 0.00294289]\n",
            "[0.00176363 0.0027209 ]\n",
            "[0.00202253 0.00335915]\n",
            "70 episode , 800 step , -34.16 Actor Loss, 7.41 Critic Loss,  0.01 Threshold , 6.51 Top reward, -48.58 Total reward\n",
            "[0.00307564 0.00243233]\n",
            "[0.00208319 0.00233121]\n",
            "[0.00113861 0.00196927]\n",
            "[0.0018046  0.00114659]\n",
            "[0.00276957 0.0025213 ]\n",
            "[0.00419657 0.00354533]\n",
            "[0.00233107 0.00225522]\n",
            "[0.0029199  0.00283377]\n",
            "71 episode , 800 step , -33.93 Actor Loss, 7.94 Critic Loss,  0.01 Threshold , 7.60 Top reward, -5.48 Total reward\n",
            "[0.00356871 0.00348282]\n",
            "[0.00339268 0.00311858]\n",
            "[0.00294475 0.00343475]\n",
            "[0.00206424 0.00290712]\n",
            "72 episode , 340 step , -33.89 Actor Loss, 6.13 Critic Loss,  0.01 Threshold , 10.43 Top reward, -33.25 Total reward\n",
            "[0.0028557  0.00289506]\n",
            "[0.00190173 0.00209342]\n",
            "[0.00334324 0.00151823]\n",
            "[0.00105709 0.0022026 ]\n",
            "[0.00158833 0.00364563]\n",
            "[0.0016113  0.00257954]\n",
            "[0.00325552 0.00143474]\n",
            "[0.00346537 0.00159955]\n",
            "73 episode , 800 step , -33.87 Actor Loss, 8.33 Critic Loss,  0.01 Threshold , 25.17 Top reward, 126.33 Total reward\n",
            "[0.0029712  0.00274223]\n",
            "[0.00308002 0.00284729]\n",
            "[0.00115726 0.00421048]\n",
            "[0.00239649 0.00332035]\n",
            "[0.00366511 0.00309745]\n",
            "74 episode , 530 step , -34.08 Actor Loss, 7.34 Critic Loss,  0.01 Threshold , 2.69 Top reward, -253.99 Total reward\n",
            "[0.0034513  0.00294405]\n",
            "[0.00217394 0.00230509]\n",
            "[0.00263485 0.00314334]\n",
            "[0.00315131 0.00246267]\n",
            "[0.00328515 0.00287641]\n",
            "[0.00213368 0.00265199]\n",
            "[0.00184771 0.00114412]\n",
            "[0.00269821 0.00164566]\n",
            "75 episode , 800 step , -33.92 Actor Loss, 5.99 Critic Loss,  0.01 Threshold , 3.90 Top reward, -40.54 Total reward\n",
            "[0.00221623 0.00135822]\n",
            "[0.00283018 0.0019085 ]\n",
            "[0.00312338 0.0035861 ]\n",
            "[0.00241782 0.00311314]\n",
            "[0.00220348 0.00307024]\n",
            "[0.0019317  0.00294535]\n",
            "[0.00347285 0.00111267]\n",
            "[0.00210683 0.00211876]\n",
            "76 episode , 800 step , -33.70 Actor Loss, 4.55 Critic Loss,  0.01 Threshold , 5.98 Top reward, 33.69 Total reward\n",
            "[0.00318025 0.00244773]\n",
            "[0.00104844 0.00278358]\n",
            "[0.00281493 0.00287009]\n",
            "[0.00327395 0.00103365]\n",
            "[0.00203083 0.00294965]\n",
            "[0.00208472 0.00303515]\n",
            "[0.00221158 0.00226878]\n",
            "[0.00205257 0.00362017]\n",
            "77 episode , 800 step , -33.51 Actor Loss, 3.58 Critic Loss,  0.01 Threshold , 4.46 Top reward, 11.11 Total reward\n",
            "[0.00187106 0.00409981]\n",
            "[0.0025011  0.00261636]\n",
            "[0.00197007 0.00318566]\n",
            "[0.00383282 0.00313719]\n",
            "[0.00155208 0.00256389]\n",
            "[0.00387253 0.00263156]\n",
            "[0.0012038  0.00242719]\n",
            "[0.00173986 0.00310977]\n",
            "78 episode , 800 step , -33.51 Actor Loss, 3.15 Critic Loss,  0.01 Threshold , 5.02 Top reward, 19.73 Total reward\n",
            "[0.00344001 0.00221003]\n",
            "[0.00234522 0.00268009]\n",
            "79 episode , 222 step , -33.35 Actor Loss, 3.76 Critic Loss,  0.01 Threshold , 4.98 Top reward, -636.97 Total reward\n",
            "[0.00296472 0.00383136]\n",
            "[0.0018662 0.0029039]\n",
            "[0.00276221 0.00369541]\n",
            "[0.00228635 0.0024488 ]\n",
            "[0.0016487  0.00267452]\n",
            "[0.00206021 0.00102721]\n",
            "[0.00197288 0.00359029]\n",
            "[0.002026  0.0027373]\n",
            "80 episode , 800 step , -32.05 Actor Loss, 6.09 Critic Loss,  0.01 Threshold , 11.16 Top reward, 25.90 Total reward\n",
            "[0.00300599 0.00194988]\n",
            "[0.00196612 0.0021554 ]\n",
            "[0.00257535 0.00243509]\n",
            "[0.00292379 0.00251197]\n",
            "[0.00338611 0.00318593]\n",
            "[0.00257023 0.00249266]\n",
            "[0.0025033  0.00230504]\n",
            "[0.00220919 0.00156748]\n",
            "81 episode , 800 step , -31.64 Actor Loss, 3.48 Critic Loss,  0.01 Threshold , 22.58 Top reward, 52.27 Total reward\n",
            "[0.00414447 0.00191126]\n",
            "82 episode , 104 step , -31.79 Actor Loss, 4.26 Critic Loss,  0.01 Threshold , 3.75 Top reward, -217.95 Total reward\n",
            "[0.00174698 0.00214554]\n",
            "[0.00372091 0.00122648]\n",
            "[0.0020819  0.00215652]\n",
            "[0.00121533 0.00299564]\n",
            "[0.00323468 0.00177067]\n",
            "83 episode , 437 step , -33.20 Actor Loss, 4.22 Critic Loss,  0.01 Threshold , 11.34 Top reward, -695.78 Total reward\n",
            "[0.00200513 0.00255439]\n",
            "[0.0038121  0.00186958]\n",
            "[0.00230824 0.00294321]\n",
            "[0.00203185 0.00364839]\n",
            "[0.00212532 0.00303208]\n",
            "[0.00190482 0.00232022]\n",
            "[0.0026267  0.00240167]\n",
            "[0.00287408 0.00273291]\n",
            "84 episode , 800 step , -33.04 Actor Loss, 4.57 Critic Loss,  0.01 Threshold , 15.80 Top reward, 101.06 Total reward\n",
            "[0.00210767 0.00204117]\n",
            "[0.00203415 0.00147344]\n",
            "[0.0027192  0.00189968]\n",
            "[0.00228579 0.00325604]\n",
            "[0.00145438 0.00247765]\n",
            "[0.00298878 0.00286507]\n",
            "[0.00426941 0.00330621]\n",
            "[0.00141042 0.00269809]\n",
            "85 episode , 800 step , -33.03 Actor Loss, 4.28 Critic Loss,  0.01 Threshold , 14.02 Top reward, 163.63 Total reward\n",
            "[0.00214809 0.00189633]\n",
            "86 episode , 112 step , -33.01 Actor Loss, 7.19 Critic Loss,  0.01 Threshold , 53.59 Top reward, -17.43 Total reward\n",
            "87 episode , 68 step , -32.43 Actor Loss, 6.52 Critic Loss,  0.01 Threshold , 13.25 Top reward, 15.60 Total reward\n",
            "[0.0031769 0.0021805]\n",
            "[0.00070177 0.00268945]\n",
            "[0.0030481  0.00377399]\n",
            "[0.00346621 0.00328571]\n",
            "[0.00291068 0.00309921]\n",
            "[0.00297957 0.00231096]\n",
            "[0.00143119 0.00346674]\n",
            "88 episode , 622 step , -32.28 Actor Loss, 7.60 Critic Loss,  0.01 Threshold , 100.00 Top reward, 206.53 Total reward\n",
            "[0.00152844 0.00128194]\n",
            "[0.00196457 0.00188189]\n",
            "89 episode , 189 step , -32.49 Actor Loss, 5.86 Critic Loss,  0.01 Threshold , 100.00 Top reward, 214.75 Total reward\n",
            "[0.00303649 0.00396453]\n",
            "[0.00224292 0.00295527]\n",
            "[0.00293547 0.00107961]\n",
            "[0.00146072 0.00258425]\n",
            "[0.00151974 0.0033229 ]\n",
            "[0.00250789 0.00222942]\n",
            "[0.00168084 0.00386485]\n",
            "[0.00181301 0.00357221]\n",
            "90 episode , 800 step , -31.97 Actor Loss, 8.61 Critic Loss,  0.01 Threshold , 15.82 Top reward, 147.77 Total reward\n",
            "[0.00234015 0.00138096]\n",
            "[0.00313397 0.0017839 ]\n",
            "91 episode , 226 step , -31.92 Actor Loss, 7.90 Critic Loss,  0.01 Threshold , 8.51 Top reward, -220.43 Total reward\n",
            "[0.00218445 0.00215748]\n",
            "92 episode , 89 step , -31.73 Actor Loss, 13.21 Critic Loss,  0.01 Threshold , 19.03 Top reward, -32.01 Total reward\n",
            "[0.00141826 0.0026153 ]\n",
            "[0.0030626  0.00266004]\n",
            "[0.00113072 0.00179278]\n",
            "93 episode , 297 step , -31.65 Actor Loss, 9.15 Critic Loss,  0.01 Threshold , 14.92 Top reward, -309.70 Total reward\n",
            "[0.00333516 0.00285175]\n",
            "[0.00272173 0.00213856]\n",
            "[0.00107702 0.0036069 ]\n",
            "[0.0029228  0.00272361]\n",
            "[0.00279966 0.00280328]\n",
            "[0.0019262  0.00289597]\n",
            "[0.00233725 0.00179986]\n",
            "[0.00224821 0.00234839]\n",
            "94 episode , 800 step , -30.85 Actor Loss, 12.05 Critic Loss,  0.01 Threshold , 20.50 Top reward, 140.51 Total reward\n",
            "[0.00113027 0.00315512]\n",
            "95 episode , 137 step , -30.63 Actor Loss, 10.75 Critic Loss,  0.01 Threshold , 14.82 Top reward, -135.78 Total reward\n",
            "[0.00243407 0.00263348]\n",
            "96 episode , 132 step , -30.48 Actor Loss, 9.05 Critic Loss,  0.01 Threshold , 16.71 Top reward, -84.99 Total reward\n",
            "[0.00261358 0.00286935]\n",
            "[0.00186289 0.00288265]\n",
            "[0.00159396 0.00282191]\n",
            "97 episode , 312 step , -30.68 Actor Loss, 12.88 Critic Loss,  0.01 Threshold , 5.98 Top reward, -154.98 Total reward\n",
            "[0.00208768 0.00262202]\n",
            "[0.00188223 0.00195402]\n",
            "[0.00295201 0.0017267 ]\n",
            "[0.00319832 0.0017297 ]\n",
            "[0.00253511 0.0035129 ]\n",
            "[0.00391164 0.00344468]\n",
            "98 episode , 543 step , -30.66 Actor Loss, 11.11 Critic Loss,  0.01 Threshold , 11.19 Top reward, -341.50 Total reward\n",
            "[0.00211258 0.00361534]\n",
            "[0.00277787 0.00233443]\n",
            "[0.00248055 0.00232436]\n",
            "[0.00245673 0.00194675]\n",
            "[0.00290051 0.00250236]\n",
            "[0.00252866 0.00141524]\n",
            "99 episode , 565 step , -30.11 Actor Loss, 10.49 Critic Loss,  0.01 Threshold , 3.83 Top reward, -137.06 Total reward\n",
            "[0.00224332 0.00179336]\n",
            "[0.00323635 0.00203506]\n",
            "[0.00283509 0.00286577]\n",
            "100 episode , 297 step , -29.64 Actor Loss, 7.63 Critic Loss,  0.01 Threshold , 3.85 Top reward, -115.10 Total reward\n",
            "[0.00263052 0.00263296]\n",
            "[0.00175847 0.00123296]\n",
            "101 episode , 253 step , -29.32 Actor Loss, 10.15 Critic Loss,  0.01 Threshold , 4.23 Top reward, -100.25 Total reward\n",
            "[0.0033826  0.00342195]\n",
            "[0.00227676 0.00274764]\n",
            "[0.00188777 0.00225767]\n",
            "[0.00160414 0.00247949]\n",
            "[0.00360005 0.00275453]\n",
            "[0.00165056 0.00372763]\n",
            "102 episode , 545 step , -29.26 Actor Loss, 12.01 Critic Loss,  0.01 Threshold , 3.87 Top reward, -195.55 Total reward\n",
            "[0.00325137 0.00178478]\n",
            "[0.00305116 0.00179247]\n",
            "103 episode , 293 step , -29.42 Actor Loss, 11.01 Critic Loss,  0.01 Threshold , 4.46 Top reward, -112.91 Total reward\n",
            "[0.00385553 0.00211539]\n",
            "[0.00149854 0.00376286]\n",
            "[0.00210471 0.00148624]\n",
            "[0.00263364 0.00140278]\n",
            "[0.00221022 0.00350615]\n",
            "[0.0021473  0.00118355]\n",
            "[0.0033078  0.00375705]\n",
            "[0.0031197  0.00318323]\n",
            "104 episode , 800 step , -30.19 Actor Loss, 9.20 Critic Loss,  0.01 Threshold , 3.19 Top reward, -90.74 Total reward\n",
            "[0.00085405 0.00229597]\n",
            "105 episode , 68 step , -30.27 Actor Loss, 9.16 Critic Loss,  0.01 Threshold , 9.85 Top reward, -17.32 Total reward\n",
            "[0.00246396 0.00256817]\n",
            "[0.00192659 0.00298223]\n",
            "[0.00303878 0.00323101]\n",
            "[0.001086   0.00305705]\n",
            "[0.00369203 0.00135755]\n",
            "[0.00153371 0.00209815]\n",
            "[0.00279995 0.00216433]\n",
            "[0.00257829 0.00345777]\n",
            "106 episode , 800 step , -30.14 Actor Loss, 9.82 Critic Loss,  0.01 Threshold , 3.54 Top reward, -54.24 Total reward\n",
            "[0.00296388 0.00252254]\n",
            "[0.00162148 0.00324095]\n",
            "[0.00192808 0.00149264]\n",
            "[0.00212194 0.00263274]\n",
            "[0.00265102 0.00309274]\n",
            "[0.00199333 0.00344566]\n",
            "[0.00299093 0.00263538]\n",
            "[0.00163428 0.00362738]\n",
            "107 episode , 800 step , -30.83 Actor Loss, 7.60 Critic Loss,  0.01 Threshold , 3.00 Top reward, -20.59 Total reward\n",
            "[0.00214064 0.00349182]\n",
            "[0.00181773 0.00233614]\n",
            "[0.00293112 0.00301137]\n",
            "[0.00205241 0.00211022]\n",
            "[0.0026731  0.00252476]\n",
            "[0.00226759 0.00196927]\n",
            "[0.00081046 0.00133474]\n",
            "[0.00338974 0.00311606]\n",
            "108 episode , 800 step , -31.57 Actor Loss, 6.78 Critic Loss,  0.01 Threshold , 3.93 Top reward, -10.89 Total reward\n",
            "[0.00336191 0.00173191]\n",
            "[0.00297565 0.00153978]\n",
            "[0.00340089 0.00257247]\n",
            "[0.00386372 0.00114164]\n",
            "[0.00336865 0.00217992]\n",
            "[0.00225203 0.00232916]\n",
            "[0.00211011 0.00315432]\n",
            "[0.00161535 0.00272919]\n",
            "109 episode , 800 step , -31.08 Actor Loss, 6.88 Critic Loss,  0.01 Threshold , 4.88 Top reward, -19.55 Total reward\n",
            "[0.00336508 0.00264001]\n",
            "[0.00260468 0.0028601 ]\n",
            "[0.00245645 0.00208871]\n",
            "[0.00165567 0.00234477]\n",
            "[0.00167035 0.00138421]\n",
            "[0.00218863 0.00345063]\n",
            "[0.00197679 0.00209444]\n",
            "[0.00227053 0.00205145]\n",
            "110 episode , 800 step , -31.28 Actor Loss, 5.56 Critic Loss,  0.01 Threshold , 4.18 Top reward, -30.04 Total reward\n",
            "[0.00258508 0.00335978]\n",
            "[0.00209944 0.00284571]\n",
            "[0.0038713 0.0033215]\n",
            "[0.00195887 0.00267214]\n",
            "[0.00129198 0.0032899 ]\n",
            "[0.00229886 0.00388946]\n",
            "[0.00218299 0.00235691]\n",
            "[0.00159021 0.00158524]\n",
            "111 episode , 800 step , -31.21 Actor Loss, 4.16 Critic Loss,  0.01 Threshold , 3.59 Top reward, -66.74 Total reward\n",
            "[0.00118908 0.00365581]\n",
            "[0.00252496 0.00189432]\n",
            "[0.00319612 0.0037738 ]\n",
            "[0.00071261 0.0028367 ]\n",
            "[0.00280795 0.00239824]\n",
            "[0.00237296 0.00328666]\n",
            "[0.00229466 0.00280246]\n",
            "[0.00334581 0.00275935]\n",
            "112 episode , 800 step , -32.20 Actor Loss, 3.80 Critic Loss,  0.01 Threshold , 3.60 Top reward, -40.11 Total reward\n",
            "[0.00307176 0.00141221]\n",
            "[0.00255824 0.00352841]\n",
            "[0.00263238 0.00095905]\n",
            "[0.00282003 0.00134726]\n",
            "[0.00286285 0.00224081]\n",
            "[0.00182436 0.00206943]\n",
            "[0.00049134 0.00169381]\n",
            "[0.00211808 0.0010923 ]\n",
            "113 episode , 800 step , -31.74 Actor Loss, 3.08 Critic Loss,  0.01 Threshold , 3.84 Top reward, -27.68 Total reward\n",
            "[0.00229841 0.00333983]\n",
            "[0.00158422 0.00348225]\n",
            "[0.00350514 0.00148036]\n",
            "[0.00090734 0.00180541]\n",
            "[0.0027406  0.00153861]\n",
            "[0.00284614 0.00276925]\n",
            "[0.00287405 0.00359787]\n",
            "[0.00160862 0.00256151]\n",
            "114 episode , 800 step , -32.19 Actor Loss, 2.78 Critic Loss,  0.01 Threshold , 4.13 Top reward, -37.79 Total reward\n",
            "[0.00150515 0.00369444]\n",
            "[0.00202068 0.00159914]\n",
            "[0.00256985 0.00183895]\n",
            "[0.00306739 0.00340718]\n",
            "[0.00198313 0.0035485 ]\n",
            "[0.00236419 0.00343455]\n",
            "[0.00368723 0.00251001]\n",
            "[0.00248338 0.00090862]\n",
            "115 episode , 800 step , -32.55 Actor Loss, 1.97 Critic Loss,  0.01 Threshold , 4.11 Top reward, -5.69 Total reward\n",
            "[0.00352792 0.00232439]\n",
            "[0.00232204 0.00147066]\n",
            "[0.00255827 0.00308505]\n",
            "[0.00197395 0.00299319]\n",
            "[0.00259107 0.00137701]\n",
            "[0.00178224 0.002738  ]\n",
            "[0.00265331 0.00341638]\n",
            "116 episode , 668 step , -33.31 Actor Loss, 1.78 Critic Loss,  0.01 Threshold , 100.00 Top reward, 163.85 Total reward\n",
            "[0.00294121 0.00237631]\n",
            "[0.00098191 0.00242741]\n",
            "[0.00263421 0.00311627]\n",
            "[0.00108653 0.00322065]\n",
            "[0.00115814 0.00216652]\n",
            "[0.00174102 0.00301532]\n",
            "[0.00310589 0.00202349]\n",
            "[0.00430721 0.00180559]\n",
            "117 episode , 800 step , -34.76 Actor Loss, 2.68 Critic Loss,  0.01 Threshold , 11.01 Top reward, 47.00 Total reward\n",
            "[0.00279172 0.00186593]\n",
            "[0.00263627 0.00362605]\n",
            "[0.00201445 0.00261383]\n",
            "[0.00118294 0.00228049]\n",
            "[0.00247037 0.00309825]\n",
            "[0.00382092 0.00238127]\n",
            "[0.00173204 0.0017752 ]\n",
            "118 episode , 729 step , -35.80 Actor Loss, 2.57 Critic Loss,  0.01 Threshold , 100.00 Top reward, 147.36 Total reward\n",
            "[0.00132473 0.00351223]\n",
            "[0.00320215 0.00199766]\n",
            "[0.0025916  0.00280627]\n",
            "[0.00183921 0.00282858]\n",
            "[0.00254306 0.00237465]\n",
            "[0.00343805 0.00201451]\n",
            "119 episode , 623 step , -36.06 Actor Loss, 2.93 Critic Loss,  0.01 Threshold , 100.00 Top reward, 170.18 Total reward\n",
            "[0.00216601 0.00225737]\n",
            "[0.00365606 0.00296898]\n",
            "[0.0023594  0.00161057]\n",
            "[0.00287619 0.00272905]\n",
            "[0.00355211 0.00388286]\n",
            "[0.00307884 0.00312095]\n",
            "[0.00332733 0.00135597]\n",
            "[0.00330289 0.00265363]\n",
            "120 episode , 800 step , -35.18 Actor Loss, 3.80 Critic Loss,  0.01 Threshold , 3.97 Top reward, 1.10 Total reward\n",
            "[0.00264439 0.00233959]\n",
            "[0.00307502 0.00345285]\n",
            "[0.00270902 0.00228985]\n",
            "[0.00253563 0.00090412]\n",
            "[0.00275325 0.00244203]\n",
            "[0.00222998 0.00249176]\n",
            "[0.0015091  0.00261467]\n",
            "[0.00330469 0.0015931 ]\n",
            "121 episode , 800 step , -34.67 Actor Loss, 3.08 Critic Loss,  0.01 Threshold , 6.45 Top reward, -16.29 Total reward\n",
            "[0.00313078 0.001924  ]\n",
            "[0.00220342 0.00288384]\n",
            "[0.00229148 0.00149878]\n",
            "[0.00273419 0.00236991]\n",
            "[0.00202883 0.00294673]\n",
            "[0.00303333 0.00222962]\n",
            "[0.00229953 0.0024661 ]\n",
            "[0.00232641 0.00123934]\n",
            "122 episode , 800 step , -34.07 Actor Loss, 2.07 Critic Loss,  0.01 Threshold , 7.42 Top reward, 9.51 Total reward\n",
            "[0.00283263 0.00145252]\n",
            "123 episode , 106 step , -33.89 Actor Loss, 3.25 Critic Loss,  0.01 Threshold , 11.32 Top reward, -75.47 Total reward\n",
            "[0.0026735 0.0025164]\n",
            "124 episode , 80 step , -33.31 Actor Loss, 4.19 Critic Loss,  0.01 Threshold , 12.53 Top reward, 3.53 Total reward\n",
            "[0.00239275 0.0018841 ]\n",
            "125 episode , 53 step , -33.05 Actor Loss, 7.00 Critic Loss,  0.01 Threshold , 11.31 Top reward, -246.62 Total reward\n",
            "126 episode , 74 step , -32.69 Actor Loss, 5.74 Critic Loss,  0.01 Threshold , 11.91 Top reward, -192.57 Total reward\n",
            "[0.00210364 0.00276756]\n",
            "[0.00331581 0.0025196 ]\n",
            "[0.0026214  0.00225805]\n",
            "[0.00273218 0.00149478]\n",
            "127 episode , 341 step , -32.27 Actor Loss, 8.92 Critic Loss,  0.01 Threshold , 3.97 Top reward, -235.35 Total reward\n",
            "[0.00200686 0.00257192]\n",
            "[0.00308669 0.00307866]\n",
            "128 episode , 226 step , -32.33 Actor Loss, 9.24 Critic Loss,  0.01 Threshold , 6.03 Top reward, -184.14 Total reward\n",
            "[0.00264621 0.00192557]\n",
            "[0.00219185 0.00244862]\n",
            "129 episode , 168 step , -32.01 Actor Loss, 8.38 Critic Loss,  0.01 Threshold , 4.72 Top reward, -202.44 Total reward\n",
            "[0.0023973  0.00242401]\n",
            "[0.00258022 0.00302015]\n",
            "[0.0033348  0.00240037]\n",
            "[0.00228453 0.00352429]\n",
            "[0.00136008 0.00176018]\n",
            "[0.00293974 0.00264327]\n",
            "[0.00168049 0.00281327]\n",
            "[0.00220389 0.00276376]\n",
            "130 episode , 800 step , -31.76 Actor Loss, 7.26 Critic Loss,  0.01 Threshold , 4.50 Top reward, -50.60 Total reward\n",
            "[0.00280139 0.00230756]\n",
            "[0.0033464  0.00210183]\n",
            "[0.0039194  0.00351773]\n",
            "[0.00170693 0.00256021]\n",
            "[0.00322365 0.00333652]\n",
            "[0.00237205 0.00304826]\n",
            "[0.00132633 0.00354269]\n",
            "[0.00369282 0.00345909]\n",
            "131 episode , 800 step , -32.33 Actor Loss, 4.50 Critic Loss,  0.01 Threshold , 13.57 Top reward, -46.16 Total reward\n",
            "[0.00331314 0.00265701]\n",
            "[0.00319232 0.0016968 ]\n",
            "[0.00333918 0.00292713]\n",
            "[0.00369305 0.00271059]\n",
            "[0.00194571 0.00233246]\n",
            "[0.00258692 0.00172393]\n",
            "[0.00117015 0.0022302 ]\n",
            "[0.00263486 0.00245193]\n",
            "132 episode , 800 step , -33.07 Actor Loss, 4.03 Critic Loss,  0.01 Threshold , 11.39 Top reward, -5.64 Total reward\n",
            "[0.00373326 0.00203206]\n",
            "[0.00298258 0.00184004]\n",
            "[0.00110243 0.00211521]\n",
            "[0.0023851  0.00293632]\n",
            "[0.00358044 0.00121223]\n",
            "[0.00222386 0.00345799]\n",
            "[0.00341113 0.00355463]\n",
            "133 episode , 723 step , -33.69 Actor Loss, 4.62 Critic Loss,  0.01 Threshold , 4.24 Top reward, -139.85 Total reward\n",
            "[0.00261429 0.00131432]\n",
            "[0.00317661 0.00179819]\n",
            "[0.00245478 0.00326862]\n",
            "[0.0016363 0.0016848]\n",
            "[0.0027336 0.0030844]\n",
            "[0.00289842 0.00189587]\n",
            "[0.0028255 0.0036121]\n",
            "[0.00286268 0.00226911]\n",
            "134 episode , 800 step , -34.09 Actor Loss, 5.90 Critic Loss,  0.01 Threshold , 4.78 Top reward, -41.13 Total reward\n",
            "[0.00117687 0.00169232]\n",
            "[0.00276025 0.00289659]\n",
            "[0.00367766 0.00293156]\n",
            "[0.00329958 0.00250646]\n",
            "[0.00201911 0.00132133]\n",
            "[0.00309533 0.00245514]\n",
            "[0.00382326 0.00190687]\n",
            "[0.0009248  0.00248094]\n",
            "135 episode , 800 step , -34.12 Actor Loss, 6.09 Critic Loss,  0.01 Threshold , 4.35 Top reward, -64.74 Total reward\n",
            "[0.00201993 0.00171973]\n",
            "[0.00170444 0.00247995]\n",
            "136 episode , 170 step , -34.11 Actor Loss, 9.00 Critic Loss,  0.01 Threshold , 4.19 Top reward, -75.19 Total reward\n",
            "[0.00289907 0.00202092]\n",
            "[0.0025347  0.00277334]\n",
            "[0.00248328 0.00286492]\n",
            "[0.00178654 0.00274332]\n",
            "[0.00283124 0.00153168]\n",
            "[0.00148823 0.00365484]\n",
            "137 episode , 581 step , -34.24 Actor Loss, 6.69 Critic Loss,  0.01 Threshold , 4.27 Top reward, -179.71 Total reward\n",
            "[0.00164451 0.00274607]\n",
            "[0.00396328 0.00297513]\n",
            "[0.00262128 0.00250007]\n",
            "[0.00174935 0.00237904]\n",
            "[0.0020065  0.00189416]\n",
            "[0.00129206 0.00297012]\n",
            "[0.00266044 0.00249812]\n",
            "[0.00392757 0.00213413]\n",
            "138 episode , 800 step , -33.86 Actor Loss, 9.91 Critic Loss,  0.01 Threshold , 4.45 Top reward, -119.77 Total reward\n",
            "[0.00285402 0.00161966]\n",
            "[0.00268593 0.00345084]\n",
            "[0.00172163 0.00147605]\n",
            "[0.00302934 0.00256506]\n",
            "[0.00419532 0.00219095]\n",
            "[0.00270404 0.00257787]\n",
            "[0.002898   0.00243379]\n",
            "[0.00198816 0.00102034]\n",
            "139 episode , 800 step , -33.25 Actor Loss, 7.40 Critic Loss,  0.01 Threshold , 4.28 Top reward, -64.48 Total reward\n",
            "[0.00150964 0.00177355]\n",
            "[0.00321959 0.00262569]\n",
            "[0.00198424 0.00235454]\n",
            "[0.00361714 0.00235061]\n",
            "[0.00166791 0.00218942]\n",
            "[0.00134067 0.0036165 ]\n",
            "[0.00132421 0.00295683]\n",
            "[0.00259797 0.00143261]\n",
            "140 episode , 800 step , -33.09 Actor Loss, 6.65 Critic Loss,  0.01 Threshold , 5.05 Top reward, -77.81 Total reward\n",
            "[0.00175209 0.00301481]\n",
            "[0.00139748 0.00226584]\n",
            "[0.00327057 0.001661  ]\n",
            "[0.002235   0.00342609]\n",
            "[0.00253723 0.00301206]\n",
            "[0.00176035 0.00229069]\n",
            "[0.00276043 0.00313144]\n",
            "[0.00130468 0.00364325]\n",
            "141 episode , 800 step , -33.09 Actor Loss, 6.29 Critic Loss,  0.01 Threshold , 4.93 Top reward, -51.26 Total reward\n",
            "[0.00351489 0.00293169]\n",
            "[0.00241981 0.00150028]\n",
            "[0.00075832 0.00311418]\n",
            "[0.00252569 0.00216494]\n",
            "[0.00256578 0.00306487]\n",
            "[0.00274197 0.00312407]\n",
            "[0.00354134 0.0029587 ]\n",
            "[0.00236133 0.00400389]\n",
            "142 episode , 800 step , -33.82 Actor Loss, 6.86 Critic Loss,  0.01 Threshold , 6.15 Top reward, -10.81 Total reward\n",
            "[0.00188667 0.00248428]\n",
            "143 episode , 177 step , -34.76 Actor Loss, 4.32 Critic Loss,  0.01 Threshold , 14.13 Top reward, -323.31 Total reward\n",
            "[0.00287958 0.00218846]\n",
            "[0.00327565 0.00408883]\n",
            "[0.0020404  0.00250562]\n",
            "[0.00281753 0.00285407]\n",
            "[0.00306554 0.00245201]\n",
            "[0.00182025 0.00198636]\n",
            "144 episode , 549 step , -34.31 Actor Loss, 7.79 Critic Loss,  0.01 Threshold , 10.08 Top reward, -287.69 Total reward\n",
            "[0.00229571 0.00345411]\n",
            "[0.0016615  0.00160091]\n",
            "[0.00187853 0.00228619]\n",
            "145 episode , 351 step , -33.70 Actor Loss, 6.66 Critic Loss,  0.01 Threshold , 7.21 Top reward, -274.80 Total reward\n",
            "[0.0027764  0.00248672]\n",
            "[0.00295966 0.00283223]\n",
            "[0.00414863 0.00164144]\n",
            "146 episode , 286 step , -33.74 Actor Loss, 10.32 Critic Loss,  0.01 Threshold , 10.24 Top reward, -288.42 Total reward\n",
            "[0.00192005 0.00165355]\n",
            "[0.00244948 0.00247637]\n",
            "[0.00225123 0.00249739]\n",
            "147 episode , 331 step , -32.90 Actor Loss, 8.80 Critic Loss,  0.01 Threshold , 8.38 Top reward, -212.32 Total reward\n",
            "[0.00302295 0.00214797]\n",
            "[0.00334321 0.0018877 ]\n",
            "[0.00256394 0.00249264]\n",
            "[0.00159011 0.00269308]\n",
            "[0.00266061 0.00186343]\n",
            "148 episode , 474 step , -32.65 Actor Loss, 8.08 Critic Loss,  0.01 Threshold , 8.27 Top reward, -295.48 Total reward\n",
            "[0.00300808 0.00224494]\n",
            "[0.00169484 0.00345999]\n",
            "[0.00223644 0.0027508 ]\n",
            "[0.00152934 0.00246474]\n",
            "[0.00104272 0.00367051]\n",
            "[0.00319243 0.00312707]\n",
            "[0.00141983 0.00227594]\n",
            "[0.00364221 0.00198746]\n",
            "149 episode , 800 step , -32.28 Actor Loss, 8.48 Critic Loss,  0.01 Threshold , 5.46 Top reward, -131.45 Total reward\n",
            "[0.00282233 0.00279662]\n",
            "[0.00329919 0.00234591]\n",
            "[0.00209047 0.00199505]\n",
            "[0.00314433 0.00347306]\n",
            "[0.00276952 0.00359861]\n",
            "150 episode , 484 step , -31.88 Actor Loss, 8.12 Critic Loss,  0.01 Threshold , 50.23 Top reward, -69.56 Total reward\n",
            "[0.00256522 0.0027948 ]\n",
            "[0.00343486 0.00284843]\n",
            "[0.00183413 0.00299819]\n",
            "[0.00218067 0.00211409]\n",
            "[0.00195682 0.0025775 ]\n",
            "[0.00185048 0.00177385]\n",
            "[0.00234088 0.00354859]\n",
            "[0.00363596 0.00187572]\n",
            "151 episode , 800 step , -31.61 Actor Loss, 8.17 Critic Loss,  0.01 Threshold , 3.47 Top reward, -117.36 Total reward\n",
            "[0.00251294 0.00256288]\n",
            "[0.00295753 0.00200335]\n",
            "[0.00216959 0.00246816]\n",
            "[0.00194542 0.00383517]\n",
            "[0.00280734 0.00179174]\n",
            "[0.00200881 0.00170621]\n",
            "[0.0027951  0.00114026]\n",
            "[0.00204147 0.00398432]\n",
            "152 episode , 800 step , -30.81 Actor Loss, 5.81 Critic Loss,  0.01 Threshold , 3.78 Top reward, -113.57 Total reward\n",
            "[0.00200363 0.00158996]\n",
            "[0.00247343 0.00336506]\n",
            "[0.00195743 0.0021798 ]\n",
            "[0.003266   0.00159355]\n",
            "[0.00126702 0.00282169]\n",
            "[0.00276619 0.002447  ]\n",
            "[0.00254621 0.00194934]\n",
            "153 episode , 683 step , -29.83 Actor Loss, 5.78 Critic Loss,  0.01 Threshold , 14.40 Top reward, -112.61 Total reward\n",
            "[0.00353129 0.00205175]\n",
            "[0.00402154 0.00272632]\n",
            "[0.00236178 0.00185307]\n",
            "[0.00228363 0.00261408]\n",
            "[0.00164369 0.00083464]\n",
            "[0.00230343 0.00095326]\n",
            "[0.0025051  0.00240948]\n",
            "[0.00211498 0.00162988]\n",
            "154 episode , 800 step , -30.10 Actor Loss, 5.23 Critic Loss,  0.01 Threshold , 12.57 Top reward, 99.17 Total reward\n",
            "[0.00289242 0.00160681]\n",
            "[0.00247519 0.00347712]\n",
            "[0.00259742 0.00254367]\n",
            "[0.00099901 0.00181063]\n",
            "[0.00298735 0.00285936]\n",
            "155 episode , 481 step , -31.00 Actor Loss, 4.34 Critic Loss,  0.01 Threshold , 31.48 Top reward, -59.61 Total reward\n",
            "[0.00263353 0.00247801]\n",
            "[0.00238465 0.00167757]\n",
            "[0.00272885 0.00217312]\n",
            "[0.00356682 0.00203985]\n",
            "[0.00322317 0.00227456]\n",
            "[0.00244027 0.00273162]\n",
            "[0.00176826 0.00294212]\n",
            "[0.00122694 0.00192366]\n",
            "156 episode , 800 step , -31.75 Actor Loss, 5.41 Critic Loss,  0.01 Threshold , 22.59 Top reward, 40.52 Total reward\n",
            "[0.00234938 0.00290984]\n",
            "[0.0018884 0.0033795]\n",
            "[0.00343041 0.00243655]\n",
            "[0.00253391 0.00248401]\n",
            "[0.00082336 0.00341106]\n",
            "[0.00221732 0.003583  ]\n",
            "[0.00387889 0.00365677]\n",
            "[0.00167906 0.0028841 ]\n",
            "157 episode , 800 step , -32.34 Actor Loss, 4.91 Critic Loss,  0.01 Threshold , 14.28 Top reward, -80.15 Total reward\n",
            "[0.00299166 0.00183805]\n",
            "[0.00169608 0.00103872]\n",
            "[0.00298971 0.00179119]\n",
            "[0.00134273 0.0021607 ]\n",
            "[0.00223362 0.00186463]\n",
            "[0.00332525 0.00299255]\n",
            "[0.00198758 0.00261026]\n",
            "[0.00077637 0.00313894]\n",
            "158 episode , 800 step , -33.12 Actor Loss, 5.87 Critic Loss,  0.01 Threshold , 13.32 Top reward, 87.40 Total reward\n",
            "[0.00267365 0.00323502]\n",
            "159 episode , 171 step , -33.19 Actor Loss, 6.18 Critic Loss,  0.01 Threshold , 3.55 Top reward, -132.97 Total reward\n",
            "[0.00150137 0.0035498 ]\n",
            "[0.00315559 0.00288093]\n",
            "[0.00289465 0.00284188]\n",
            "160 episode , 233 step , -33.40 Actor Loss, 6.53 Critic Loss,  0.01 Threshold , 4.66 Top reward, -173.19 Total reward\n",
            "[0.00231076 0.00307523]\n",
            "[0.00226078 0.00175245]\n",
            "[0.00269013 0.00269483]\n",
            "[0.00315248 0.00323027]\n",
            "[0.00299675 0.00355683]\n",
            "[0.00232547 0.00243604]\n",
            "161 episode , 645 step , -34.11 Actor Loss, 7.26 Critic Loss,  0.01 Threshold , 100.00 Top reward, 194.10 Total reward\n",
            "[0.00170392 0.00246747]\n",
            "[0.00143736 0.00214705]\n",
            "[0.00289544 0.00176784]\n",
            "[0.00348866 0.00205289]\n",
            "[0.00120162 0.0021828 ]\n",
            "[0.00306278 0.00179349]\n",
            "[0.00155364 0.00164276]\n",
            "[0.00278439 0.00184217]\n",
            "162 episode , 800 step , -35.05 Actor Loss, 8.05 Critic Loss,  0.01 Threshold , 9.76 Top reward, -2.92 Total reward\n",
            "[0.00279388 0.00330213]\n",
            "[0.00295603 0.00280894]\n",
            "[0.00128697 0.00315987]\n",
            "[0.00375325 0.00236826]\n",
            "[0.00228652 0.00193888]\n",
            "[0.00383068 0.0020398 ]\n",
            "[0.00339959 0.00268414]\n",
            "163 episode , 713 step , -36.99 Actor Loss, 8.06 Critic Loss,  0.01 Threshold , 100.00 Top reward, 186.20 Total reward\n",
            "[0.00296091 0.0022804 ]\n",
            "[0.0023717  0.00319605]\n",
            "[0.00291453 0.00162812]\n",
            "164 episode , 273 step , -38.34 Actor Loss, 9.25 Critic Loss,  0.01 Threshold , 22.26 Top reward, 24.62 Total reward\n",
            "[0.00351993 0.0014699 ]\n",
            "[0.00415779 0.00241773]\n",
            "[0.00069432 0.00197171]\n",
            "[0.00306123 0.00206567]\n",
            "[0.00334392 0.00327659]\n",
            "[0.00188564 0.00178482]\n",
            "[0.00140349 0.00233729]\n",
            "[0.00355772 0.00361373]\n",
            "165 episode , 800 step , -39.31 Actor Loss, 9.88 Critic Loss,  0.01 Threshold , 24.28 Top reward, 117.79 Total reward\n",
            "[0.00202457 0.00317215]\n",
            "[0.00382169 0.00234912]\n",
            "[0.00213718 0.00291239]\n",
            "[0.0036539  0.00274506]\n",
            "166 episode , 412 step , -40.24 Actor Loss, 7.42 Critic Loss,  0.01 Threshold , 100.00 Top reward, 187.94 Total reward\n",
            "[0.00360321 0.00317028]\n",
            "[0.00238103 0.00163188]\n",
            "[0.00283403 0.00277569]\n",
            "[0.00328323 0.0030587 ]\n",
            "[0.00147768 0.00337128]\n",
            "[0.00268974 0.00390847]\n",
            "[0.00313694 0.00327335]\n",
            "[0.00301364 0.00242567]\n",
            "167 episode , 800 step , -42.34 Actor Loss, 8.54 Critic Loss,  0.01 Threshold , 17.48 Top reward, 155.11 Total reward\n",
            "[0.00237693 0.00304116]\n",
            "[0.00113448 0.00206747]\n",
            "[0.00238035 0.00264409]\n",
            "[0.00349129 0.00181081]\n",
            "[0.00283317 0.00260306]\n",
            "[0.00301228 0.00217962]\n",
            "168 episode , 541 step , -43.76 Actor Loss, 8.13 Critic Loss,  0.01 Threshold , 100.00 Top reward, 183.91 Total reward\n",
            "[0.00146339 0.00338842]\n",
            "[0.00266502 0.0028019 ]\n",
            "[0.00088048 0.00256998]\n",
            "[0.00257871 0.00337263]\n",
            "[0.00174231 0.00263619]\n",
            "[0.00219293 0.00234419]\n",
            "[0.00174453 0.00269838]\n",
            "[0.00195263 0.00366485]\n",
            "169 episode , 800 step , -45.07 Actor Loss, 8.32 Critic Loss,  0.01 Threshold , 13.72 Top reward, 72.39 Total reward\n",
            "[0.00367168 0.0024397 ]\n",
            "[0.0040201  0.00357909]\n",
            "[0.00226401 0.00344318]\n",
            "[0.00370474 0.00186438]\n",
            "170 episode , 393 step , -45.88 Actor Loss, 6.54 Critic Loss,  0.01 Threshold , 3.99 Top reward, -107.52 Total reward\n",
            "[0.0020048  0.00390412]\n",
            "171 episode , 141 step , -45.58 Actor Loss, 9.68 Critic Loss,  0.01 Threshold , 9.96 Top reward, -330.07 Total reward\n",
            "[0.00291298 0.00193003]\n",
            "[0.00263541 0.00302242]\n",
            "[0.00241569 0.00208074]\n",
            "[0.00248558 0.00313837]\n",
            "172 episode , 414 step , -44.61 Actor Loss, 14.14 Critic Loss,  0.01 Threshold , 100.00 Top reward, 142.11 Total reward\n",
            "[0.00287599 0.00180549]\n",
            "[0.00249711 0.0022496 ]\n",
            "[0.00094286 0.00170776]\n",
            "[0.00213264 0.00213855]\n",
            "[0.0021605  0.00318511]\n",
            "173 episode , 471 step , -44.71 Actor Loss, 9.73 Critic Loss,  0.01 Threshold , 16.76 Top reward, -60.02 Total reward\n",
            "[0.00208225 0.00262424]\n",
            "[0.00234679 0.00303543]\n",
            "[0.00275182 0.00210244]\n",
            "[0.00359787 0.00304158]\n",
            "174 episode , 401 step , -44.93 Actor Loss, 12.58 Critic Loss,  0.01 Threshold , 100.00 Top reward, 127.83 Total reward\n",
            "[0.00326751 0.00313351]\n",
            "[0.00087952 0.00205819]\n",
            "[0.00150293 0.00211016]\n",
            "[0.00341256 0.00326823]\n",
            "[0.00310956 0.00267319]\n",
            "[0.00162054 0.00274805]\n",
            "[0.00132481 0.00268901]\n",
            "[0.00287821 0.00262526]\n",
            "175 episode , 800 step , -44.91 Actor Loss, 10.23 Critic Loss,  0.01 Threshold , 6.86 Top reward, -24.74 Total reward\n",
            "[0.00232919 0.00257812]\n",
            "[0.00253705 0.00349356]\n",
            "[0.00288548 0.00140645]\n",
            "[0.00323342 0.00304674]\n",
            "[0.0024007  0.00346321]\n",
            "[0.00157752 0.00245781]\n",
            "[0.0029987  0.00322853]\n",
            "176 episode , 757 step , -44.99 Actor Loss, 11.03 Critic Loss,  0.01 Threshold , 16.27 Top reward, -149.89 Total reward\n",
            "[0.00408413 0.00090435]\n",
            "[0.00351161 0.00200168]\n",
            "[0.00386662 0.00139435]\n",
            "[0.00344685 0.00102926]\n",
            "[0.00292507 0.00318978]\n",
            "[0.00277245 0.00199598]\n",
            "[0.00195727 0.00222513]\n",
            "177 episode , 633 step , -45.12 Actor Loss, 11.59 Critic Loss,  0.01 Threshold , 10.52 Top reward, -142.13 Total reward\n",
            "[0.00328622 0.00249327]\n",
            "[0.00209454 0.003051  ]\n",
            "178 episode , 213 step , -45.31 Actor Loss, 11.59 Critic Loss,  0.01 Threshold , 3.23 Top reward, -67.21 Total reward\n",
            "[0.00229214 0.00205275]\n",
            "[0.00314326 0.00274142]\n",
            "179 episode , 221 step , -45.04 Actor Loss, 11.59 Critic Loss,  0.01 Threshold , 3.67 Top reward, -58.27 Total reward\n",
            "[0.00143668 0.00284151]\n",
            "[0.00283588 0.00255408]\n",
            "[0.00278673 0.0023101 ]\n",
            "[0.00322435 0.00323686]\n",
            "[0.00102745 0.00201204]\n",
            "180 episode , 469 step , -45.21 Actor Loss, 12.02 Critic Loss,  0.01 Threshold , 100.00 Top reward, 203.59 Total reward\n",
            "[0.00278131 0.00181344]\n",
            "[0.00195806 0.00345489]\n",
            "[0.00184557 0.00331886]\n",
            "181 episode , 332 step , -45.57 Actor Loss, 11.08 Critic Loss,  0.01 Threshold , 100.00 Top reward, 212.09 Total reward\n",
            "[0.0028394  0.00163679]\n",
            "[0.00178824 0.00362816]\n",
            "[0.00270731 0.00198767]\n",
            "182 episode , 326 step , -45.69 Actor Loss, 11.70 Critic Loss,  0.01 Threshold , 4.12 Top reward, -110.36 Total reward\n",
            "[0.00245378 0.00263078]\n",
            "[0.00329805 0.00232332]\n",
            "[0.00200177 0.00342946]\n",
            "[0.00183281 0.00295273]\n",
            "[0.00275346 0.00215971]\n",
            "[0.00259281 0.00228997]\n",
            "[0.00280596 0.00169129]\n",
            "[0.00361168 0.00364252]\n",
            "183 episode , 800 step , -45.53 Actor Loss, 12.77 Critic Loss,  0.01 Threshold , 13.00 Top reward, 160.72 Total reward\n",
            "[0.00311019 0.00214334]\n",
            "[0.00234215 0.00312057]\n",
            "184 episode , 216 step , -45.78 Actor Loss, 10.62 Critic Loss,  0.01 Threshold , 100.00 Top reward, 231.27 Total reward\n",
            "[0.00330789 0.00149012]\n",
            "[0.00168955 0.00277262]\n",
            "[0.00250734 0.00139985]\n",
            "185 episode , 257 step , -45.99 Actor Loss, 11.40 Critic Loss,  0.01 Threshold , 9.17 Top reward, -45.08 Total reward\n",
            "[0.00293951 0.00286275]\n",
            "[0.00355053 0.00298062]\n",
            "[0.00248022 0.00233521]\n",
            "186 episode , 258 step , -46.14 Actor Loss, 14.24 Critic Loss,  0.01 Threshold , 13.24 Top reward, -19.28 Total reward\n",
            "[0.00168323 0.00387838]\n",
            "[0.00159081 0.00237109]\n",
            "[0.00158528 0.00245075]\n",
            "[0.00321318 0.00277876]\n",
            "[0.00178826 0.00270554]\n",
            "187 episode , 585 step , -46.31 Actor Loss, 13.17 Critic Loss,  0.01 Threshold , 3.02 Top reward, -129.56 Total reward\n",
            "[0.00305259 0.00199058]\n",
            "[0.00078976 0.0023908 ]\n",
            "[0.00284677 0.00330107]\n",
            "[0.00203532 0.00164969]\n",
            "[0.00141522 0.00088473]\n",
            "[0.0024044  0.00212844]\n",
            "188 episode , 537 step , -45.97 Actor Loss, 15.77 Critic Loss,  0.01 Threshold , 100.00 Top reward, 257.06 Total reward\n",
            "[0.00354277 0.00161712]\n",
            "189 episode , 108 step , -45.79 Actor Loss, 14.28 Critic Loss,  0.01 Threshold , 13.14 Top reward, 16.50 Total reward\n",
            "[0.00204029 0.00206544]\n",
            "190 episode , 141 step , -46.09 Actor Loss, 11.75 Critic Loss,  0.01 Threshold , 18.33 Top reward, -27.84 Total reward\n",
            "[0.00361849 0.003734  ]\n",
            "[0.00353792 0.00164018]\n",
            "[0.00116474 0.00130923]\n",
            "[0.00118235 0.00154364]\n",
            "[0.00185221 0.00168562]\n",
            "191 episode , 430 step , -46.14 Actor Loss, 17.87 Critic Loss,  0.01 Threshold , 100.00 Top reward, 173.46 Total reward\n",
            "[0.00301972 0.00247499]\n",
            "[0.00327055 0.00290618]\n",
            "192 episode , 261 step , -46.03 Actor Loss, 16.12 Critic Loss,  0.01 Threshold , 100.00 Top reward, 278.58 Total reward\n",
            "[0.00250174 0.00416734]\n",
            "[0.00253022 0.00326908]\n",
            "[0.00246001 0.00272598]\n",
            "[0.00131783 0.00194026]\n",
            "[0.00288577 0.00343674]\n",
            "[0.00282596 0.00109918]\n",
            "[0.00360225 0.0021634 ]\n",
            "[0.00311308 0.00174958]\n",
            "193 episode , 800 step , -45.80 Actor Loss, 13.58 Critic Loss,  0.01 Threshold , 4.28 Top reward, 41.86 Total reward\n",
            "[0.00271132 0.000816  ]\n",
            "[0.00267106 0.00253552]\n",
            "[0.00263085 0.00238566]\n",
            "[0.0012036  0.00262897]\n",
            "[0.00282749 0.001822  ]\n",
            "[0.00338241 0.00273174]\n",
            "[0.00209748 0.00130601]\n",
            "[0.00325625 0.00385921]\n",
            "194 episode , 800 step , -46.85 Actor Loss, 15.00 Critic Loss,  0.01 Threshold , 5.32 Top reward, -27.93 Total reward\n",
            "[0.00199946 0.00284372]\n",
            "[0.00184623 0.00278087]\n",
            "[0.00295421 0.00278407]\n",
            "[0.00211866 0.00171264]\n",
            "[0.00324082 0.00390705]\n",
            "[0.00130986 0.00291282]\n",
            "[0.00389503 0.00151515]\n",
            "[0.00348264 0.00187677]\n",
            "195 episode , 800 step , -48.15 Actor Loss, 10.25 Critic Loss,  0.01 Threshold , 5.06 Top reward, 14.41 Total reward\n",
            "[0.00252812 0.00118792]\n",
            "[0.00178011 0.00271407]\n",
            "[0.00205744 0.00238192]\n",
            "[0.00168804 0.00343286]\n",
            "[0.00214575 0.00284437]\n",
            "[0.00202904 0.0037018 ]\n",
            "[0.00184556 0.00250118]\n",
            "196 episode , 679 step , -48.80 Actor Loss, 10.32 Critic Loss,  0.01 Threshold , 100.00 Top reward, 145.86 Total reward\n",
            "[0.00156982 0.00370485]\n",
            "[0.00365412 0.00252277]\n",
            "[0.00264953 0.00199994]\n",
            "[0.00277183 0.00306128]\n",
            "197 episode , 361 step , -49.65 Actor Loss, 12.80 Critic Loss,  0.01 Threshold , 100.00 Top reward, 220.15 Total reward\n",
            "[0.00090302 0.00302974]\n",
            "[0.00291134 0.00126903]\n",
            "[0.00165312 0.00074925]\n",
            "[0.00237415 0.00308645]\n",
            "198 episode , 419 step , -49.84 Actor Loss, 12.75 Critic Loss,  0.01 Threshold , 100.00 Top reward, 171.20 Total reward\n",
            "[0.00247024 0.00115463]\n",
            "[0.00243811 0.00208331]\n",
            "[0.00182293 0.0030708 ]\n",
            "[0.0019116 0.0033765]\n",
            "[0.00194489 0.00180031]\n",
            "[0.00344377 0.00136683]\n",
            "[0.00291014 0.00084117]\n",
            "[0.00317231 0.00385359]\n",
            "199 episode , 783 step , -49.68 Actor Loss, 11.22 Critic Loss,  0.01 Threshold , 13.68 Top reward, -102.04 Total reward\n",
            "[0.00200975 0.00193016]\n",
            "[0.00348236 0.00259114]\n",
            "[0.0030236  0.00304036]\n",
            "[0.00204076 0.00200819]\n",
            "[0.00243475 0.00182166]\n",
            "[0.00303637 0.00219304]\n",
            "[0.00076279 0.00244807]\n",
            "[0.00374394 0.00350328]\n",
            "200 episode , 800 step , -49.38 Actor Loss, 10.77 Critic Loss,  0.01 Threshold , 12.20 Top reward, -30.30 Total reward\n",
            "[0.00201372 0.0025566 ]\n",
            "[0.00424308 0.00194532]\n",
            "[0.00351254 0.00212571]\n",
            "[0.00239289 0.00314891]\n",
            "201 episode , 480 step , -49.83 Actor Loss, 10.60 Critic Loss,  0.01 Threshold , 100.00 Top reward, 180.86 Total reward\n",
            "[0.00234665 0.00171493]\n",
            "[0.00393624 0.00203079]\n",
            "[0.00222404 0.0020482 ]\n",
            "[0.00207768 0.00231869]\n",
            "[0.00249799 0.00356666]\n",
            "202 episode , 428 step , -49.62 Actor Loss, 9.99 Critic Loss,  0.01 Threshold , 100.00 Top reward, 204.14 Total reward\n",
            "[0.00304965 0.00243107]\n",
            "[0.0014099  0.00291838]\n",
            "[0.00336931 0.00250885]\n",
            "[0.00154071 0.00210155]\n",
            "[0.00180177 0.00348381]\n",
            "[0.00396789 0.00134337]\n",
            "203 episode , 641 step , -50.62 Actor Loss, 8.91 Critic Loss,  0.01 Threshold , 100.00 Top reward, 173.30 Total reward\n",
            "[0.00245264 0.00242188]\n",
            "[0.00146067 0.00166766]\n",
            "[0.00265097 0.00293835]\n",
            "[0.00165251 0.00363527]\n",
            "204 episode , 369 step , -50.99 Actor Loss, 11.51 Critic Loss,  0.01 Threshold , 100.00 Top reward, 237.66 Total reward\n",
            "[0.00196423 0.00117576]\n",
            "[0.00353364 0.00402561]\n",
            "205 episode , 227 step , -51.06 Actor Loss, 11.14 Critic Loss,  0.01 Threshold , 100.00 Top reward, 265.27 Total reward\n",
            "[0.00216607 0.00316349]\n",
            "[0.0028435  0.00263994]\n",
            "[0.00207569 0.00289179]\n",
            "[0.00136613 0.00259853]\n",
            "[0.00226013 0.00095349]\n",
            "[0.00282402 0.00224222]\n",
            "[0.00247873 0.00270711]\n",
            "[0.00184336 0.00247263]\n",
            "206 episode , 800 step , -51.47 Actor Loss, 9.92 Critic Loss,  0.01 Threshold , 11.32 Top reward, 67.35 Total reward\n",
            "[0.00270479 0.00306096]\n",
            "[0.00246992 0.00151067]\n",
            "[0.00251588 0.00274638]\n",
            "[0.00301685 0.00074078]\n",
            "[0.00111355 0.00344082]\n",
            "[0.00238864 0.00306389]\n",
            "[0.0020461  0.00213061]\n",
            "[0.0019796  0.00266243]\n",
            "207 episode , 800 step , -53.42 Actor Loss, 8.56 Critic Loss,  0.01 Threshold , 5.24 Top reward, 13.45 Total reward\n",
            "[0.00204654 0.00222466]\n",
            "[0.00175587 0.00256534]\n",
            "208 episode , 153 step , -54.23 Actor Loss, 8.17 Critic Loss,  0.01 Threshold , 17.68 Top reward, -8.63 Total reward\n",
            "[0.00382402 0.00344299]\n",
            "[0.002783   0.00126742]\n",
            "[0.00137629 0.00219059]\n",
            "[0.00357105 0.00315278]\n",
            "[0.00356219 0.00251465]\n",
            "[0.00263325 0.0016763 ]\n",
            "[0.00224029 0.00333813]\n",
            "[0.0019988  0.00271059]\n",
            "209 episode , 800 step , -54.24 Actor Loss, 9.82 Critic Loss,  0.01 Threshold , 12.23 Top reward, 123.63 Total reward\n",
            "[0.00128135 0.00264246]\n",
            "[0.00243483 0.00219583]\n",
            "[0.00242833 0.00256567]\n",
            "[0.00219619 0.00271652]\n",
            "210 episode , 407 step , -54.46 Actor Loss, 7.26 Critic Loss,  0.01 Threshold , 100.00 Top reward, 241.85 Total reward\n",
            "[0.00292018 0.00286554]\n",
            "[0.00101469 0.00248437]\n",
            "211 episode , 243 step , -54.57 Actor Loss, 7.42 Critic Loss,  0.01 Threshold , 3.81 Top reward, -200.69 Total reward\n",
            "[0.00334353 0.00328862]\n",
            "[0.00194986 0.00320365]\n",
            "[0.00194616 0.00185212]\n",
            "[0.00165661 0.00339931]\n",
            "[0.00327267 0.00289204]\n",
            "[0.00209293 0.00284361]\n",
            "[0.00204306 0.00345363]\n",
            "[0.00160514 0.00327455]\n",
            "212 episode , 779 step , -54.84 Actor Loss, 8.84 Critic Loss,  0.01 Threshold , 100.00 Top reward, 235.59 Total reward\n",
            "[0.0038145  0.00054842]\n",
            "[0.00329347 0.00227001]\n",
            "[0.00274583 0.00386454]\n",
            "[0.00292575 0.00290615]\n",
            "[0.00178271 0.00233467]\n",
            "[0.00388896 0.00259174]\n",
            "[0.00176544 0.00299742]\n",
            "[0.00243962 0.00125914]\n",
            "213 episode , 800 step , -54.92 Actor Loss, 8.46 Critic Loss,  0.01 Threshold , 13.73 Top reward, 127.14 Total reward\n",
            "[0.0031491  0.00368312]\n",
            "[0.00295931 0.00406432]\n",
            "214 episode , 254 step , -54.82 Actor Loss, 9.01 Critic Loss,  0.01 Threshold , 100.00 Top reward, 232.73 Total reward\n",
            "[0.00245963 0.00224664]\n",
            "[0.00269618 0.00212994]\n",
            "[0.0014407  0.00384045]\n",
            "[0.00201377 0.00289174]\n",
            "[0.00190947 0.00276782]\n",
            "215 episode , 458 step , -54.95 Actor Loss, 9.71 Critic Loss,  0.01 Threshold , 100.00 Top reward, 237.70 Total reward\n",
            "[0.00321123 0.00257246]\n",
            "[0.00166494 0.00060858]\n",
            "[0.00206503 0.00167121]\n",
            "[0.00299449 0.00249585]\n",
            "216 episode , 414 step , -55.06 Actor Loss, 8.17 Critic Loss,  0.01 Threshold , 100.00 Top reward, 260.36 Total reward\n",
            "[0.00219946 0.00137311]\n",
            "[0.0029093  0.00238991]\n",
            "[0.00212132 0.00268977]\n",
            "[0.0014294  0.00207009]\n",
            "[0.0040418  0.00145236]\n",
            "[0.00274454 0.00250345]\n",
            "[0.00325697 0.00222663]\n",
            "[0.00222976 0.00214455]\n",
            "217 episode , 800 step , -55.12 Actor Loss, 7.53 Critic Loss,  0.01 Threshold , 4.73 Top reward, 13.82 Total reward\n",
            "[0.00257265 0.00356626]\n",
            "[0.00278246 0.00220487]\n",
            "[0.00200284 0.00376617]\n",
            "218 episode , 273 step , -55.11 Actor Loss, 9.20 Critic Loss,  0.01 Threshold , 100.00 Top reward, 263.77 Total reward\n",
            "[0.00201235 0.00305049]\n",
            "[0.00151535 0.00221211]\n",
            "[0.00372998 0.00160253]\n",
            "219 episode , 347 step , -55.45 Actor Loss, 6.80 Critic Loss,  0.01 Threshold , 18.02 Top reward, -35.67 Total reward\n",
            "[0.00229173 0.00236344]\n",
            "[0.00340355 0.00224161]\n",
            "[0.00168061 0.00297002]\n",
            "220 episode , 257 step , -55.49 Actor Loss, 8.91 Critic Loss,  0.01 Threshold , 21.06 Top reward, -11.86 Total reward\n",
            "[0.00276655 0.00253153]\n",
            "[0.00253257 0.00296331]\n",
            "[0.00313926 0.00298877]\n",
            "[0.00246078 0.00336645]\n",
            "[0.00116601 0.00311483]\n",
            "[0.00407179 0.00269039]\n",
            "221 episode , 637 step , -55.56 Actor Loss, 9.41 Critic Loss,  0.01 Threshold , 100.00 Top reward, 170.53 Total reward\n",
            "[0.00290409 0.00219041]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcHJFUbphoyy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lg-4834irdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}