{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEIlGpyharSvHjj1SRaRBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDQN%5D%5BMW%5D%20CartPole-v0%20(No%20CNN)%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "5fdece26-df81-4489-a21d-7b893223cc58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps = 5000\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwNNaGeeyMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        \n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 256\n",
        "class DQN_custom(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs):\n",
        "        super(DQN_custom, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "fb0b02c2-7add-4ce1-b059-a853dd3d2a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwM3MMuUEbmaWKSdwM7NMOYGbmWXq/wGd5vHPY3TB6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.01\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 1500\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "n_obvs = 4\n",
        "\n",
        "policy_net = DQN_custom(n_obvs, n_actions).to(device)\n",
        "policy_net.eval()\n",
        "target_net = DQN_custom(n_obvs, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(obs):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            obs = torch.FloatTensor(obs).to(device)\n",
        "            output = policy_net(obs).cpu()\n",
        "            predicted = np.argmax(output.data.numpy())\n",
        "            return torch.tensor([[predicted]],device=device, dtype=torch.long)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.cat(batch.state).to(device)\n",
        "    reward_batch = torch.cat(batch.reward).to(device)\n",
        "    action_batch = torch.cat(batch.action).to(device)\n",
        " \n",
        "    non_final_mask = map(lambda s: s is not None, batch.next_state)\n",
        "    non_final_mask = tuple(non_final_mask)\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.BoolTensor(non_final_mask)\n",
        "\n",
        "    # We don't want to backprop through the expected action values and volatile\n",
        "    # will save us on temporarily changing the model parameters'\n",
        "    # requires_grad to False!\n",
        "    non_final_next_states = [s.view(1, -1) for s in batch.next_state if s is not None]\n",
        "    non_final_next_states = torch.cat(non_final_next_states, 0).to(device)\n",
        "  \n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Huber 손실 계산\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    return loss.cpu().data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "55d899b2-5c6c-44ed-9d99-8032d187d860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_episodes = EPISODE_SIZE\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_loss = 0\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action.item())\n",
        "        if done:\n",
        "            reward -= 100\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        if not done:\n",
        "          next_state = torch.FloatTensor([next_obv])\n",
        "        else:\n",
        "          next_state = None\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.push(torch.FloatTensor([obv]), action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        total_loss += optimize_model()\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Loss, %.2f Threshold'%(i_episode,t+1,total_loss/(t+1),E))\n",
        "            plot_durations()\n",
        "            total_loss = 0\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 13 step , -1.00 Loss, 0.89 Threshold\n",
            "1 episode , 57 step , -1.00 Loss, 0.87 Threshold\n",
            "2 episode , 48 step , -1.00 Loss, 0.85 Threshold\n",
            "3 episode , 24 step , 1.18 Loss, 0.84 Threshold\n",
            "4 episode , 12 step , 2.92 Loss, 0.83 Threshold\n",
            "5 episode , 26 step , 2.98 Loss, 0.82 Threshold\n",
            "6 episode , 25 step , 3.18 Loss, 0.81 Threshold\n",
            "7 episode , 17 step , 3.58 Loss, 0.81 Threshold\n",
            "8 episode , 22 step , 2.93 Loss, 0.80 Threshold\n",
            "9 episode , 29 step , 3.40 Loss, 0.79 Threshold\n",
            "10 episode , 19 step , 3.36 Loss, 0.78 Threshold\n",
            "11 episode , 19 step , 3.52 Loss, 0.77 Threshold\n",
            "12 episode , 17 step , 3.58 Loss, 0.77 Threshold\n",
            "13 episode , 20 step , 3.91 Loss, 0.76 Threshold\n",
            "14 episode , 16 step , 4.15 Loss, 0.75 Threshold\n",
            "15 episode , 32 step , 3.94 Loss, 0.74 Threshold\n",
            "16 episode , 13 step , 3.40 Loss, 0.74 Threshold\n",
            "17 episode , 38 step , 4.40 Loss, 0.72 Threshold\n",
            "18 episode , 12 step , 3.30 Loss, 0.72 Threshold\n",
            "19 episode , 24 step , 4.22 Loss, 0.71 Threshold\n",
            "20 episode , 17 step , 4.03 Loss, 0.70 Threshold\n",
            "21 episode , 23 step , 3.82 Loss, 0.70 Threshold\n",
            "22 episode , 23 step , 4.27 Loss, 0.69 Threshold\n",
            "23 episode , 29 step , 3.50 Loss, 0.68 Threshold\n",
            "24 episode , 11 step , 3.47 Loss, 0.67 Threshold\n",
            "25 episode , 36 step , 3.87 Loss, 0.66 Threshold\n",
            "26 episode , 59 step , 3.59 Loss, 0.64 Threshold\n",
            "27 episode , 46 step , 3.57 Loss, 0.63 Threshold\n",
            "28 episode , 13 step , 3.27 Loss, 0.62 Threshold\n",
            "29 episode , 14 step , 3.95 Loss, 0.62 Threshold\n",
            "30 episode , 26 step , 3.24 Loss, 0.61 Threshold\n",
            "31 episode , 16 step , 4.12 Loss, 0.61 Threshold\n",
            "32 episode , 34 step , 3.14 Loss, 0.60 Threshold\n",
            "33 episode , 37 step , 3.26 Loss, 0.59 Threshold\n",
            "34 episode , 52 step , 3.25 Loss, 0.57 Threshold\n",
            "35 episode , 15 step , 3.13 Loss, 0.57 Threshold\n",
            "36 episode , 14 step , 3.35 Loss, 0.56 Threshold\n",
            "37 episode , 28 step , 3.13 Loss, 0.56 Threshold\n",
            "38 episode , 19 step , 2.91 Loss, 0.55 Threshold\n",
            "39 episode , 25 step , 3.21 Loss, 0.54 Threshold\n",
            "40 episode , 39 step , 2.78 Loss, 0.53 Threshold\n",
            "41 episode , 10 step , 3.19 Loss, 0.53 Threshold\n",
            "42 episode , 55 step , 2.46 Loss, 0.52 Threshold\n",
            "43 episode , 11 step , 2.59 Loss, 0.51 Threshold\n",
            "44 episode , 12 step , 2.26 Loss, 0.51 Threshold\n",
            "45 episode , 17 step , 2.03 Loss, 0.51 Threshold\n",
            "46 episode , 73 step , 2.37 Loss, 0.49 Threshold\n",
            "47 episode , 18 step , 2.38 Loss, 0.49 Threshold\n",
            "48 episode , 61 step , 2.32 Loss, 0.47 Threshold\n",
            "49 episode , 14 step , 2.70 Loss, 0.47 Threshold\n",
            "50 episode , 14 step , 2.63 Loss, 0.46 Threshold\n",
            "51 episode , 48 step , 2.25 Loss, 0.45 Threshold\n",
            "52 episode , 12 step , 1.73 Loss, 0.45 Threshold\n",
            "53 episode , 10 step , 2.06 Loss, 0.45 Threshold\n",
            "54 episode , 12 step , 2.12 Loss, 0.45 Threshold\n",
            "55 episode , 9 step , 2.07 Loss, 0.44 Threshold\n",
            "56 episode , 15 step , 2.21 Loss, 0.44 Threshold\n",
            "57 episode , 11 step , 1.94 Loss, 0.44 Threshold\n",
            "58 episode , 16 step , 2.09 Loss, 0.44 Threshold\n",
            "59 episode , 23 step , 2.27 Loss, 0.43 Threshold\n",
            "60 episode , 46 step , 2.30 Loss, 0.42 Threshold\n",
            "61 episode , 17 step , 2.61 Loss, 0.42 Threshold\n",
            "62 episode , 24 step , 2.01 Loss, 0.41 Threshold\n",
            "63 episode , 11 step , 1.91 Loss, 0.41 Threshold\n",
            "64 episode , 14 step , 2.41 Loss, 0.41 Threshold\n",
            "65 episode , 43 step , 2.15 Loss, 0.40 Threshold\n",
            "66 episode , 13 step , 1.98 Loss, 0.40 Threshold\n",
            "67 episode , 27 step , 2.21 Loss, 0.39 Threshold\n",
            "68 episode , 13 step , 2.43 Loss, 0.39 Threshold\n",
            "69 episode , 11 step , 2.22 Loss, 0.39 Threshold\n",
            "70 episode , 39 step , 2.33 Loss, 0.38 Threshold\n",
            "71 episode , 18 step , 2.64 Loss, 0.38 Threshold\n",
            "72 episode , 57 step , 2.36 Loss, 0.37 Threshold\n",
            "73 episode , 25 step , 2.33 Loss, 0.36 Threshold\n",
            "74 episode , 15 step , 3.11 Loss, 0.36 Threshold\n",
            "75 episode , 10 step , 2.47 Loss, 0.36 Threshold\n",
            "76 episode , 49 step , 2.32 Loss, 0.35 Threshold\n",
            "77 episode , 12 step , 2.51 Loss, 0.35 Threshold\n",
            "78 episode , 55 step , 2.48 Loss, 0.34 Threshold\n",
            "79 episode , 16 step , 2.38 Loss, 0.33 Threshold\n",
            "80 episode , 22 step , 2.25 Loss, 0.33 Threshold\n",
            "81 episode , 15 step , 2.59 Loss, 0.33 Threshold\n",
            "82 episode , 77 step , 2.61 Loss, 0.32 Threshold\n",
            "83 episode , 15 step , 2.78 Loss, 0.31 Threshold\n",
            "84 episode , 14 step , 1.91 Loss, 0.31 Threshold\n",
            "85 episode , 56 step , 2.56 Loss, 0.30 Threshold\n",
            "86 episode , 16 step , 2.90 Loss, 0.30 Threshold\n",
            "87 episode , 14 step , 2.60 Loss, 0.30 Threshold\n",
            "88 episode , 52 step , 2.43 Loss, 0.29 Threshold\n",
            "89 episode , 12 step , 2.11 Loss, 0.29 Threshold\n",
            "90 episode , 39 step , 2.61 Loss, 0.29 Threshold\n",
            "91 episode , 19 step , 3.15 Loss, 0.28 Threshold\n",
            "92 episode , 57 step , 2.87 Loss, 0.27 Threshold\n",
            "93 episode , 10 step , 2.59 Loss, 0.27 Threshold\n",
            "94 episode , 65 step , 2.83 Loss, 0.27 Threshold\n",
            "95 episode , 16 step , 2.68 Loss, 0.26 Threshold\n",
            "96 episode , 58 step , 2.82 Loss, 0.26 Threshold\n",
            "97 episode , 17 step , 2.60 Loss, 0.25 Threshold\n",
            "98 episode , 69 step , 2.61 Loss, 0.25 Threshold\n",
            "99 episode , 11 step , 2.37 Loss, 0.24 Threshold\n",
            "100 episode , 74 step , 2.64 Loss, 0.24 Threshold\n",
            "101 episode , 11 step , 3.03 Loss, 0.23 Threshold\n",
            "102 episode , 58 step , 2.70 Loss, 0.23 Threshold\n",
            "103 episode , 79 step , 2.89 Loss, 0.22 Threshold\n",
            "104 episode , 92 step , 2.64 Loss, 0.21 Threshold\n",
            "105 episode , 118 step , 2.63 Loss, 0.20 Threshold\n",
            "106 episode , 86 step , 2.57 Loss, 0.19 Threshold\n",
            "107 episode , 37 step , 2.78 Loss, 0.19 Threshold\n",
            "108 episode , 87 step , 2.37 Loss, 0.18 Threshold\n",
            "109 episode , 79 step , 2.71 Loss, 0.17 Threshold\n",
            "110 episode , 40 step , 2.51 Loss, 0.17 Threshold\n",
            "111 episode , 94 step , 3.02 Loss, 0.16 Threshold\n",
            "112 episode , 112 step , 3.03 Loss, 0.15 Threshold\n",
            "113 episode , 44 step , 2.36 Loss, 0.15 Threshold\n",
            "114 episode , 101 step , 2.66 Loss, 0.14 Threshold\n",
            "115 episode , 86 step , 2.50 Loss, 0.14 Threshold\n",
            "116 episode , 105 step , 2.32 Loss, 0.13 Threshold\n",
            "117 episode , 66 step , 2.60 Loss, 0.13 Threshold\n",
            "118 episode , 87 step , 2.38 Loss, 0.12 Threshold\n",
            "119 episode , 34 step , 2.49 Loss, 0.12 Threshold\n",
            "120 episode , 84 step , 2.43 Loss, 0.12 Threshold\n",
            "121 episode , 18 step , 2.71 Loss, 0.12 Threshold\n",
            "122 episode , 31 step , 2.46 Loss, 0.11 Threshold\n",
            "123 episode , 51 step , 2.62 Loss, 0.11 Threshold\n",
            "124 episode , 46 step , 2.67 Loss, 0.11 Threshold\n",
            "125 episode , 39 step , 2.56 Loss, 0.11 Threshold\n",
            "126 episode , 38 step , 2.95 Loss, 0.11 Threshold\n",
            "127 episode , 23 step , 2.55 Loss, 0.10 Threshold\n",
            "128 episode , 90 step , 2.50 Loss, 0.10 Threshold\n",
            "129 episode , 21 step , 2.52 Loss, 0.10 Threshold\n",
            "130 episode , 81 step , 2.51 Loss, 0.10 Threshold\n",
            "131 episode , 18 step , 2.81 Loss, 0.09 Threshold\n",
            "132 episode , 66 step , 2.85 Loss, 0.09 Threshold\n",
            "133 episode , 16 step , 2.78 Loss, 0.09 Threshold\n",
            "134 episode , 21 step , 2.95 Loss, 0.09 Threshold\n",
            "135 episode , 79 step , 2.80 Loss, 0.09 Threshold\n",
            "136 episode , 86 step , 2.45 Loss, 0.08 Threshold\n",
            "137 episode , 71 step , 2.55 Loss, 0.08 Threshold\n",
            "138 episode , 11 step , 2.67 Loss, 0.08 Threshold\n",
            "139 episode , 78 step , 2.50 Loss, 0.08 Threshold\n",
            "140 episode , 76 step , 2.47 Loss, 0.08 Threshold\n",
            "141 episode , 206 step , 2.37 Loss, 0.07 Threshold\n",
            "142 episode , 25 step , 2.29 Loss, 0.07 Threshold\n",
            "143 episode , 226 step , 2.48 Loss, 0.06 Threshold\n",
            "144 episode , 12 step , 2.49 Loss, 0.06 Threshold\n",
            "145 episode , 349 step , 2.26 Loss, 0.05 Threshold\n",
            "146 episode , 20 step , 2.46 Loss, 0.05 Threshold\n",
            "147 episode , 93 step , 2.19 Loss, 0.05 Threshold\n",
            "148 episode , 15 step , 2.37 Loss, 0.05 Threshold\n",
            "149 episode , 98 step , 2.31 Loss, 0.05 Threshold\n",
            "150 episode , 103 step , 2.39 Loss, 0.05 Threshold\n",
            "151 episode , 118 step , 2.01 Loss, 0.05 Threshold\n",
            "152 episode , 47 step , 2.12 Loss, 0.04 Threshold\n",
            "153 episode , 86 step , 2.14 Loss, 0.04 Threshold\n",
            "154 episode , 85 step , 1.98 Loss, 0.04 Threshold\n",
            "155 episode , 12 step , 1.61 Loss, 0.04 Threshold\n",
            "156 episode , 102 step , 2.04 Loss, 0.04 Threshold\n",
            "157 episode , 106 step , 2.10 Loss, 0.04 Threshold\n",
            "158 episode , 12 step , 1.42 Loss, 0.04 Threshold\n",
            "159 episode , 89 step , 2.17 Loss, 0.04 Threshold\n",
            "160 episode , 10 step , 2.84 Loss, 0.04 Threshold\n",
            "161 episode , 73 step , 1.98 Loss, 0.04 Threshold\n",
            "162 episode , 113 step , 2.11 Loss, 0.03 Threshold\n",
            "163 episode , 261 step , 1.95 Loss, 0.03 Threshold\n",
            "164 episode , 108 step , 2.09 Loss, 0.03 Threshold\n",
            "165 episode , 13 step , 1.83 Loss, 0.03 Threshold\n",
            "166 episode , 151 step , 1.93 Loss, 0.03 Threshold\n",
            "167 episode , 13 step , 1.83 Loss, 0.03 Threshold\n",
            "168 episode , 125 step , 1.95 Loss, 0.03 Threshold\n",
            "169 episode , 167 step , 1.74 Loss, 0.03 Threshold\n",
            "170 episode , 155 step , 1.82 Loss, 0.02 Threshold\n",
            "171 episode , 109 step , 1.85 Loss, 0.02 Threshold\n",
            "172 episode , 118 step , 1.84 Loss, 0.02 Threshold\n",
            "173 episode , 352 step , 1.82 Loss, 0.02 Threshold\n",
            "174 episode , 153 step , 1.86 Loss, 0.02 Threshold\n",
            "175 episode , 117 step , 1.65 Loss, 0.02 Threshold\n",
            "176 episode , 126 step , 1.80 Loss, 0.02 Threshold\n",
            "177 episode , 306 step , 1.64 Loss, 0.02 Threshold\n",
            "178 episode , 123 step , 1.66 Loss, 0.02 Threshold\n",
            "179 episode , 316 step , 1.66 Loss, 0.02 Threshold\n",
            "180 episode , 168 step , 1.74 Loss, 0.02 Threshold\n",
            "181 episode , 155 step , 1.72 Loss, 0.02 Threshold\n",
            "182 episode , 1964 step , 1.27 Loss, 0.01 Threshold\n",
            "183 episode , 922 step , 0.80 Loss, 0.01 Threshold\n",
            "184 episode , 98 step , 0.70 Loss, 0.01 Threshold\n",
            "185 episode , 114 step , 0.62 Loss, 0.01 Threshold\n",
            "186 episode , 253 step , 0.62 Loss, 0.01 Threshold\n",
            "187 episode , 111 step , 0.64 Loss, 0.01 Threshold\n",
            "188 episode , 479 step , 0.62 Loss, 0.01 Threshold\n",
            "189 episode , 424 step , 0.51 Loss, 0.01 Threshold\n",
            "190 episode , 114 step , 0.48 Loss, 0.01 Threshold\n",
            "191 episode , 48 step , 0.62 Loss, 0.01 Threshold\n",
            "192 episode , 1948 step , 0.34 Loss, 0.01 Threshold\n",
            "193 episode , 2118 step , 0.20 Loss, 0.01 Threshold\n",
            "194 episode , 4404 step , 0.11 Loss, 0.01 Threshold\n",
            "195 episode , 5000 step , 0.05 Loss, 0.01 Threshold\n",
            "196 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "197 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "198 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "199 episode , 47 step , 0.05 Loss, 0.01 Threshold\n",
            "200 episode , 5000 step , 0.04 Loss, 0.01 Threshold\n",
            "201 episode , 5000 step , 0.04 Loss, 0.01 Threshold\n",
            "202 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "203 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "204 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "205 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "206 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "207 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "208 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "209 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "210 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "211 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "212 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "213 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n",
            "214 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "215 episode , 56 step , 0.03 Loss, 0.01 Threshold\n",
            "216 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "217 episode , 5000 step , 0.03 Loss, 0.01 Threshold\n",
            "218 episode , 5000 step , 0.02 Loss, 0.01 Threshold\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}