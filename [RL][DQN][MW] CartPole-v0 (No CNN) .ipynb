{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdRIkfqYFxXi3uD0/HtMAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDQN%5D%5BMW%5D%20CartPole-v0%20(No%20CNN)%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "69fd0c37-ed7b-4443-8749-0a36889d86a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 66%\r\rReading package lists... 66%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 88%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps = 10000\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwNNaGeeyMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        \n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 256\n",
        "class DQN_custom(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs):\n",
        "        super(DQN_custom, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "3e608cd4-d912-476f-bc00-45ff0fee9517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.01\n",
        "MEMORY_SIZE = 100000\n",
        "EPISODE_SIZE = 1000\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "n_obvs = 4\n",
        "\n",
        "policy_net = DQN_custom(n_obvs, n_actions).to(device)\n",
        "policy_net.eval()\n",
        "target_net = DQN_custom(n_obvs, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(obs):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            obs = torch.FloatTensor(obs).to(device)\n",
        "            output = policy_net(obs).cpu()\n",
        "            predicted = np.argmax(output.data.numpy())\n",
        "            return torch.tensor([[predicted]],device=device, dtype=torch.long)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.cat(batch.state).to(device)\n",
        "    reward_batch = torch.cat(batch.reward).to(device)\n",
        "    action_batch = torch.cat(batch.action).to(device)\n",
        " \n",
        "    non_final_mask = map(lambda s: s is not None, batch.next_state)\n",
        "    non_final_mask = tuple(non_final_mask)\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.BoolTensor(non_final_mask)\n",
        "\n",
        "    # We don't want to backprop through the expected action values and volatile\n",
        "    # will save us on temporarily changing the model parameters'\n",
        "    # requires_grad to False!\n",
        "    non_final_next_states = [s.view(1, -1) for s in batch.next_state if s is not None]\n",
        "    non_final_next_states = torch.cat(non_final_next_states, 0).to(device)\n",
        "  \n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Huber 손실 계산\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    return loss.cpu().data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "03e79f94-c3d5-4985-fa11-df6b6611f2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_episodes = EPISODE_SIZE\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_loss = 0\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action.item())\n",
        "        if done:\n",
        "            reward -= 100\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        if not done:\n",
        "          next_state = torch.FloatTensor([next_obv])\n",
        "        else:\n",
        "          next_state = None\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.push(torch.FloatTensor([obv]), action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        total_loss += optimize_model()\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Loss, %.2f Threshold'%(i_episode,t+1,total_loss/(t+1),E))\n",
        "            plot_durations()\n",
        "            total_loss = 0\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 11 step , -1.00 Loss, 0.90 Threshold\n",
            "1 episode , 17 step , -1.00 Loss, 0.89 Threshold\n",
            "2 episode , 13 step , -1.00 Loss, 0.88 Threshold\n",
            "3 episode , 16 step , -1.00 Loss, 0.87 Threshold\n",
            "4 episode , 15 step , -1.00 Loss, 0.87 Threshold\n",
            "5 episode , 17 step , -1.00 Loss, 0.86 Threshold\n",
            "6 episode , 14 step , -1.00 Loss, 0.86 Threshold\n",
            "7 episode , 19 step , -1.00 Loss, 0.85 Threshold\n",
            "8 episode , 21 step , 4.27 Loss, 0.84 Threshold\n",
            "9 episode , 11 step , 5.84 Loss, 0.83 Threshold\n",
            "10 episode , 17 step , 6.20 Loss, 0.83 Threshold\n",
            "11 episode , 14 step , 5.51 Loss, 0.82 Threshold\n",
            "12 episode , 44 step , 5.19 Loss, 0.80 Threshold\n",
            "13 episode , 10 step , 4.86 Loss, 0.80 Threshold\n",
            "14 episode , 34 step , 4.91 Loss, 0.79 Threshold\n",
            "15 episode , 12 step , 4.73 Loss, 0.78 Threshold\n",
            "16 episode , 16 step , 5.23 Loss, 0.78 Threshold\n",
            "17 episode , 12 step , 3.89 Loss, 0.77 Threshold\n",
            "18 episode , 19 step , 4.59 Loss, 0.76 Threshold\n",
            "19 episode , 12 step , 4.57 Loss, 0.76 Threshold\n",
            "20 episode , 31 step , 4.54 Loss, 0.75 Threshold\n",
            "21 episode , 14 step , 4.95 Loss, 0.74 Threshold\n",
            "22 episode , 13 step , 3.54 Loss, 0.74 Threshold\n",
            "23 episode , 73 step , 3.04 Loss, 0.71 Threshold\n",
            "24 episode , 58 step , 2.63 Loss, 0.69 Threshold\n",
            "25 episode , 32 step , 2.08 Loss, 0.68 Threshold\n",
            "26 episode , 16 step , 2.12 Loss, 0.68 Threshold\n",
            "27 episode , 34 step , 2.28 Loss, 0.66 Threshold\n",
            "28 episode , 18 step , 1.76 Loss, 0.66 Threshold\n",
            "29 episode , 25 step , 2.02 Loss, 0.65 Threshold\n",
            "30 episode , 31 step , 1.94 Loss, 0.64 Threshold\n",
            "31 episode , 24 step , 2.87 Loss, 0.63 Threshold\n",
            "32 episode , 17 step , 2.03 Loss, 0.63 Threshold\n",
            "33 episode , 14 step , 2.16 Loss, 0.62 Threshold\n",
            "34 episode , 14 step , 2.47 Loss, 0.62 Threshold\n",
            "35 episode , 14 step , 2.03 Loss, 0.61 Threshold\n",
            "36 episode , 29 step , 2.10 Loss, 0.61 Threshold\n",
            "37 episode , 16 step , 2.41 Loss, 0.60 Threshold\n",
            "38 episode , 21 step , 2.07 Loss, 0.60 Threshold\n",
            "39 episode , 12 step , 2.13 Loss, 0.59 Threshold\n",
            "40 episode , 18 step , 2.26 Loss, 0.59 Threshold\n",
            "41 episode , 12 step , 2.25 Loss, 0.58 Threshold\n",
            "42 episode , 29 step , 2.30 Loss, 0.57 Threshold\n",
            "43 episode , 30 step , 2.12 Loss, 0.57 Threshold\n",
            "44 episode , 11 step , 2.01 Loss, 0.56 Threshold\n",
            "45 episode , 39 step , 2.04 Loss, 0.55 Threshold\n",
            "46 episode , 16 step , 2.24 Loss, 0.55 Threshold\n",
            "47 episode , 18 step , 1.79 Loss, 0.54 Threshold\n",
            "48 episode , 29 step , 1.98 Loss, 0.54 Threshold\n",
            "49 episode , 29 step , 1.84 Loss, 0.53 Threshold\n",
            "50 episode , 51 step , 1.91 Loss, 0.52 Threshold\n",
            "51 episode , 82 step , 1.98 Loss, 0.50 Threshold\n",
            "52 episode , 15 step , 2.14 Loss, 0.49 Threshold\n",
            "53 episode , 18 step , 1.91 Loss, 0.49 Threshold\n",
            "54 episode , 20 step , 2.11 Loss, 0.48 Threshold\n",
            "55 episode , 15 step , 2.14 Loss, 0.48 Threshold\n",
            "56 episode , 13 step , 1.75 Loss, 0.48 Threshold\n",
            "57 episode , 21 step , 1.81 Loss, 0.47 Threshold\n",
            "58 episode , 35 step , 1.85 Loss, 0.46 Threshold\n",
            "59 episode , 11 step , 1.83 Loss, 0.46 Threshold\n",
            "60 episode , 10 step , 1.78 Loss, 0.46 Threshold\n",
            "61 episode , 42 step , 1.92 Loss, 0.45 Threshold\n",
            "62 episode , 28 step , 1.73 Loss, 0.44 Threshold\n",
            "63 episode , 32 step , 1.41 Loss, 0.44 Threshold\n",
            "64 episode , 15 step , 1.64 Loss, 0.43 Threshold\n",
            "65 episode , 35 step , 1.45 Loss, 0.43 Threshold\n",
            "66 episode , 12 step , 1.35 Loss, 0.42 Threshold\n",
            "67 episode , 36 step , 1.58 Loss, 0.42 Threshold\n",
            "68 episode , 25 step , 1.46 Loss, 0.41 Threshold\n",
            "69 episode , 41 step , 1.26 Loss, 0.40 Threshold\n",
            "70 episode , 34 step , 1.62 Loss, 0.40 Threshold\n",
            "71 episode , 32 step , 1.90 Loss, 0.39 Threshold\n",
            "72 episode , 42 step , 1.41 Loss, 0.38 Threshold\n",
            "73 episode , 21 step , 1.25 Loss, 0.38 Threshold\n",
            "74 episode , 14 step , 1.45 Loss, 0.38 Threshold\n",
            "75 episode , 16 step , 1.52 Loss, 0.37 Threshold\n",
            "76 episode , 12 step , 1.26 Loss, 0.37 Threshold\n",
            "77 episode , 13 step , 1.43 Loss, 0.37 Threshold\n",
            "78 episode , 16 step , 1.61 Loss, 0.37 Threshold\n",
            "79 episode , 11 step , 1.48 Loss, 0.36 Threshold\n",
            "80 episode , 26 step , 1.24 Loss, 0.36 Threshold\n",
            "81 episode , 11 step , 1.87 Loss, 0.36 Threshold\n",
            "82 episode , 52 step , 1.68 Loss, 0.35 Threshold\n",
            "83 episode , 18 step , 1.37 Loss, 0.34 Threshold\n",
            "84 episode , 36 step , 1.55 Loss, 0.34 Threshold\n",
            "85 episode , 51 step , 1.34 Loss, 0.33 Threshold\n",
            "86 episode , 45 step , 1.34 Loss, 0.32 Threshold\n",
            "87 episode , 12 step , 1.21 Loss, 0.32 Threshold\n",
            "88 episode , 43 step , 1.00 Loss, 0.31 Threshold\n",
            "89 episode , 11 step , 1.00 Loss, 0.31 Threshold\n",
            "90 episode , 31 step , 1.14 Loss, 0.31 Threshold\n",
            "91 episode , 64 step , 1.54 Loss, 0.30 Threshold\n",
            "92 episode , 14 step , 1.61 Loss, 0.30 Threshold\n",
            "93 episode , 89 step , 1.11 Loss, 0.28 Threshold\n",
            "94 episode , 132 step , 1.14 Loss, 0.27 Threshold\n",
            "95 episode , 32 step , 1.04 Loss, 0.26 Threshold\n",
            "96 episode , 37 step , 1.06 Loss, 0.26 Threshold\n",
            "97 episode , 67 step , 1.07 Loss, 0.25 Threshold\n",
            "98 episode , 64 step , 1.15 Loss, 0.24 Threshold\n",
            "99 episode , 14 step , 1.55 Loss, 0.24 Threshold\n",
            "100 episode , 89 step , 1.06 Loss, 0.23 Threshold\n",
            "101 episode , 89 step , 1.31 Loss, 0.22 Threshold\n",
            "102 episode , 65 step , 1.26 Loss, 0.21 Threshold\n",
            "103 episode , 11 step , 1.74 Loss, 0.21 Threshold\n",
            "104 episode , 226 step , 1.23 Loss, 0.19 Threshold\n",
            "105 episode , 170 step , 0.89 Loss, 0.18 Threshold\n",
            "106 episode , 146 step , 1.03 Loss, 0.17 Threshold\n",
            "107 episode , 154 step , 0.96 Loss, 0.15 Threshold\n",
            "108 episode , 150 step , 0.99 Loss, 0.14 Threshold\n",
            "109 episode , 98 step , 1.06 Loss, 0.14 Threshold\n",
            "110 episode , 215 step , 0.94 Loss, 0.12 Threshold\n",
            "111 episode , 97 step , 1.08 Loss, 0.12 Threshold\n",
            "112 episode , 117 step , 1.07 Loss, 0.11 Threshold\n",
            "113 episode , 101 step , 0.96 Loss, 0.11 Threshold\n",
            "114 episode , 88 step , 1.02 Loss, 0.10 Threshold\n",
            "115 episode , 106 step , 0.98 Loss, 0.10 Threshold\n",
            "116 episode , 199 step , 0.93 Loss, 0.09 Threshold\n",
            "117 episode , 97 step , 0.75 Loss, 0.09 Threshold\n",
            "118 episode , 34 step , 0.74 Loss, 0.08 Threshold\n",
            "119 episode , 87 step , 0.82 Loss, 0.08 Threshold\n",
            "120 episode , 123 step , 0.92 Loss, 0.08 Threshold\n",
            "121 episode , 94 step , 1.10 Loss, 0.07 Threshold\n",
            "122 episode , 92 step , 1.02 Loss, 0.07 Threshold\n",
            "123 episode , 100 step , 1.04 Loss, 0.07 Threshold\n",
            "124 episode , 105 step , 0.93 Loss, 0.07 Threshold\n",
            "125 episode , 106 step , 0.94 Loss, 0.06 Threshold\n",
            "126 episode , 94 step , 0.84 Loss, 0.06 Threshold\n",
            "127 episode , 91 step , 0.87 Loss, 0.06 Threshold\n",
            "128 episode , 90 step , 1.21 Loss, 0.06 Threshold\n",
            "129 episode , 96 step , 1.02 Loss, 0.05 Threshold\n",
            "130 episode , 88 step , 0.85 Loss, 0.05 Threshold\n",
            "131 episode , 84 step , 1.22 Loss, 0.05 Threshold\n",
            "132 episode , 70 step , 1.17 Loss, 0.05 Threshold\n",
            "133 episode , 113 step , 0.96 Loss, 0.05 Threshold\n",
            "134 episode , 94 step , 1.11 Loss, 0.04 Threshold\n",
            "135 episode , 98 step , 0.97 Loss, 0.04 Threshold\n",
            "136 episode , 122 step , 1.16 Loss, 0.04 Threshold\n",
            "137 episode , 85 step , 0.86 Loss, 0.04 Threshold\n",
            "138 episode , 89 step , 1.00 Loss, 0.04 Threshold\n",
            "139 episode , 78 step , 0.94 Loss, 0.04 Threshold\n",
            "140 episode , 119 step , 1.16 Loss, 0.04 Threshold\n",
            "141 episode , 12 step , 1.22 Loss, 0.04 Threshold\n",
            "142 episode , 116 step , 1.10 Loss, 0.03 Threshold\n",
            "143 episode , 122 step , 1.21 Loss, 0.03 Threshold\n",
            "144 episode , 124 step , 0.97 Loss, 0.03 Threshold\n",
            "145 episode , 127 step , 1.14 Loss, 0.03 Threshold\n",
            "146 episode , 113 step , 1.03 Loss, 0.03 Threshold\n",
            "147 episode , 115 step , 0.94 Loss, 0.03 Threshold\n",
            "148 episode , 110 step , 1.22 Loss, 0.03 Threshold\n",
            "149 episode , 120 step , 0.98 Loss, 0.03 Threshold\n",
            "150 episode , 101 step , 1.07 Loss, 0.03 Threshold\n",
            "151 episode , 233 step , 1.03 Loss, 0.02 Threshold\n",
            "152 episode , 123 step , 1.10 Loss, 0.02 Threshold\n",
            "153 episode , 130 step , 1.03 Loss, 0.02 Threshold\n",
            "154 episode , 197 step , 0.91 Loss, 0.02 Threshold\n",
            "155 episode , 111 step , 0.81 Loss, 0.02 Threshold\n",
            "156 episode , 96 step , 0.90 Loss, 0.02 Threshold\n",
            "157 episode , 98 step , 0.87 Loss, 0.02 Threshold\n",
            "158 episode , 131 step , 1.00 Loss, 0.02 Threshold\n",
            "159 episode , 116 step , 0.95 Loss, 0.02 Threshold\n",
            "160 episode , 128 step , 1.16 Loss, 0.02 Threshold\n",
            "161 episode , 55 step , 1.00 Loss, 0.02 Threshold\n",
            "162 episode , 57 step , 0.89 Loss, 0.02 Threshold\n",
            "163 episode , 133 step , 1.13 Loss, 0.02 Threshold\n",
            "164 episode , 188 step , 0.89 Loss, 0.02 Threshold\n",
            "165 episode , 130 step , 0.88 Loss, 0.02 Threshold\n",
            "166 episode , 77 step , 1.10 Loss, 0.02 Threshold\n",
            "167 episode , 598 step , 0.93 Loss, 0.01 Threshold\n",
            "168 episode , 587 step , 0.92 Loss, 0.01 Threshold\n",
            "169 episode , 380 step , 0.94 Loss, 0.01 Threshold\n",
            "170 episode , 84 step , 0.74 Loss, 0.01 Threshold\n",
            "171 episode , 130 step , 0.93 Loss, 0.01 Threshold\n",
            "172 episode , 81 step , 1.04 Loss, 0.01 Threshold\n",
            "173 episode , 89 step , 0.97 Loss, 0.01 Threshold\n",
            "174 episode , 77 step , 1.06 Loss, 0.01 Threshold\n",
            "175 episode , 79 step , 1.06 Loss, 0.01 Threshold\n",
            "176 episode , 110 step , 1.00 Loss, 0.01 Threshold\n",
            "177 episode , 104 step , 0.94 Loss, 0.01 Threshold\n",
            "178 episode , 96 step , 1.02 Loss, 0.01 Threshold\n",
            "179 episode , 86 step , 1.08 Loss, 0.01 Threshold\n",
            "180 episode , 115 step , 1.00 Loss, 0.01 Threshold\n",
            "181 episode , 74 step , 1.09 Loss, 0.01 Threshold\n",
            "182 episode , 75 step , 0.81 Loss, 0.01 Threshold\n",
            "183 episode , 77 step , 0.97 Loss, 0.01 Threshold\n",
            "184 episode , 76 step , 1.11 Loss, 0.01 Threshold\n",
            "185 episode , 73 step , 0.96 Loss, 0.01 Threshold\n",
            "186 episode , 103 step , 1.00 Loss, 0.01 Threshold\n",
            "187 episode , 105 step , 1.08 Loss, 0.01 Threshold\n",
            "188 episode , 115 step , 1.02 Loss, 0.01 Threshold\n",
            "189 episode , 90 step , 0.89 Loss, 0.01 Threshold\n",
            "190 episode , 98 step , 1.13 Loss, 0.01 Threshold\n",
            "191 episode , 122 step , 0.93 Loss, 0.01 Threshold\n",
            "192 episode , 108 step , 0.93 Loss, 0.01 Threshold\n",
            "193 episode , 60 step , 0.89 Loss, 0.01 Threshold\n",
            "194 episode , 66 step , 0.87 Loss, 0.01 Threshold\n",
            "195 episode , 106 step , 0.74 Loss, 0.01 Threshold\n",
            "196 episode , 104 step , 0.98 Loss, 0.01 Threshold\n",
            "197 episode , 66 step , 1.13 Loss, 0.01 Threshold\n",
            "198 episode , 96 step , 0.96 Loss, 0.01 Threshold\n",
            "199 episode , 100 step , 0.99 Loss, 0.01 Threshold\n",
            "200 episode , 107 step , 0.84 Loss, 0.01 Threshold\n",
            "201 episode , 100 step , 0.96 Loss, 0.01 Threshold\n",
            "202 episode , 95 step , 1.01 Loss, 0.01 Threshold\n",
            "203 episode , 91 step , 1.05 Loss, 0.01 Threshold\n",
            "204 episode , 96 step , 1.12 Loss, 0.01 Threshold\n",
            "205 episode , 96 step , 1.23 Loss, 0.01 Threshold\n",
            "206 episode , 97 step , 1.07 Loss, 0.01 Threshold\n",
            "207 episode , 94 step , 1.07 Loss, 0.01 Threshold\n",
            "208 episode , 89 step , 1.16 Loss, 0.01 Threshold\n",
            "209 episode , 90 step , 1.29 Loss, 0.01 Threshold\n",
            "210 episode , 91 step , 1.23 Loss, 0.01 Threshold\n",
            "211 episode , 90 step , 0.97 Loss, 0.01 Threshold\n",
            "212 episode , 104 step , 1.00 Loss, 0.01 Threshold\n",
            "213 episode , 84 step , 1.08 Loss, 0.01 Threshold\n",
            "214 episode , 80 step , 1.08 Loss, 0.01 Threshold\n",
            "215 episode , 98 step , 0.97 Loss, 0.01 Threshold\n",
            "216 episode , 92 step , 1.02 Loss, 0.01 Threshold\n",
            "217 episode , 87 step , 0.93 Loss, 0.01 Threshold\n",
            "218 episode , 97 step , 1.07 Loss, 0.01 Threshold\n",
            "219 episode , 98 step , 1.00 Loss, 0.01 Threshold\n",
            "220 episode , 105 step , 1.07 Loss, 0.01 Threshold\n",
            "221 episode , 104 step , 1.40 Loss, 0.01 Threshold\n",
            "222 episode , 96 step , 1.23 Loss, 0.01 Threshold\n",
            "223 episode , 101 step , 1.14 Loss, 0.01 Threshold\n",
            "224 episode , 111 step , 1.27 Loss, 0.01 Threshold\n",
            "225 episode , 81 step , 0.86 Loss, 0.01 Threshold\n",
            "226 episode , 26 step , 0.90 Loss, 0.01 Threshold\n",
            "227 episode , 98 step , 1.21 Loss, 0.01 Threshold\n",
            "228 episode , 42 step , 1.18 Loss, 0.01 Threshold\n",
            "229 episode , 103 step , 1.03 Loss, 0.01 Threshold\n",
            "230 episode , 69 step , 1.04 Loss, 0.01 Threshold\n",
            "231 episode , 93 step , 1.19 Loss, 0.01 Threshold\n",
            "232 episode , 76 step , 1.16 Loss, 0.01 Threshold\n",
            "233 episode , 92 step , 1.27 Loss, 0.01 Threshold\n",
            "234 episode , 102 step , 1.34 Loss, 0.01 Threshold\n",
            "235 episode , 102 step , 1.21 Loss, 0.01 Threshold\n",
            "236 episode , 54 step , 1.07 Loss, 0.01 Threshold\n",
            "237 episode , 35 step , 0.81 Loss, 0.01 Threshold\n",
            "238 episode , 32 step , 1.45 Loss, 0.01 Threshold\n",
            "239 episode , 23 step , 1.02 Loss, 0.01 Threshold\n",
            "240 episode , 32 step , 1.20 Loss, 0.01 Threshold\n",
            "241 episode , 23 step , 1.42 Loss, 0.01 Threshold\n",
            "242 episode , 90 step , 1.18 Loss, 0.01 Threshold\n",
            "243 episode , 78 step , 1.11 Loss, 0.01 Threshold\n",
            "244 episode , 94 step , 1.38 Loss, 0.01 Threshold\n",
            "245 episode , 83 step , 1.34 Loss, 0.01 Threshold\n",
            "246 episode , 91 step , 1.22 Loss, 0.01 Threshold\n",
            "247 episode , 93 step , 0.97 Loss, 0.01 Threshold\n",
            "248 episode , 55 step , 1.15 Loss, 0.01 Threshold\n",
            "249 episode , 94 step , 1.13 Loss, 0.01 Threshold\n",
            "250 episode , 76 step , 1.22 Loss, 0.01 Threshold\n",
            "251 episode , 67 step , 1.43 Loss, 0.01 Threshold\n",
            "252 episode , 57 step , 1.42 Loss, 0.01 Threshold\n",
            "253 episode , 40 step , 1.60 Loss, 0.01 Threshold\n",
            "254 episode , 40 step , 1.33 Loss, 0.01 Threshold\n",
            "255 episode , 51 step , 1.24 Loss, 0.01 Threshold\n",
            "256 episode , 86 step , 1.15 Loss, 0.01 Threshold\n",
            "257 episode , 93 step , 1.38 Loss, 0.01 Threshold\n",
            "258 episode , 95 step , 1.15 Loss, 0.01 Threshold\n",
            "259 episode , 40 step , 1.07 Loss, 0.01 Threshold\n",
            "260 episode , 53 step , 0.99 Loss, 0.01 Threshold\n",
            "261 episode , 41 step , 1.39 Loss, 0.01 Threshold\n",
            "262 episode , 42 step , 1.34 Loss, 0.01 Threshold\n",
            "263 episode , 79 step , 1.23 Loss, 0.01 Threshold\n",
            "264 episode , 100 step , 1.29 Loss, 0.01 Threshold\n",
            "265 episode , 77 step , 1.20 Loss, 0.01 Threshold\n",
            "266 episode , 63 step , 1.56 Loss, 0.01 Threshold\n",
            "267 episode , 92 step , 1.34 Loss, 0.01 Threshold\n",
            "268 episode , 100 step , 1.24 Loss, 0.01 Threshold\n",
            "269 episode , 43 step , 1.33 Loss, 0.01 Threshold\n",
            "270 episode , 99 step , 1.29 Loss, 0.01 Threshold\n",
            "271 episode , 95 step , 1.56 Loss, 0.01 Threshold\n",
            "272 episode , 89 step , 1.44 Loss, 0.01 Threshold\n",
            "273 episode , 94 step , 1.27 Loss, 0.01 Threshold\n",
            "274 episode , 94 step , 1.16 Loss, 0.01 Threshold\n",
            "275 episode , 64 step , 1.24 Loss, 0.01 Threshold\n",
            "276 episode , 45 step , 1.23 Loss, 0.01 Threshold\n",
            "277 episode , 93 step , 1.17 Loss, 0.01 Threshold\n",
            "278 episode , 72 step , 1.20 Loss, 0.01 Threshold\n",
            "279 episode , 101 step , 1.38 Loss, 0.01 Threshold\n",
            "280 episode , 91 step , 1.40 Loss, 0.01 Threshold\n",
            "281 episode , 52 step , 1.42 Loss, 0.01 Threshold\n",
            "282 episode , 97 step , 1.40 Loss, 0.01 Threshold\n",
            "283 episode , 65 step , 1.38 Loss, 0.01 Threshold\n",
            "284 episode , 95 step , 1.57 Loss, 0.01 Threshold\n",
            "285 episode , 110 step , 1.36 Loss, 0.01 Threshold\n",
            "286 episode , 91 step , 1.53 Loss, 0.01 Threshold\n",
            "287 episode , 99 step , 1.44 Loss, 0.01 Threshold\n",
            "288 episode , 67 step , 1.36 Loss, 0.01 Threshold\n",
            "289 episode , 80 step , 1.09 Loss, 0.01 Threshold\n",
            "290 episode , 93 step , 1.36 Loss, 0.01 Threshold\n",
            "291 episode , 95 step , 1.42 Loss, 0.01 Threshold\n",
            "292 episode , 14 step , 1.49 Loss, 0.01 Threshold\n",
            "293 episode , 91 step , 1.35 Loss, 0.01 Threshold\n",
            "294 episode , 90 step , 1.58 Loss, 0.01 Threshold\n",
            "295 episode , 94 step , 1.43 Loss, 0.01 Threshold\n",
            "296 episode , 86 step , 1.30 Loss, 0.01 Threshold\n",
            "297 episode , 104 step , 1.26 Loss, 0.01 Threshold\n",
            "298 episode , 96 step , 1.22 Loss, 0.01 Threshold\n",
            "299 episode , 106 step , 1.20 Loss, 0.01 Threshold\n",
            "300 episode , 105 step , 1.20 Loss, 0.01 Threshold\n",
            "301 episode , 79 step , 1.73 Loss, 0.01 Threshold\n",
            "302 episode , 30 step , 1.70 Loss, 0.01 Threshold\n",
            "303 episode , 47 step , 1.55 Loss, 0.01 Threshold\n",
            "304 episode , 22 step , 1.41 Loss, 0.01 Threshold\n",
            "305 episode , 26 step , 1.56 Loss, 0.01 Threshold\n",
            "306 episode , 16 step , 2.01 Loss, 0.01 Threshold\n",
            "307 episode , 13 step , 1.98 Loss, 0.01 Threshold\n",
            "308 episode , 91 step , 1.60 Loss, 0.01 Threshold\n",
            "309 episode , 16 step , 1.09 Loss, 0.01 Threshold\n",
            "310 episode , 18 step , 1.29 Loss, 0.01 Threshold\n",
            "311 episode , 27 step , 1.89 Loss, 0.01 Threshold\n",
            "312 episode , 13 step , 1.63 Loss, 0.01 Threshold\n",
            "313 episode , 93 step , 1.51 Loss, 0.01 Threshold\n",
            "314 episode , 96 step , 1.39 Loss, 0.01 Threshold\n",
            "315 episode , 91 step , 1.64 Loss, 0.01 Threshold\n",
            "316 episode , 21 step , 1.48 Loss, 0.01 Threshold\n",
            "317 episode , 124 step , 1.46 Loss, 0.01 Threshold\n",
            "318 episode , 97 step , 1.38 Loss, 0.01 Threshold\n",
            "319 episode , 101 step , 1.38 Loss, 0.01 Threshold\n",
            "320 episode , 121 step , 1.33 Loss, 0.01 Threshold\n",
            "321 episode , 98 step , 1.61 Loss, 0.01 Threshold\n",
            "322 episode , 104 step , 1.43 Loss, 0.01 Threshold\n",
            "323 episode , 121 step , 1.35 Loss, 0.01 Threshold\n",
            "324 episode , 99 step , 1.48 Loss, 0.01 Threshold\n",
            "325 episode , 100 step , 1.66 Loss, 0.01 Threshold\n",
            "326 episode , 99 step , 1.64 Loss, 0.01 Threshold\n",
            "327 episode , 104 step , 1.29 Loss, 0.01 Threshold\n",
            "328 episode , 116 step , 1.60 Loss, 0.01 Threshold\n",
            "329 episode , 111 step , 1.47 Loss, 0.01 Threshold\n",
            "330 episode , 103 step , 1.54 Loss, 0.01 Threshold\n",
            "331 episode , 117 step , 1.70 Loss, 0.01 Threshold\n",
            "332 episode , 107 step , 1.42 Loss, 0.01 Threshold\n",
            "333 episode , 115 step , 1.46 Loss, 0.01 Threshold\n",
            "334 episode , 112 step , 1.37 Loss, 0.01 Threshold\n",
            "335 episode , 119 step , 1.26 Loss, 0.01 Threshold\n",
            "336 episode , 124 step , 1.48 Loss, 0.01 Threshold\n",
            "337 episode , 121 step , 1.52 Loss, 0.01 Threshold\n",
            "338 episode , 222 step , 1.38 Loss, 0.01 Threshold\n",
            "339 episode , 119 step , 1.75 Loss, 0.01 Threshold\n",
            "340 episode , 139 step , 1.63 Loss, 0.01 Threshold\n",
            "341 episode , 177 step , 1.52 Loss, 0.01 Threshold\n",
            "342 episode , 110 step , 1.60 Loss, 0.01 Threshold\n",
            "343 episode , 235 step , 1.45 Loss, 0.01 Threshold\n",
            "344 episode , 104 step , 1.41 Loss, 0.01 Threshold\n",
            "345 episode , 120 step , 1.53 Loss, 0.01 Threshold\n",
            "346 episode , 114 step , 1.36 Loss, 0.01 Threshold\n",
            "347 episode , 94 step , 1.29 Loss, 0.01 Threshold\n",
            "348 episode , 98 step , 1.38 Loss, 0.01 Threshold\n",
            "349 episode , 102 step , 1.44 Loss, 0.01 Threshold\n",
            "350 episode , 111 step , 1.59 Loss, 0.01 Threshold\n",
            "351 episode , 97 step , 1.37 Loss, 0.01 Threshold\n",
            "352 episode , 113 step , 1.32 Loss, 0.01 Threshold\n",
            "353 episode , 111 step , 1.31 Loss, 0.01 Threshold\n",
            "354 episode , 205 step , 1.47 Loss, 0.01 Threshold\n",
            "355 episode , 396 step , 1.53 Loss, 0.01 Threshold\n",
            "356 episode , 150 step , 1.32 Loss, 0.01 Threshold\n",
            "357 episode , 78 step , 1.43 Loss, 0.01 Threshold\n",
            "358 episode , 185 step , 1.45 Loss, 0.01 Threshold\n",
            "359 episode , 82 step , 1.25 Loss, 0.01 Threshold\n",
            "360 episode , 66 step , 1.60 Loss, 0.01 Threshold\n",
            "361 episode , 66 step , 1.49 Loss, 0.01 Threshold\n",
            "362 episode , 193 step , 1.36 Loss, 0.01 Threshold\n",
            "363 episode , 107 step , 1.30 Loss, 0.01 Threshold\n",
            "364 episode , 70 step , 1.37 Loss, 0.01 Threshold\n",
            "365 episode , 91 step , 1.26 Loss, 0.01 Threshold\n",
            "366 episode , 245 step , 1.49 Loss, 0.01 Threshold\n",
            "367 episode , 100 step , 1.33 Loss, 0.01 Threshold\n",
            "368 episode , 73 step , 1.50 Loss, 0.01 Threshold\n",
            "369 episode , 62 step , 1.51 Loss, 0.01 Threshold\n",
            "370 episode , 65 step , 1.41 Loss, 0.01 Threshold\n",
            "371 episode , 201 step , 1.40 Loss, 0.01 Threshold\n",
            "372 episode , 67 step , 1.48 Loss, 0.01 Threshold\n",
            "373 episode , 14 step , 1.05 Loss, 0.01 Threshold\n",
            "374 episode , 119 step , 1.36 Loss, 0.01 Threshold\n",
            "375 episode , 187 step , 1.58 Loss, 0.01 Threshold\n",
            "376 episode , 103 step , 1.37 Loss, 0.01 Threshold\n",
            "377 episode , 167 step , 1.39 Loss, 0.01 Threshold\n",
            "378 episode , 203 step , 1.42 Loss, 0.01 Threshold\n",
            "379 episode , 191 step , 1.41 Loss, 0.01 Threshold\n",
            "380 episode , 163 step , 1.35 Loss, 0.01 Threshold\n",
            "381 episode , 70 step , 1.42 Loss, 0.01 Threshold\n",
            "382 episode , 84 step , 1.15 Loss, 0.01 Threshold\n",
            "383 episode , 73 step , 1.49 Loss, 0.01 Threshold\n",
            "384 episode , 256 step , 1.30 Loss, 0.01 Threshold\n",
            "385 episode , 286 step , 1.42 Loss, 0.01 Threshold\n",
            "386 episode , 240 step , 1.40 Loss, 0.01 Threshold\n",
            "387 episode , 93 step , 1.45 Loss, 0.01 Threshold\n",
            "388 episode , 72 step , 1.43 Loss, 0.01 Threshold\n",
            "389 episode , 233 step , 1.35 Loss, 0.01 Threshold\n",
            "390 episode , 234 step , 1.42 Loss, 0.01 Threshold\n",
            "391 episode , 80 step , 1.27 Loss, 0.01 Threshold\n",
            "392 episode , 75 step , 1.57 Loss, 0.01 Threshold\n",
            "393 episode , 77 step , 1.60 Loss, 0.01 Threshold\n",
            "394 episode , 80 step , 1.39 Loss, 0.01 Threshold\n",
            "395 episode , 69 step , 1.25 Loss, 0.01 Threshold\n",
            "396 episode , 90 step , 1.25 Loss, 0.01 Threshold\n",
            "397 episode , 74 step , 1.49 Loss, 0.01 Threshold\n",
            "398 episode , 74 step , 1.30 Loss, 0.01 Threshold\n",
            "399 episode , 68 step , 1.28 Loss, 0.01 Threshold\n",
            "400 episode , 81 step , 1.30 Loss, 0.01 Threshold\n",
            "401 episode , 125 step , 1.45 Loss, 0.01 Threshold\n",
            "402 episode , 72 step , 1.31 Loss, 0.01 Threshold\n",
            "403 episode , 84 step , 1.30 Loss, 0.01 Threshold\n",
            "404 episode , 246 step , 1.34 Loss, 0.01 Threshold\n",
            "405 episode , 69 step , 1.27 Loss, 0.01 Threshold\n",
            "406 episode , 218 step , 1.20 Loss, 0.01 Threshold\n",
            "407 episode , 271 step , 1.37 Loss, 0.01 Threshold\n",
            "408 episode , 80 step , 1.35 Loss, 0.01 Threshold\n",
            "409 episode , 75 step , 1.56 Loss, 0.01 Threshold\n",
            "410 episode , 273 step , 1.41 Loss, 0.01 Threshold\n",
            "411 episode , 331 step , 1.45 Loss, 0.01 Threshold\n",
            "412 episode , 246 step , 1.45 Loss, 0.01 Threshold\n",
            "413 episode , 249 step , 1.35 Loss, 0.01 Threshold\n",
            "414 episode , 281 step , 1.37 Loss, 0.01 Threshold\n",
            "415 episode , 238 step , 1.37 Loss, 0.01 Threshold\n",
            "416 episode , 247 step , 1.41 Loss, 0.01 Threshold\n",
            "417 episode , 244 step , 1.36 Loss, 0.01 Threshold\n",
            "418 episode , 368 step , 1.27 Loss, 0.01 Threshold\n",
            "419 episode , 280 step , 1.21 Loss, 0.01 Threshold\n",
            "420 episode , 334 step , 1.30 Loss, 0.01 Threshold\n",
            "421 episode , 212 step , 1.37 Loss, 0.01 Threshold\n",
            "422 episode , 195 step , 1.20 Loss, 0.01 Threshold\n",
            "423 episode , 231 step , 1.22 Loss, 0.01 Threshold\n",
            "424 episode , 225 step , 1.18 Loss, 0.01 Threshold\n",
            "425 episode , 204 step , 1.32 Loss, 0.01 Threshold\n",
            "426 episode , 223 step , 1.29 Loss, 0.01 Threshold\n",
            "427 episode , 203 step , 1.21 Loss, 0.01 Threshold\n",
            "428 episode , 226 step , 1.18 Loss, 0.01 Threshold\n",
            "429 episode , 255 step , 1.33 Loss, 0.01 Threshold\n",
            "430 episode , 213 step , 1.26 Loss, 0.01 Threshold\n",
            "431 episode , 228 step , 1.43 Loss, 0.01 Threshold\n",
            "432 episode , 307 step , 1.29 Loss, 0.01 Threshold\n",
            "433 episode , 206 step , 1.21 Loss, 0.01 Threshold\n",
            "434 episode , 210 step , 1.25 Loss, 0.01 Threshold\n",
            "435 episode , 235 step , 1.09 Loss, 0.01 Threshold\n",
            "436 episode , 150 step , 1.11 Loss, 0.01 Threshold\n",
            "437 episode , 327 step , 1.14 Loss, 0.01 Threshold\n",
            "438 episode , 226 step , 1.16 Loss, 0.01 Threshold\n",
            "439 episode , 220 step , 1.19 Loss, 0.01 Threshold\n",
            "440 episode , 221 step , 1.15 Loss, 0.01 Threshold\n",
            "441 episode , 191 step , 1.46 Loss, 0.01 Threshold\n",
            "442 episode , 204 step , 1.28 Loss, 0.01 Threshold\n",
            "443 episode , 200 step , 1.17 Loss, 0.01 Threshold\n",
            "444 episode , 194 step , 1.25 Loss, 0.01 Threshold\n",
            "445 episode , 213 step , 1.19 Loss, 0.01 Threshold\n",
            "446 episode , 164 step , 1.20 Loss, 0.01 Threshold\n",
            "447 episode , 196 step , 1.12 Loss, 0.01 Threshold\n",
            "448 episode , 164 step , 1.33 Loss, 0.01 Threshold\n",
            "449 episode , 208 step , 1.21 Loss, 0.01 Threshold\n",
            "450 episode , 163 step , 1.40 Loss, 0.01 Threshold\n",
            "451 episode , 10000 step , 1.09 Loss, 0.01 Threshold\n",
            "452 episode , 607 step , 1.03 Loss, 0.01 Threshold\n",
            "453 episode , 10000 step , 0.89 Loss, 0.01 Threshold\n",
            "454 episode , 1348 step , 0.83 Loss, 0.01 Threshold\n",
            "455 episode , 5400 step , 0.80 Loss, 0.01 Threshold\n",
            "456 episode , 1322 step , 0.73 Loss, 0.01 Threshold\n",
            "457 episode , 2132 step , 0.78 Loss, 0.01 Threshold\n",
            "458 episode , 367 step , 0.74 Loss, 0.01 Threshold\n",
            "459 episode , 10000 step , 0.69 Loss, 0.01 Threshold\n",
            "460 episode , 2462 step , 0.66 Loss, 0.01 Threshold\n",
            "461 episode , 89 step , 0.63 Loss, 0.01 Threshold\n",
            "462 episode , 10000 step , 0.61 Loss, 0.01 Threshold\n",
            "463 episode , 10000 step , 0.52 Loss, 0.01 Threshold\n",
            "464 episode , 10000 step , 0.40 Loss, 0.01 Threshold\n",
            "465 episode , 10000 step , 0.23 Loss, 0.01 Threshold\n",
            "466 episode , 10000 step , 0.11 Loss, 0.01 Threshold\n",
            "467 episode , 10000 step , 0.05 Loss, 0.01 Threshold\n",
            "468 episode , 10000 step , 0.03 Loss, 0.01 Threshold\n",
            "469 episode , 10000 step , 0.03 Loss, 0.01 Threshold\n",
            "470 episode , 10000 step , 0.03 Loss, 0.01 Threshold\n",
            "471 episode , 58 step , 0.07 Loss, 0.01 Threshold\n",
            "472 episode , 13 step , 0.01 Loss, 0.01 Threshold\n",
            "473 episode , 10000 step , 0.03 Loss, 0.01 Threshold\n",
            "474 episode , 5307 step , 0.02 Loss, 0.01 Threshold\n",
            "475 episode , 1817 step , 0.01 Loss, 0.01 Threshold\n",
            "476 episode , 182 step , 0.03 Loss, 0.01 Threshold\n",
            "477 episode , 9309 step , 0.02 Loss, 0.01 Threshold\n",
            "478 episode , 842 step , 0.01 Loss, 0.01 Threshold\n",
            "479 episode , 256 step , 0.04 Loss, 0.01 Threshold\n",
            "480 episode , 577 step , 0.03 Loss, 0.01 Threshold\n",
            "481 episode , 14 step , 0.16 Loss, 0.01 Threshold\n",
            "482 episode , 31 step , 0.02 Loss, 0.01 Threshold\n",
            "483 episode , 11 step , 0.01 Loss, 0.01 Threshold\n",
            "484 episode , 9 step , 0.00 Loss, 0.01 Threshold\n",
            "485 episode , 11 step , 0.00 Loss, 0.01 Threshold\n",
            "486 episode , 9 step , 0.00 Loss, 0.01 Threshold\n",
            "487 episode , 10 step , 0.01 Loss, 0.01 Threshold\n",
            "488 episode , 12 step , 0.00 Loss, 0.01 Threshold\n",
            "489 episode , 10 step , 0.00 Loss, 0.01 Threshold\n",
            "490 episode , 9 step , 0.02 Loss, 0.01 Threshold\n",
            "491 episode , 11 step , 0.16 Loss, 0.01 Threshold\n",
            "492 episode , 10 step , 0.03 Loss, 0.01 Threshold\n",
            "493 episode , 9 step , 0.02 Loss, 0.01 Threshold\n",
            "494 episode , 11 step , 0.02 Loss, 0.01 Threshold\n",
            "495 episode , 9 step , 0.02 Loss, 0.01 Threshold\n",
            "496 episode , 10 step , 0.00 Loss, 0.01 Threshold\n",
            "497 episode , 9 step , 0.13 Loss, 0.01 Threshold\n",
            "498 episode , 9 step , 0.01 Loss, 0.01 Threshold\n",
            "499 episode , 9 step , 0.01 Loss, 0.01 Threshold\n",
            "500 episode , 10 step , 0.01 Loss, 0.01 Threshold\n",
            "501 episode , 10 step , 0.16 Loss, 0.01 Threshold\n",
            "502 episode , 10 step , 0.02 Loss, 0.01 Threshold\n",
            "503 episode , 10 step , 0.02 Loss, 0.01 Threshold\n",
            "504 episode , 11 step , 0.01 Loss, 0.01 Threshold\n",
            "505 episode , 11 step , 0.01 Loss, 0.01 Threshold\n",
            "506 episode , 11 step , 0.01 Loss, 0.01 Threshold\n",
            "507 episode , 15 step , 0.09 Loss, 0.01 Threshold\n",
            "508 episode , 13 step , 0.18 Loss, 0.01 Threshold\n",
            "509 episode , 10 step , 0.05 Loss, 0.01 Threshold\n",
            "510 episode , 11 step , 0.02 Loss, 0.01 Threshold\n",
            "511 episode , 11 step , 0.16 Loss, 0.01 Threshold\n",
            "512 episode , 10 step , 0.17 Loss, 0.01 Threshold\n",
            "513 episode , 10 step , 0.13 Loss, 0.01 Threshold\n",
            "514 episode , 12 step , 0.14 Loss, 0.01 Threshold\n",
            "515 episode , 12 step , 0.04 Loss, 0.01 Threshold\n",
            "516 episode , 12 step , 0.03 Loss, 0.01 Threshold\n",
            "517 episode , 10 step , 0.19 Loss, 0.01 Threshold\n",
            "518 episode , 11 step , 0.13 Loss, 0.01 Threshold\n",
            "519 episode , 9 step , 0.23 Loss, 0.01 Threshold\n",
            "520 episode , 9 step , 0.03 Loss, 0.01 Threshold\n",
            "521 episode , 10 step , 0.47 Loss, 0.01 Threshold\n",
            "522 episode , 13 step , 0.20 Loss, 0.01 Threshold\n",
            "523 episode , 13 step , 0.32 Loss, 0.01 Threshold\n",
            "524 episode , 12 step , 0.11 Loss, 0.01 Threshold\n",
            "525 episode , 12 step , 0.31 Loss, 0.01 Threshold\n",
            "526 episode , 12 step , 0.01 Loss, 0.01 Threshold\n",
            "527 episode , 13 step , 0.01 Loss, 0.01 Threshold\n",
            "528 episode , 13 step , 0.16 Loss, 0.01 Threshold\n",
            "529 episode , 13 step , 0.01 Loss, 0.01 Threshold\n",
            "530 episode , 16 step , 0.10 Loss, 0.01 Threshold\n",
            "531 episode , 13 step , 0.30 Loss, 0.01 Threshold\n",
            "532 episode , 11 step , 0.31 Loss, 0.01 Threshold\n",
            "533 episode , 10 step , 0.15 Loss, 0.01 Threshold\n",
            "534 episode , 11 step , 0.42 Loss, 0.01 Threshold\n",
            "535 episode , 12 step , 0.04 Loss, 0.01 Threshold\n",
            "536 episode , 11 step , 0.19 Loss, 0.01 Threshold\n",
            "537 episode , 11 step , 0.12 Loss, 0.01 Threshold\n",
            "538 episode , 13 step , 0.09 Loss, 0.01 Threshold\n",
            "539 episode , 11 step , 0.18 Loss, 0.01 Threshold\n",
            "540 episode , 10 step , 0.12 Loss, 0.01 Threshold\n",
            "541 episode , 11 step , 0.46 Loss, 0.01 Threshold\n",
            "542 episode , 12 step , 0.23 Loss, 0.01 Threshold\n",
            "543 episode , 15 step , 0.12 Loss, 0.01 Threshold\n",
            "544 episode , 13 step , 0.25 Loss, 0.01 Threshold\n",
            "545 episode , 8 step , 0.17 Loss, 0.01 Threshold\n",
            "546 episode , 10 step , 0.15 Loss, 0.01 Threshold\n",
            "547 episode , 11 step , 0.04 Loss, 0.01 Threshold\n",
            "548 episode , 13 step , 0.06 Loss, 0.01 Threshold\n",
            "549 episode , 10 step , 0.32 Loss, 0.01 Threshold\n",
            "550 episode , 10 step , 0.09 Loss, 0.01 Threshold\n",
            "551 episode , 12 step , 0.35 Loss, 0.01 Threshold\n",
            "552 episode , 10 step , 0.36 Loss, 0.01 Threshold\n",
            "553 episode , 12 step , 0.28 Loss, 0.01 Threshold\n",
            "554 episode , 11 step , 0.03 Loss, 0.01 Threshold\n",
            "555 episode , 11 step , 0.10 Loss, 0.01 Threshold\n",
            "556 episode , 13 step , 0.14 Loss, 0.01 Threshold\n",
            "557 episode , 12 step , 0.19 Loss, 0.01 Threshold\n",
            "558 episode , 13 step , 0.12 Loss, 0.01 Threshold\n",
            "559 episode , 15 step , 0.14 Loss, 0.01 Threshold\n",
            "560 episode , 13 step , 0.12 Loss, 0.01 Threshold\n",
            "561 episode , 12 step , 0.48 Loss, 0.01 Threshold\n",
            "562 episode , 11 step , 0.31 Loss, 0.01 Threshold\n",
            "563 episode , 10 step , 0.05 Loss, 0.01 Threshold\n",
            "564 episode , 13 step , 0.27 Loss, 0.01 Threshold\n",
            "565 episode , 14 step , 0.33 Loss, 0.01 Threshold\n",
            "566 episode , 18 step , 0.15 Loss, 0.01 Threshold\n",
            "567 episode , 16 step , 0.18 Loss, 0.01 Threshold\n",
            "568 episode , 12 step , 0.23 Loss, 0.01 Threshold\n",
            "569 episode , 15 step , 0.11 Loss, 0.01 Threshold\n",
            "570 episode , 19 step , 0.07 Loss, 0.01 Threshold\n",
            "571 episode , 16 step , 0.52 Loss, 0.01 Threshold\n",
            "572 episode , 13 step , 0.16 Loss, 0.01 Threshold\n",
            "573 episode , 15 step , 0.04 Loss, 0.01 Threshold\n",
            "574 episode , 17 step , 0.20 Loss, 0.01 Threshold\n",
            "575 episode , 12 step , 0.23 Loss, 0.01 Threshold\n",
            "576 episode , 18 step , 0.23 Loss, 0.01 Threshold\n",
            "577 episode , 13 step , 0.13 Loss, 0.01 Threshold\n",
            "578 episode , 16 step , 0.06 Loss, 0.01 Threshold\n",
            "579 episode , 16 step , 0.04 Loss, 0.01 Threshold\n",
            "580 episode , 15 step , 0.30 Loss, 0.01 Threshold\n",
            "581 episode , 13 step , 0.79 Loss, 0.01 Threshold\n",
            "582 episode , 16 step , 0.34 Loss, 0.01 Threshold\n",
            "583 episode , 92 step , 0.19 Loss, 0.01 Threshold\n",
            "584 episode , 18 step , 0.33 Loss, 0.01 Threshold\n",
            "585 episode , 104 step , 0.18 Loss, 0.01 Threshold\n",
            "586 episode , 90 step , 0.24 Loss, 0.01 Threshold\n",
            "587 episode , 19 step , 0.13 Loss, 0.01 Threshold\n",
            "588 episode , 60 step , 0.28 Loss, 0.01 Threshold\n",
            "589 episode , 55 step , 0.21 Loss, 0.01 Threshold\n",
            "590 episode , 105 step , 0.24 Loss, 0.01 Threshold\n",
            "591 episode , 100 step , 0.30 Loss, 0.01 Threshold\n",
            "592 episode , 25 step , 0.40 Loss, 0.01 Threshold\n",
            "593 episode , 17 step , 0.40 Loss, 0.01 Threshold\n",
            "594 episode , 18 step , 0.12 Loss, 0.01 Threshold\n",
            "595 episode , 35 step , 0.16 Loss, 0.01 Threshold\n",
            "596 episode , 31 step , 0.18 Loss, 0.01 Threshold\n",
            "597 episode , 20 step , 0.14 Loss, 0.01 Threshold\n",
            "598 episode , 33 step , 0.23 Loss, 0.01 Threshold\n",
            "599 episode , 17 step , 0.27 Loss, 0.01 Threshold\n",
            "600 episode , 16 step , 0.25 Loss, 0.01 Threshold\n",
            "601 episode , 18 step , 0.80 Loss, 0.01 Threshold\n",
            "602 episode , 18 step , 0.40 Loss, 0.01 Threshold\n",
            "603 episode , 18 step , 0.38 Loss, 0.01 Threshold\n",
            "604 episode , 16 step , 0.23 Loss, 0.01 Threshold\n",
            "605 episode , 14 step , 0.05 Loss, 0.01 Threshold\n",
            "606 episode , 15 step , 0.09 Loss, 0.01 Threshold\n",
            "607 episode , 13 step , 0.27 Loss, 0.01 Threshold\n",
            "608 episode , 15 step , 0.22 Loss, 0.01 Threshold\n",
            "609 episode , 18 step , 0.14 Loss, 0.01 Threshold\n",
            "610 episode , 17 step , 0.13 Loss, 0.01 Threshold\n",
            "611 episode , 33 step , 0.49 Loss, 0.01 Threshold\n",
            "612 episode , 16 step , 0.35 Loss, 0.01 Threshold\n",
            "613 episode , 20 step , 0.09 Loss, 0.01 Threshold\n",
            "614 episode , 20 step , 0.20 Loss, 0.01 Threshold\n",
            "615 episode , 18 step , 0.14 Loss, 0.01 Threshold\n",
            "616 episode , 18 step , 0.33 Loss, 0.01 Threshold\n",
            "617 episode , 38 step , 0.22 Loss, 0.01 Threshold\n",
            "618 episode , 117 step , 0.25 Loss, 0.01 Threshold\n",
            "619 episode , 13 step , 0.23 Loss, 0.01 Threshold\n",
            "620 episode , 41 step , 0.31 Loss, 0.01 Threshold\n",
            "621 episode , 104 step , 0.42 Loss, 0.01 Threshold\n",
            "622 episode , 50 step , 0.32 Loss, 0.01 Threshold\n",
            "623 episode , 93 step , 0.27 Loss, 0.01 Threshold\n",
            "624 episode , 14 step , 0.19 Loss, 0.01 Threshold\n",
            "625 episode , 57 step , 0.25 Loss, 0.01 Threshold\n",
            "626 episode , 52 step , 0.34 Loss, 0.01 Threshold\n",
            "627 episode , 107 step , 0.29 Loss, 0.01 Threshold\n",
            "628 episode , 145 step , 0.35 Loss, 0.01 Threshold\n",
            "629 episode , 128 step , 0.31 Loss, 0.01 Threshold\n",
            "630 episode , 67 step , 0.31 Loss, 0.01 Threshold\n",
            "631 episode , 116 step , 0.37 Loss, 0.01 Threshold\n",
            "632 episode , 118 step , 0.32 Loss, 0.01 Threshold\n",
            "633 episode , 120 step , 0.33 Loss, 0.01 Threshold\n",
            "634 episode , 77 step , 0.23 Loss, 0.01 Threshold\n",
            "635 episode , 122 step , 0.33 Loss, 0.01 Threshold\n",
            "636 episode , 117 step , 0.33 Loss, 0.01 Threshold\n",
            "637 episode , 100 step , 0.35 Loss, 0.01 Threshold\n",
            "638 episode , 114 step , 0.30 Loss, 0.01 Threshold\n",
            "639 episode , 115 step , 0.28 Loss, 0.01 Threshold\n",
            "640 episode , 128 step , 0.32 Loss, 0.01 Threshold\n",
            "641 episode , 119 step , 0.36 Loss, 0.01 Threshold\n",
            "642 episode , 118 step , 0.35 Loss, 0.01 Threshold\n",
            "643 episode , 114 step , 0.33 Loss, 0.01 Threshold\n",
            "644 episode , 118 step , 0.27 Loss, 0.01 Threshold\n",
            "645 episode , 122 step , 0.38 Loss, 0.01 Threshold\n",
            "646 episode , 121 step , 0.36 Loss, 0.01 Threshold\n",
            "647 episode , 118 step , 0.33 Loss, 0.01 Threshold\n",
            "648 episode , 115 step , 0.37 Loss, 0.01 Threshold\n",
            "649 episode , 121 step , 0.34 Loss, 0.01 Threshold\n",
            "650 episode , 122 step , 0.36 Loss, 0.01 Threshold\n",
            "651 episode , 115 step , 0.49 Loss, 0.01 Threshold\n",
            "652 episode , 123 step , 0.35 Loss, 0.01 Threshold\n",
            "653 episode , 126 step , 0.35 Loss, 0.01 Threshold\n",
            "654 episode , 119 step , 0.34 Loss, 0.01 Threshold\n",
            "655 episode , 123 step , 0.30 Loss, 0.01 Threshold\n",
            "656 episode , 116 step , 0.42 Loss, 0.01 Threshold\n",
            "657 episode , 117 step , 0.40 Loss, 0.01 Threshold\n",
            "658 episode , 114 step , 0.28 Loss, 0.01 Threshold\n",
            "659 episode , 113 step , 0.29 Loss, 0.01 Threshold\n",
            "660 episode , 124 step , 0.33 Loss, 0.01 Threshold\n",
            "661 episode , 116 step , 0.52 Loss, 0.01 Threshold\n",
            "662 episode , 125 step , 0.35 Loss, 0.01 Threshold\n",
            "663 episode , 126 step , 0.39 Loss, 0.01 Threshold\n",
            "664 episode , 123 step , 0.41 Loss, 0.01 Threshold\n",
            "665 episode , 130 step , 0.37 Loss, 0.01 Threshold\n",
            "666 episode , 128 step , 0.37 Loss, 0.01 Threshold\n",
            "667 episode , 124 step , 0.42 Loss, 0.01 Threshold\n",
            "668 episode , 128 step , 0.38 Loss, 0.01 Threshold\n",
            "669 episode , 123 step , 0.46 Loss, 0.01 Threshold\n",
            "670 episode , 123 step , 0.40 Loss, 0.01 Threshold\n",
            "671 episode , 126 step , 0.51 Loss, 0.01 Threshold\n",
            "672 episode , 154 step , 0.39 Loss, 0.01 Threshold\n",
            "673 episode , 156 step , 0.32 Loss, 0.01 Threshold\n",
            "674 episode , 144 step , 0.36 Loss, 0.01 Threshold\n",
            "675 episode , 150 step , 0.34 Loss, 0.01 Threshold\n",
            "676 episode , 142 step , 0.40 Loss, 0.01 Threshold\n",
            "677 episode , 152 step , 0.42 Loss, 0.01 Threshold\n",
            "678 episode , 145 step , 0.37 Loss, 0.01 Threshold\n",
            "679 episode , 147 step , 0.41 Loss, 0.01 Threshold\n",
            "680 episode , 141 step , 0.33 Loss, 0.01 Threshold\n",
            "681 episode , 139 step , 0.47 Loss, 0.01 Threshold\n",
            "682 episode , 149 step , 0.34 Loss, 0.01 Threshold\n",
            "683 episode , 155 step , 0.40 Loss, 0.01 Threshold\n",
            "684 episode , 140 step , 0.31 Loss, 0.01 Threshold\n",
            "685 episode , 143 step , 0.37 Loss, 0.01 Threshold\n",
            "686 episode , 141 step , 0.37 Loss, 0.01 Threshold\n",
            "687 episode , 143 step , 0.34 Loss, 0.01 Threshold\n",
            "688 episode , 130 step , 0.34 Loss, 0.01 Threshold\n",
            "689 episode , 135 step , 0.32 Loss, 0.01 Threshold\n",
            "690 episode , 134 step , 0.43 Loss, 0.01 Threshold\n",
            "691 episode , 154 step , 0.43 Loss, 0.01 Threshold\n",
            "692 episode , 151 step , 0.34 Loss, 0.01 Threshold\n",
            "693 episode , 155 step , 0.40 Loss, 0.01 Threshold\n",
            "694 episode , 158 step , 0.39 Loss, 0.01 Threshold\n",
            "695 episode , 336 step , 0.31 Loss, 0.01 Threshold\n",
            "696 episode , 173 step , 0.35 Loss, 0.01 Threshold\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}