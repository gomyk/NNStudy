{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6hPBuVuDbRWfxQuFFPiF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDQN%5D%5BMW%5D%20CartPole-v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "e9d2e47b-ee03-4c43-8667-6ed24d8bb1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwNNaGeeyMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        \n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "57c3f4af-0eba-4a6f-cfe1-af27c2c6bf99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.01\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 5000\n",
        "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
        "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
        "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    print(eps_threshold)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
        "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
        "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
        "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber 손실 계산\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "922ffd59-a8c5-4804-8fb1-2c87ffcc0605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "source": [
        "num_episodes = EPISODE_SIZE\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # 새로운 상태 관찰\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        state = next_state\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            output.clear()\n",
        "            episode_durations.append(t + 1)\n",
        "            #plot_durations()\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10599354126244251\n",
            "tensor(0.1872, grad_fn=<SmoothL1LossBackward>)\n",
            "0.10571427231032068\n",
            "tensor(0.1115, grad_fn=<SmoothL1LossBackward>)\n",
            "0.1054363962179084\n",
            "tensor(0.1177, grad_fn=<SmoothL1LossBackward>)\n",
            "0.10515990603828887\n",
            "tensor(0.0963, grad_fn=<SmoothL1LossBackward>)\n",
            "0.10488479485919319\n",
            "tensor(0.1058, grad_fn=<SmoothL1LossBackward>)\n",
            "0.10461105580282762\n",
            "tensor(0.1382, grad_fn=<SmoothL1LossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-97447ca59bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# 최적화 한단계 수행(목표 네트워크에서)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b6d42f2f54c6>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 모델 최적화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}