{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPH39wirxamKBRav7Z0KB/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDQN%5D%5BMW%5D%20CartPole-v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "215bbdc6-8689-4b94-95af-193f08199b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwNNaGeeyMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        \n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "7c544b8a-9c68-44f9-de01-3c30c2bab07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADECAYAAACGNXroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAATNUlEQVR4nO3df5BdZX3H8fcnu0kIIUBClkwgkUWM\nUOhA0JQfo7UIQVNbhZk6Cq0WHCy1pSOxIALOtNo6U5ki6IwdKopKxeIPBMFUhRBirb+ADT8EEjAB\n+RHcJJuYECAYs+TbP86z4dybvbuX3bv33mf385o5s+c5z9nzfM+P+73Pfe6Po4jAzMzyM6nVAZiZ\n2cg4gZuZZcoJ3MwsU07gZmaZcgI3M8uUE7iZWaacwK3pJJ0r6SetjqOdSOqWFJI6Wx2L5cMJfJyR\n9KSklyS9UJo+3+q4Wk3SKZLWj+H2PyHphrHavtlg/Gw/Pr0zIu5sdRC5kdQZEf2tjmMsjOd9m8jc\nA59AJF0j6Tul8hWSVqgwU9IySX2Stqb5eaV1fyTpU5J+lnr135N0kKSvS9ou6V5J3aX1Q9KHJT0h\nabOkf5c06PUm6ShJyyX9VtJjkt4zxD4cIOk6Sb2Snk0xdQyzf9OBHwCHlF6VHJJ6zTdJukHSduBc\nSSdI+rmkbamNz0uaUtrmMaVYN0q6XNIS4HLgvWnbD9YRa4ekK9OxeQL4s2HO3cfSNp5Px+i00nYu\nl/R4qlslaX7pHFwgaS2wdrhjLWlqiunptG//KWlaqjtF0npJF0nalPbpA0PFbE0QEZ7G0QQ8CSyu\nUbcv8CvgXOCPgc3AvFR3EPAXaZ0ZwLeB75b+90fAOuAI4ABgddrWYopXcv8FfKW0fgArgVnAa9K6\nH0x15wI/SfPTgWeAD6TtHJ/iOrrGPtwCfCH938HAPcDf1rF/pwDrq7b1CWAXcCZFZ2Ya8EbgpBRL\nN7AGWJrWnwH0AhcB+6TyiaVt3fAqYv0Q8CgwPx2jlemYdQ6yz0emY3RIKncDR6T5jwIPpXUEHAcc\nVDoHy9P2pw13rIGrgdvS+jOA7wH/Vjp+/cC/AJOBdwA7gJmtvuYn8tTyADw1+IQWCfwFYFtp+ptS\n/YnAb4GngLOH2M5CYGup/CPg46XyZ4AflMrvBB4olQNYUir/PbAizZ/LKwn8vcD/VbX9BeCfB4lp\nDrATmFZadjawcrj9o3YC//Ewx3MpcEuprftrrPcJSgl8uFiBu4APlereRu0E/jpgE8WT5eSquseA\nM2rEFMCppXLNY02R/F8kPTGkupOBX5eO30vl+FJMJ7X6mp/Ik8fAx6czo8YYeETcnV6yHwx8a2C5\npH0pemBLgJlp8QxJHRHxcipvLG3qpUHK+1U190xp/ingkEFCOgw4UdK20rJO4Gs11p0M9EoaWDap\n3E6t/RtCOUYkvR64ClhE0aPvBFal6vnA43Vss55YD2Hv4zOoiFgnaSnFk8Qxkm4H/jEiflNHTOU2\nhjrWXRT7u6oUr4CO0rpbonIcfQd7n3NrIo+BTzCSLgCmAr8BLilVXUTxMvzEiNgfeMvAv4yiufml\n+dekNqs9A/xvRBxYmvaLiL+rse5OYHZp3f0j4piBFYbYv1o/u1m9/BqKoY0F6ThczivH4BngtXVu\nZ7hYe9n7+NQUEf8dEW+mSMIBXFFq54ih/rUqplrHejPFk/AxpboDIsIJuo05gU8gqXf5KeB9wPuB\nSyQtTNUzKB7A2yTNonhZPVofTW+OzgcuBL45yDrLgNdLer+kyWn6I0l/UL1iRPQCdwCfkbS/pEmS\njpD0J3Xs30bgIEkHDBPzDGA78IKko4DyE8kyYK6kpekNvxmSTixtv3vgjdrhYqV4dfBhSfMkzQQu\nrRWQpCMlnSppKvA7ivO0O1V/CfhXSQtUOFbSQTU2VfNYR8Ru4IvA1ZIOTu0eKuntwxwvayEn8PHp\ne6r8HPgtKr4gcgNwRUQ8GBFrKXqXX0uJ4bMUb3RtBn4B/LABcdxKMfzwAPA/wHXVK0TE8xTjv2dR\n9Jo3UPQup9bY5l8DUyjeRN0K3ESRVIfcv4h4FLgReCJ9wmSw4RyAi4G/BJ6nSGh7nnRSrKdTjPdv\noPhkx1tT9bfT3y2S7hsq1lT3ReB24EHgPuDmGvGQjsWnKc7NBorhoctS3VUUTwZ3UDzxXEdxHvdS\nx7H+GMUb1b9In8q5k+JVmbUpRfiGDtZ4koJiGGJdq2MxG6/cAzczy5QTuJlZpjyEYmaWqVH1wCUt\nSV/HXSep5rvoZmbWeCPugaffdPgVxbvy64F7Kb75trpx4ZmZWS2j+SbmCcC6iHgCQNI3gDMoPjI1\nqNmzZ0d3d/comjQzm3hWrVq1OSK6qpePJoEfSuXXdNdT/A5FTd3d3fT09IyiSTOziUfSoD+1MOaf\nQpF0vqQeST19fX1j3ZyZ2YQxmgT+LJW/5TAvLasQEddGxKKIWNTVtdcrADMzG6HRJPB7gQWSDlfx\ng/dnUfyWsJmZNcGIx8Ajol/SP1D8nkMH8OWIeKRhkZmZ2ZBG9XvgEfF94PsNisXMzF4F39DBJqzd\nL+/aM1+6iUFRnuSHhrU//xaKmVmmnMDNzDLlBG5mlikP9Nm4tWPL0xXlZ35WeY/j/h3b98wf9pb3\nVdTtN/f1YxeYWYO4B25mlikncDOzTDmBm5llymPgNm71v/RCRfm5px6qKGvSK/2XwHemsvy4B25m\nlikncDOzTDmBm5llymPgNm5NmX5gRXnytBkV5f6dL+6Z//3zWyr/ee6YhWXWMO6Bm5llygnczCxT\nHkKxcatz3wMqypMm71NRjpde+Sr9zu2bmxKTWSO5B25mlikncDOzTDmBm5llymPgNn5F9dfja39d\nXnJfxvLjq9bMLFNO4GZmmXICNzPLlBO4mVmmnMDNzDLlBG5mlikncDOzTDmBm5llatgELunLkjZJ\neri0bJak5ZLWpr8zxzZMMzOrVk8P/KvAkqpllwIrImIBsCKVzcysiYZN4BHxY+C3VYvPAK5P89cD\nZzY4LjMzG8ZIx8DnRERvmt8AzGlQPGZmVqdRv4kZEcEQvxIk6XxJPZJ6+vr6RtucmZklI03gGyXN\nBUh/N9VaMSKujYhFEbGoq6trhM2ZmVm1kSbw24Bz0vw5wK2NCcfMzOpVz8cIbwR+Dhwpab2k84BP\nA6dLWgssTmUzM2uiYW/oEBFn16g6rcGxmJnZq+BvYpqZZcq3VLMJpPYt1ZCaF4ZZg7gHbmaWKSdw\nM7NMeQjFxq2OKdMqylNnHFxR3rl98575HZvXNyUms0ZyD9zMLFNO4GZmmXICNzPLlMfAbdzSpI6K\n8qQp+9Rc9+WdL451OGYN5x64mVmmnMDNzDLlBG5mlimPgdvEEf4qvY0v7oGbmWXKCdzMLFNO4GZm\nmXICNzPLlBO4mVmmnMDNzDLlBG5mlikncDOzTDmBm5llygnczCxTTuBmZplyAjczy5QTuJlZppzA\nzcwy5QRuZpapYRO4pPmSVkpaLekRSRem5bMkLZe0Nv2dOfbhmpnZgHp64P3ARRFxNHAScIGko4FL\ngRURsQBYkcpmZtYkwybwiOiNiPvS/PPAGuBQ4Azg+rTa9cCZYxWkWSNoUkfFVBaxu2IionIya0Ov\nagxcUjdwPHA3MCcielPVBmBOQyMzM7Mh1Z3AJe0HfAdYGhHby3UREcCg3RRJ50vqkdTT19c3qmDN\nzOwVdSVwSZMpkvfXI+LmtHijpLmpfi6wabD/jYhrI2JRRCzq6upqRMxmZkYdd6WXJOA6YE1EXFWq\nug04B/h0+nvrmERo1iDTuw6rKG99omfP/M5tGyrq+n+/o6LcOXX62AVmNkLDJnDgTcD7gYckPZCW\nXU6RuL8l6TzgKeA9YxOimZkNZtgEHhE/AVSj+rTGhmNmZvXyNzHNzDJVzxCK2bhQ/dnvsojd1QvG\nOBqz0XMP3MwsU07gZmaZcgI3M8uUE7iZWaacwM3MMuUEbmaWKSdwM7NMOYGbmWXKCdzMLFNO4GZm\nmXICNzPLlBO4mVmmnMDNzDLlBG5mlikncDOzTDmBm5llygnczCxTTuBmZpnyLdVswtjrtmkVqu7b\nrVr38TZrH+6Bm5llygnczCxTHkKxCWPq/rMryuW71O/e9buKuv4dz1WUO6dOH7vAzEbIPXAzs0w5\ngZuZZcoJ3MwsUx4Dtwlj6ozaY+Av73qpom7XS5Vj4PvMPGTsAjMbIffAzcwyNWwCl7SPpHskPSjp\nEUmfTMsPl3S3pHWSvilpytiHa2ZmA+rpge8ETo2I44CFwBJJJwFXAFdHxOuArcB5YxemmZlVGzaB\nR+GFVJycpgBOBW5Ky68HzhyTCM0apKNjUsUkojRRMXV2Tq6YzNpRXWPgkjokPQBsApYDjwPbIqI/\nrbIeOLTG/54vqUdST19fXyNiNjMz6kzgEfFyRCwE5gEnAEfV20BEXBsRiyJiUVdX1wjDNDOzaq/q\nY4QRsU3SSuBk4EBJnakXPg94diwCtInt/vvvryhffPHFI97Wgjn7VJQ/eMpra677kaUXVpTXbvxd\njTWHd+WVV1aUjz/++BFvy6ysnk+hdEk6MM1PA04H1gArgXen1c4Bbh2rIM3MbG/19MDnAtdL6qBI\n+N+KiGWSVgPfkPQp4H7gujGM08zMqgybwCPil8Ber/ki4gmK8XAzM2sBf5Xe2tqWLVsqynfdddeI\nt/XsYd0V5aOOvWTPfNBRUXfnTz9QUX786XUjbrd6H8waxV+lNzPLlBO4mVmmnMDNzDLlMXBra52d\njbtEO6bMqCjv7pi1Z/73/ZV3oZ80uXLd0WjkPpiVuQduZpYpJ3Azs0w5gZuZZaqpg3O7du2it7e3\nmU1a5jZv3tywbT237cmK8s/v/Oie+dVPVrazsXd1w9qt3gc/BqxR3AM3M8uUE7iZWaaaOoTS39+P\nb+pgr8a2bdsatq1n+56vKN90x+0N2/ZQqvfBjwFrFPfAzcwy5QRuZpYpJ3Azs0w1dQx82rRpHHvs\nsc1s0jK3devWVocwagsWLKgo+zFgjeIeuJlZppzAzcwy5QRuZpYp/86ltbVdu3a1OoRRGw/7YO3J\nPXAzs0w5gZuZZcoJ3MwsUx4Dt7Y2e/bsivLixYtbFMnIVe+DWaO4B25mlikncDOzTHkIxdrawoUL\nK8rLly9vUSRm7cc9cDOzTDmBm5llygnczCxTiojmNSb1AU8Bs4HG3W68MRxTfRxT/doxLsdUn3aL\n6bCI6Kpe2NQEvqdRqSciFjW94SE4pvo4pvq1Y1yOqT7tGNNgPIRiZpYpJ3Azs0y1KoFf26J2h+KY\n6uOY6teOcTmm+rRjTHtpyRi4mZmNnodQzMwy1dQELmmJpMckrZN0aTPbrorjy5I2SXq4tGyWpOWS\n1qa/M5sc03xJKyWtlvSIpAtbHZekfSTdI+nBFNMn0/LDJd2dzuM3JU1pVkyl2Dok3S9pWTvEJOlJ\nSQ9JekBST1rW6mvqQEk3SXpU0hpJJ7dBTEemYzQwbZe0tA3i+ki6xh+WdGO69lt+nQ+naQlcUgfw\nH8CfAkcDZ0s6ulntV/kqsKRq2aXAiohYAKxI5WbqBy6KiKOBk4AL0vFpZVw7gVMj4jhgIbBE0knA\nFcDVEfE6YCtwXhNjGnAhsKZUboeY3hoRC0sfP2v1NfU54IcRcRRwHMXxamlMEfFYOkYLgTcCO4Bb\nWhmXpEOBDwOLIuIPgQ7gLNrjmhpaRDRlAk4Gbi+VLwMua1b7g8TTDTxcKj8GzE3zc4HHWhVbiuFW\n4PR2iQvYF7gPOJHiCw6dg53XJsUyj+JBfiqwDFAbxPQkMLtqWcvOHXAA8GvS+1ztENMgMb4N+Gmr\n4wIOBZ4BZlH8wN8y4O2tvqbqmZo5hDJwkAasT8vaxZyI6E3zG4A5rQpEUjdwPHA3LY4rDVU8AGwC\nlgOPA9sioj+t0orz+FngEmB3Kh/UBjEFcIekVZLOT8taee4OB/qAr6Shpi9Jmt7imKqdBdyY5lsW\nV0Q8C1wJPA30As8Bq2j9NTUsv4k5iCieclvy8RxJ+wHfAZZGxPZWxxURL0fxcncecAJwVDPbrybp\nz4FNEbGqlXEM4s0R8QaKIcILJL2lXNmCc9cJvAG4JiKOB16kaliixdf5FOBdwLer65odVxpvP4Pi\nSe8QYDp7D7G2pWYm8GeB+aXyvLSsXWyUNBcg/d3U7AAkTaZI3l+PiJvbJS6AiNgGrKR4KXmgpIHf\nkm/2eXwT8C5JTwLfoBhG+VyLYxroxRERmyjGdE+gteduPbA+Iu5O5ZsoEnpbXE8UT3T3RcTGVG5l\nXIuBX0dEX0TsAm6muM5aek3Vo5kJ/F5gQXpndwrFy6fbmtj+cG4Dzknz51CMQTeNJAHXAWsi4qp2\niEtSl6QD0/w0ijH5NRSJ/N2tiCkiLouIeRHRTXEN3RURf9XKmCRNlzRjYJ5ibPdhWnjuImID8Iyk\nI9Oi04DVrYypytm8MnwCrY3raeAkSfumx+HAsWrZNVW3Zg64A+8AfkUxjvrxVg38U1w4vcAuip7K\neRTjqCuAtcCdwKwmx/RmipeNvwQeSNM7WhkXcCxwf4rpYeCf0vLXAvcA6yheAk9t0Xk8BVjW6phS\n2w+m6ZGBa7sNrqmFQE86f98FZrY6phTXdGALcEBpWauP1SeBR9N1/jVgartc50NN/iammVmm/Cam\nmVmmnMDNzDLlBG5mlikncDOzTDmBm5llygnczCxTTuBmZplyAjczy9T/A5Kr9D9of3NwAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "LR = 0.01\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 5000\n",
        "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
        "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
        "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
        "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
        "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
        "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber 손실 계산\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "7b1ef523-43cc-4677-d918-c6056adc498d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "num_episodes = EPISODE_SIZE\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # 새로운 상태 관찰\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        state = next_state\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            output.clear()\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step'%(i,t+1))\n",
        "            #plot_durations()\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12118174175336657\n",
            "0.12082672133527055\n",
            "0.12047347158889676\n",
            "0.12012198368298324\n",
            "0.11977224883031398\n",
            "0.11942425828749945\n",
            "0.11907800335475795\n",
            "0.11873347537569812\n",
            "0.11839066573710258\n",
            "0.11804956586871246\n",
            "0.11771016724301334\n",
            "0.11737246137502182\n",
            "0.1170364398220737\n",
            "0.11670209418361262\n",
            "0.1163694161009802\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}