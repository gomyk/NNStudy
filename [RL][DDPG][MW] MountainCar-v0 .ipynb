{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPd3USUY3/2q4GoI60NH3Ho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20MountainCar-v0%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "30ba7c88-491e-45bb-f0a8-f16501b9011f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.linear(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size + action_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, 1)\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "2d04497b-ab63-4346-8256-f7fdcb08b164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.01\n",
        "CRITIC_LR = 0.01\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 2000\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "n_obvs = 2\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    if sample < eps_threshold:\n",
        "            selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "    else:\n",
        "        selected_action = actor(\n",
        "             torch.FloatTensor(state).to(device)\n",
        "         ).detach().cpu().numpy()\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1 , -1\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + GAMMA * next_value * masks\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.mse_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()\n",
        "    print(critic_loss)      \n",
        "    # train actor\n",
        "    actor_loss = -critic(state, actor(state)).mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "b6a09090-27d4-427b-d770-a717cbd237c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(np.argmax(action))\n",
        "        reward  = obv[0]\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        if done :\n",
        "          if t != 199:\n",
        "            reward += 100\n",
        "            print('success')\n",
        "\n",
        "        if not done:\n",
        "          next_state = next_obv\n",
        "        else:\n",
        "          next_state = None\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_state, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward))\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        actor_target.load_state_dict(actor.state_dict())\n",
        "        critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 200 step , -1.00 Actor Loss, -1.00 Critic Loss,  0.82 Threshold , -0.34 Top reward\n",
            "1 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.75 Threshold , -0.43 Top reward\n",
            "2 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.68 Threshold , -0.43 Top reward\n",
            "3 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.62 Threshold , -0.40 Top reward\n",
            "4 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.57 Threshold , -0.47 Top reward\n",
            "5 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.52 Threshold , -0.49 Top reward\n",
            "6 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.47 Threshold , -0.49 Top reward\n",
            "7 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.43 Threshold , -0.54 Top reward\n",
            "8 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.40 Threshold , -0.50 Top reward\n",
            "9 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.36 Threshold , -0.47 Top reward\n",
            "10 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.33 Threshold , -0.51 Top reward\n",
            "11 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.31 Threshold , -0.52 Top reward\n",
            "12 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.28 Threshold , -0.54 Top reward\n",
            "13 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.26 Threshold , -0.47 Top reward\n",
            "14 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.24 Threshold , -0.43 Top reward\n",
            "15 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.22 Threshold , -0.50 Top reward\n",
            "16 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.21 Threshold , -0.51 Top reward\n",
            "17 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.19 Threshold , -0.51 Top reward\n",
            "18 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.18 Threshold , -0.35 Top reward\n",
            "19 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.17 Threshold , -0.49 Top reward\n",
            "20 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.15 Threshold , -0.54 Top reward\n",
            "21 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.14 Threshold , -0.46 Top reward\n",
            "22 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.14 Threshold , -0.54 Top reward\n",
            "23 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.13 Threshold , -0.33 Top reward\n",
            "24 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.12 Threshold , -0.49 Top reward\n",
            "25 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.11 Threshold , -0.58 Top reward\n",
            "26 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.11 Threshold , -0.55 Top reward\n",
            "27 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.10 Threshold , -0.55 Top reward\n",
            "28 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.10 Threshold , -0.40 Top reward\n",
            "29 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.09 Threshold , -0.44 Top reward\n",
            "30 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.09 Threshold , -0.45 Top reward\n",
            "31 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.08 Threshold , -0.59 Top reward\n",
            "32 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.08 Threshold , -0.58 Top reward\n",
            "33 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.08 Threshold , -0.45 Top reward\n",
            "34 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.08 Threshold , -0.46 Top reward\n",
            "35 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.07 Threshold , -0.55 Top reward\n",
            "36 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.07 Threshold , -0.56 Top reward\n",
            "37 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.07 Threshold , -0.37 Top reward\n",
            "38 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.07 Threshold , -0.40 Top reward\n",
            "39 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.07 Threshold , -0.41 Top reward\n",
            "40 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.06 Threshold , -0.43 Top reward\n",
            "41 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.06 Threshold , -0.43 Top reward\n",
            "42 episode , 200 step , nan Actor Loss, nan Critic Loss,  0.06 Threshold , -0.38 Top reward\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-dff4d65a2686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 행동 선택과 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnext_obv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mreward\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mobv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0b11027c2c74>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         selected_action = actor(\n\u001b[0;32m---> 52\u001b[0;31m              \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m          ).detach().cpu().numpy()\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselected_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c6bc9dfcebf8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMw7+dQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}