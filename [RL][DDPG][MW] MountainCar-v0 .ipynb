{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaBCa5TFAuQCYxJmaDLxTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20MountainCar-v0%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "5c570bf5-d8dc-4b2f-b65d-e71c46b27c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.linear(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Netword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size + action_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, 1)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "3fb21ce2-3784-4630-c2e0-2d53ceb21d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVjAObjOzgnFwm5kVzP8Hix1MdYmlJNkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.001\n",
        "CRITIC_LR = 0.001\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 2000\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "\n",
        "RECORD_INTERVAL = 100\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "n_obvs = 2\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    if sample < eps_threshold:\n",
        "            selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "    else:\n",
        "        selected_action = actor(\n",
        "             torch.FloatTensor(state).to(device)\n",
        "         ).detach().cpu().numpy()\n",
        "    _noise = noise.sample()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1 , -1\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + GAMMA * next_value * masks\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.smooth_l1_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    # train actor\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "d5d98db2-7586-4f70-d32a-e79b3c96eb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(np.argmax(action))\n",
        "        total_action_count[np.argmax(action)] += 1\n",
        "        reward  = obv[0]\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        if done :\n",
        "          if t != 199:\n",
        "            reward += 100\n",
        "            print('success')\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward))\n",
        "            print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        actor_target.load_state_dict(actor.state_dict())\n",
        "        critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 200 step , 0.57 Actor Loss, 0.01 Critic Loss,  0.73 Threshold , -0.36 Top reward\n",
            "[50, 54, 96]\n",
            "1 episode , 200 step , 1.48 Actor Loss, 0.00 Critic Loss,  0.66 Threshold , -0.45 Top reward\n",
            "[117, 44, 39]\n",
            "2 episode , 200 step , 1.96 Actor Loss, 0.00 Critic Loss,  0.60 Threshold , -0.39 Top reward\n",
            "[72, 39, 89]\n",
            "3 episode , 200 step , 2.35 Actor Loss, 0.01 Critic Loss,  0.55 Threshold , -0.34 Top reward\n",
            "[47, 38, 115]\n",
            "4 episode , 200 step , 2.75 Actor Loss, 0.01 Critic Loss,  0.50 Threshold , -0.21 Top reward\n",
            "[35, 45, 120]\n",
            "5 episode , 200 step , 3.13 Actor Loss, 0.01 Critic Loss,  0.46 Threshold , -0.30 Top reward\n",
            "[32, 29, 139]\n",
            "6 episode , 200 step , 3.53 Actor Loss, 0.01 Critic Loss,  0.42 Threshold , -0.29 Top reward\n",
            "[27, 25, 148]\n",
            "7 episode , 200 step , 3.90 Actor Loss, 0.02 Critic Loss,  0.39 Threshold , -0.39 Top reward\n",
            "[20, 39, 141]\n",
            "8 episode , 200 step , 4.27 Actor Loss, 0.02 Critic Loss,  0.35 Threshold , -0.25 Top reward\n",
            "[21, 23, 156]\n",
            "9 episode , 200 step , 4.66 Actor Loss, 0.02 Critic Loss,  0.33 Threshold , -0.28 Top reward\n",
            "[15, 23, 162]\n",
            "10 episode , 200 step , 5.03 Actor Loss, 0.02 Critic Loss,  0.30 Threshold , -0.38 Top reward\n",
            "[26, 24, 150]\n",
            "11 episode , 200 step , 5.79 Actor Loss, 0.03 Critic Loss,  0.28 Threshold , -0.30 Top reward\n",
            "[21, 14, 165]\n",
            "12 episode , 200 step , 6.13 Actor Loss, 0.02 Critic Loss,  0.25 Threshold , -0.24 Top reward\n",
            "[12, 21, 167]\n",
            "13 episode , 200 step , 6.45 Actor Loss, 0.03 Critic Loss,  0.23 Threshold , -0.25 Top reward\n",
            "[15, 15, 170]\n",
            "14 episode , 200 step , 6.80 Actor Loss, 0.03 Critic Loss,  0.22 Threshold , -0.29 Top reward\n",
            "[16, 25, 159]\n",
            "15 episode , 200 step , 7.14 Actor Loss, 0.03 Critic Loss,  0.20 Threshold , -0.35 Top reward\n",
            "[10, 16, 174]\n",
            "16 episode , 200 step , 7.46 Actor Loss, 0.04 Critic Loss,  0.19 Threshold , -0.36 Top reward\n",
            "[6, 23, 171]\n",
            "17 episode , 200 step , 7.78 Actor Loss, 0.04 Critic Loss,  0.17 Threshold , -0.24 Top reward\n",
            "[12, 8, 180]\n",
            "18 episode , 200 step , 8.10 Actor Loss, 0.04 Critic Loss,  0.16 Threshold , -0.20 Top reward\n",
            "[13, 8, 179]\n",
            "19 episode , 200 step , 8.38 Actor Loss, 0.04 Critic Loss,  0.15 Threshold , -0.16 Top reward\n",
            "[10, 9, 181]\n",
            "20 episode , 200 step , 8.65 Actor Loss, 0.04 Critic Loss,  0.14 Threshold , -0.21 Top reward\n",
            "[9, 5, 186]\n",
            "21 episode , 200 step , 9.28 Actor Loss, 0.05 Critic Loss,  0.13 Threshold , -0.36 Top reward\n",
            "[7, 9, 184]\n",
            "22 episode , 200 step , 9.60 Actor Loss, 0.05 Critic Loss,  0.12 Threshold , -0.33 Top reward\n",
            "[5, 1, 194]\n",
            "23 episode , 200 step , 9.88 Actor Loss, 0.05 Critic Loss,  0.12 Threshold , -0.33 Top reward\n",
            "[8, 5, 187]\n",
            "24 episode , 200 step , 10.19 Actor Loss, 0.05 Critic Loss,  0.11 Threshold , -0.33 Top reward\n",
            "[9, 6, 185]\n",
            "25 episode , 200 step , 10.50 Actor Loss, 0.05 Critic Loss,  0.11 Threshold , -0.28 Top reward\n",
            "[2, 6, 192]\n",
            "26 episode , 200 step , 10.80 Actor Loss, 0.05 Critic Loss,  0.10 Threshold , -0.35 Top reward\n",
            "[6, 12, 182]\n",
            "27 episode , 200 step , 11.02 Actor Loss, 0.06 Critic Loss,  0.10 Threshold , -0.27 Top reward\n",
            "[6, 7, 187]\n",
            "28 episode , 200 step , 11.43 Actor Loss, 0.06 Critic Loss,  0.09 Threshold , -0.32 Top reward\n",
            "[7, 3, 190]\n",
            "29 episode , 200 step , 11.67 Actor Loss, 0.06 Critic Loss,  0.09 Threshold , -0.31 Top reward\n",
            "[6, 6, 188]\n",
            "30 episode , 200 step , 11.94 Actor Loss, 0.06 Critic Loss,  0.08 Threshold , -0.17 Top reward\n",
            "[5, 8, 187]\n",
            "31 episode , 200 step , 12.51 Actor Loss, 0.07 Critic Loss,  0.08 Threshold , -0.26 Top reward\n",
            "[6, 12, 182]\n",
            "32 episode , 200 step , 12.78 Actor Loss, 0.07 Critic Loss,  0.08 Threshold , -0.24 Top reward\n",
            "[4, 4, 192]\n",
            "33 episode , 200 step , 13.07 Actor Loss, 0.06 Critic Loss,  0.07 Threshold , -0.15 Top reward\n",
            "[5, 5, 190]\n",
            "34 episode , 200 step , 13.35 Actor Loss, 0.08 Critic Loss,  0.07 Threshold , -0.34 Top reward\n",
            "[4, 4, 192]\n",
            "35 episode , 200 step , 13.62 Actor Loss, 0.07 Critic Loss,  0.07 Threshold , -0.14 Top reward\n",
            "[5, 4, 191]\n",
            "36 episode , 200 step , 13.87 Actor Loss, 0.08 Critic Loss,  0.07 Threshold , -0.30 Top reward\n",
            "[7, 4, 189]\n",
            "37 episode , 200 step , 14.14 Actor Loss, 0.07 Critic Loss,  0.07 Threshold , -0.28 Top reward\n",
            "[7, 4, 189]\n",
            "38 episode , 200 step , 14.43 Actor Loss, 0.07 Critic Loss,  0.07 Threshold , -0.25 Top reward\n",
            "[7, 4, 189]\n",
            "39 episode , 200 step , 14.71 Actor Loss, 0.07 Critic Loss,  0.06 Threshold , -0.19 Top reward\n",
            "[6, 3, 191]\n",
            "40 episode , 200 step , 14.94 Actor Loss, 0.08 Critic Loss,  0.06 Threshold , -0.25 Top reward\n",
            "[20, 6, 174]\n",
            "41 episode , 200 step , 15.88 Actor Loss, 0.10 Critic Loss,  0.06 Threshold , -0.29 Top reward\n",
            "[20, 1, 179]\n",
            "42 episode , 200 step , 16.05 Actor Loss, 0.08 Critic Loss,  0.06 Threshold , -0.23 Top reward\n",
            "[5, 0, 195]\n",
            "43 episode , 200 step , 16.27 Actor Loss, 0.08 Critic Loss,  0.06 Threshold , -0.31 Top reward\n",
            "[0, 5, 195]\n",
            "44 episode , 200 step , 16.58 Actor Loss, 0.08 Critic Loss,  0.06 Threshold , -0.33 Top reward\n",
            "[4, 1, 195]\n",
            "45 episode , 200 step , 16.82 Actor Loss, 0.09 Critic Loss,  0.06 Threshold , -0.35 Top reward\n",
            "[4, 4, 192]\n",
            "46 episode , 200 step , 17.10 Actor Loss, 0.10 Critic Loss,  0.06 Threshold , -0.21 Top reward\n",
            "[5, 3, 192]\n",
            "47 episode , 200 step , 17.40 Actor Loss, 0.10 Critic Loss,  0.06 Threshold , -0.19 Top reward\n",
            "[5, 6, 189]\n",
            "48 episode , 200 step , 17.70 Actor Loss, 0.09 Critic Loss,  0.06 Threshold , -0.29 Top reward\n",
            "[2, 3, 195]\n",
            "49 episode , 200 step , 18.04 Actor Loss, 0.11 Critic Loss,  0.06 Threshold , -0.22 Top reward\n",
            "[3, 5, 192]\n",
            "50 episode , 200 step , 18.37 Actor Loss, 0.09 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[3, 2, 195]\n",
            "51 episode , 200 step , 19.00 Actor Loss, 0.10 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[2, 2, 196]\n",
            "52 episode , 200 step , 19.35 Actor Loss, 0.10 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[3, 2, 195]\n",
            "53 episode , 200 step , 19.70 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.19 Top reward\n",
            "[1, 2, 197]\n",
            "54 episode , 200 step , 20.06 Actor Loss, 0.10 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[5, 3, 192]\n",
            "55 episode , 200 step , 20.40 Actor Loss, 0.12 Critic Loss,  0.05 Threshold , -0.34 Top reward\n",
            "[2, 2, 196]\n",
            "56 episode , 200 step , 20.75 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[2, 3, 195]\n",
            "57 episode , 200 step , 21.11 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.15 Top reward\n",
            "[4, 4, 192]\n",
            "58 episode , 200 step , 21.42 Actor Loss, 0.12 Critic Loss,  0.05 Threshold , -0.21 Top reward\n",
            "[4, 70, 126]\n",
            "59 episode , 200 step , 21.78 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.37 Top reward\n",
            "[2, 92, 106]\n",
            "60 episode , 200 step , 22.11 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[3, 29, 168]\n",
            "61 episode , 200 step , 22.76 Actor Loss, 0.12 Critic Loss,  0.05 Threshold , -0.23 Top reward\n",
            "[3, 43, 154]\n",
            "62 episode , 200 step , 23.11 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.23 Top reward\n",
            "[5, 13, 182]\n",
            "63 episode , 200 step , 23.46 Actor Loss, 0.10 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[5, 3, 192]\n",
            "64 episode , 200 step , 23.80 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[1, 6, 193]\n",
            "65 episode , 200 step , 24.15 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[3, 2, 195]\n",
            "66 episode , 200 step , 24.49 Actor Loss, 0.11 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[2, 1, 197]\n",
            "67 episode , 200 step , 24.84 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[7, 2, 191]\n",
            "68 episode , 200 step , 25.18 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.34 Top reward\n",
            "[1, 2, 197]\n",
            "69 episode , 200 step , 25.52 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.24 Top reward\n",
            "[3, 6, 191]\n",
            "70 episode , 200 step , 25.86 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[1, 15, 184]\n",
            "71 episode , 200 step , 26.48 Actor Loss, 0.14 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[0, 1, 199]\n",
            "72 episode , 200 step , 26.81 Actor Loss, 0.14 Critic Loss,  0.05 Threshold , -0.20 Top reward\n",
            "[3, 8, 189]\n",
            "73 episode , 200 step , 27.14 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.17 Top reward\n",
            "[2, 9, 189]\n",
            "74 episode , 200 step , 27.47 Actor Loss, 0.13 Critic Loss,  0.05 Threshold , -0.19 Top reward\n",
            "[2, 3, 195]\n",
            "75 episode , 200 step , 27.79 Actor Loss, 0.12 Critic Loss,  0.05 Threshold , -0.22 Top reward\n",
            "[2, 3, 195]\n",
            "76 episode , 200 step , 28.12 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.22 Top reward\n",
            "[8, 2, 190]\n",
            "77 episode , 200 step , 28.45 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[8, 1, 191]\n",
            "78 episode , 200 step , 28.79 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[4, 1, 195]\n",
            "79 episode , 200 step , 29.12 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.15 Top reward\n",
            "[5, 2, 193]\n",
            "80 episode , 200 step , 29.46 Actor Loss, 0.16 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[3, 5, 192]\n",
            "81 episode , 200 step , 30.13 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[2, 3, 195]\n",
            "82 episode , 200 step , 30.46 Actor Loss, 0.14 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[2, 2, 196]\n",
            "83 episode , 200 step , 30.79 Actor Loss, 0.14 Critic Loss,  0.05 Threshold , -0.18 Top reward\n",
            "[4, 7, 189]\n",
            "84 episode , 200 step , 31.04 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.32 Top reward\n",
            "[6, 4, 190]\n",
            "85 episode , 200 step , 31.39 Actor Loss, 0.15 Critic Loss,  0.05 Threshold , -0.37 Top reward\n",
            "[6, 7, 187]\n",
            "86 episode , 200 step , 31.72 Actor Loss, 0.16 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[6, 3, 191]\n",
            "87 episode , 200 step , 32.04 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[3, 3, 194]\n",
            "88 episode , 200 step , 32.39 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , -0.35 Top reward\n",
            "[6, 5, 189]\n",
            "89 episode , 200 step , 32.70 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , -0.24 Top reward\n",
            "[6, 3, 191]\n",
            "90 episode , 200 step , 33.03 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , -0.18 Top reward\n",
            "[1, 3, 196]\n",
            "91 episode , 200 step , 33.55 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.08 Top reward\n",
            "[7, 25, 168]\n",
            "92 episode , 200 step , 33.84 Actor Loss, 0.16 Critic Loss,  0.05 Threshold , -0.37 Top reward\n",
            "[1, 3, 196]\n",
            "93 episode , 200 step , 34.13 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , 0.03 Top reward\n",
            "[18, 12, 170]\n",
            "94 episode , 200 step , 34.41 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.12 Top reward\n",
            "[4, 27, 169]\n",
            "95 episode , 200 step , 34.71 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.11 Top reward\n",
            "[4, 2, 194]\n",
            "96 episode , 200 step , 35.01 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.28 Top reward\n",
            "[9, 3, 188]\n",
            "97 episode , 200 step , 35.30 Actor Loss, 0.19 Critic Loss,  0.05 Threshold , -0.28 Top reward\n",
            "[6, 2, 192]\n",
            "98 episode , 200 step , 35.60 Actor Loss, 0.19 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[5, 2, 193]\n",
            "99 episode , 200 step , 35.90 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.34 Top reward\n",
            "[1, 3, 196]\n",
            "100 episode , 200 step , 36.19 Actor Loss, 0.17 Critic Loss,  0.05 Threshold , -0.18 Top reward\n",
            "[0, 2, 198]\n",
            "101 episode , 200 step , 36.74 Actor Loss, 0.19 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[1, 2, 197]\n",
            "102 episode , 200 step , 37.04 Actor Loss, 0.19 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[2, 6, 192]\n",
            "103 episode , 200 step , 37.35 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[5, 4, 191]\n",
            "104 episode , 200 step , 37.67 Actor Loss, 0.18 Critic Loss,  0.05 Threshold , -0.34 Top reward\n",
            "[3, 3, 194]\n",
            "105 episode , 200 step , 37.93 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.18 Top reward\n",
            "[2, 3, 195]\n",
            "106 episode , 200 step , 38.20 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.22 Top reward\n",
            "[2, 8, 190]\n",
            "107 episode , 200 step , 38.50 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.21 Top reward\n",
            "[3, 2, 195]\n",
            "108 episode , 200 step , 38.77 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.19 Top reward\n",
            "[2, 5, 193]\n",
            "109 episode , 200 step , 39.04 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.21 Top reward\n",
            "[2, 5, 193]\n",
            "110 episode , 200 step , 39.30 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[3, 4, 193]\n",
            "111 episode , 200 step , 39.83 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.22 Top reward\n",
            "[7, 3, 190]\n",
            "112 episode , 200 step , 40.17 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[5, 2, 193]\n",
            "113 episode , 200 step , 40.47 Actor Loss, 0.20 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[3, 2, 195]\n",
            "114 episode , 200 step , 40.75 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.23 Top reward\n",
            "[5, 4, 191]\n",
            "115 episode , 200 step , 41.01 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.20 Top reward\n",
            "[6, 2, 192]\n",
            "116 episode , 200 step , 41.30 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[0, 5, 195]\n",
            "117 episode , 200 step , 41.61 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[2, 5, 193]\n",
            "118 episode , 200 step , 41.91 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[4, 5, 191]\n",
            "119 episode , 200 step , 42.16 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[5, 3, 192]\n",
            "120 episode , 200 step , 42.46 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.34 Top reward\n",
            "[5, 1, 194]\n",
            "121 episode , 200 step , 42.91 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.35 Top reward\n",
            "[2, 8, 190]\n",
            "122 episode , 200 step , 43.16 Actor Loss, 0.23 Critic Loss,  0.05 Threshold , -0.15 Top reward\n",
            "[4, 43, 153]\n",
            "123 episode , 200 step , 43.13 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[1, 117, 82]\n",
            "124 episode , 200 step , 43.36 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.50 Top reward\n",
            "[0, 197, 3]\n",
            "125 episode , 200 step , 43.50 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.50 Top reward\n",
            "[6, 192, 2]\n",
            "126 episode , 200 step , 43.72 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.49 Top reward\n",
            "[2, 194, 4]\n",
            "127 episode , 200 step , 44.12 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.43 Top reward\n",
            "[8, 191, 1]\n",
            "128 episode , 200 step , 44.46 Actor Loss, 0.21 Critic Loss,  0.05 Threshold , -0.41 Top reward\n",
            "[1, 193, 6]\n",
            "129 episode , 200 step , 44.79 Actor Loss, 0.19 Critic Loss,  0.05 Threshold , -0.47 Top reward\n",
            "[4, 194, 2]\n",
            "130 episode , 200 step , 45.09 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.49 Top reward\n",
            "[4, 193, 3]\n",
            "131 episode , 200 step , 45.53 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.46 Top reward\n",
            "[4, 194, 2]\n",
            "132 episode , 200 step , 45.83 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.48 Top reward\n",
            "[11, 186, 3]\n",
            "133 episode , 200 step , 46.16 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.49 Top reward\n",
            "[13, 185, 2]\n",
            "134 episode , 200 step , 46.50 Actor Loss, 0.23 Critic Loss,  0.05 Threshold , -0.45 Top reward\n",
            "[141, 53, 6]\n",
            "135 episode , 200 step , 46.84 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.46 Top reward\n",
            "[31, 167, 2]\n",
            "136 episode , 200 step , 47.22 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.42 Top reward\n",
            "[74, 117, 9]\n",
            "137 episode , 200 step , 47.62 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.49 Top reward\n",
            "[2, 194, 4]\n",
            "138 episode , 200 step , 47.98 Actor Loss, 0.22 Critic Loss,  0.05 Threshold , -0.42 Top reward\n",
            "[53, 108, 39]\n",
            "139 episode , 200 step , 48.28 Actor Loss, 0.26 Critic Loss,  0.05 Threshold , -0.12 Top reward\n",
            "[33, 87, 80]\n",
            "140 episode , 200 step , 48.43 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[4, 124, 72]\n",
            "141 episode , 200 step , 48.75 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.30 Top reward\n",
            "[6, 2, 192]\n",
            "142 episode , 200 step , 48.96 Actor Loss, 0.23 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[2, 6, 192]\n",
            "143 episode , 200 step , 49.20 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.23 Top reward\n",
            "[6, 1, 193]\n",
            "144 episode , 200 step , 49.57 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[3, 2, 195]\n",
            "145 episode , 200 step , 49.77 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[4, 4, 192]\n",
            "146 episode , 200 step , 49.96 Actor Loss, 0.26 Critic Loss,  0.05 Threshold , -0.28 Top reward\n",
            "[2, 1, 197]\n",
            "147 episode , 200 step , 50.29 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.22 Top reward\n",
            "[5, 2, 193]\n",
            "148 episode , 200 step , 50.45 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[4, 0, 196]\n",
            "149 episode , 200 step , 50.28 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , 0.03 Top reward\n",
            "[49, 55, 96]\n",
            "150 episode , 200 step , 50.28 Actor Loss, 0.31 Critic Loss,  0.05 Threshold , 0.07 Top reward\n",
            "[49, 61, 90]\n",
            "151 episode , 200 step , 50.22 Actor Loss, 0.28 Critic Loss,  0.05 Threshold , -0.36 Top reward\n",
            "[4, 4, 192]\n",
            "152 episode , 200 step , 50.34 Actor Loss, 0.28 Critic Loss,  0.05 Threshold , -0.21 Top reward\n",
            "[6, 1, 193]\n",
            "153 episode , 200 step , 50.38 Actor Loss, 0.28 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[2, 1, 197]\n",
            "154 episode , 200 step , 49.47 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[60, 39, 101]\n",
            "155 episode , 200 step , 46.91 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , -0.51 Top reward\n",
            "[194, 3, 3]\n",
            "156 episode , 200 step , 47.05 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , -0.51 Top reward\n",
            "[195, 2, 3]\n",
            "157 episode , 200 step , 47.81 Actor Loss, 0.25 Critic Loss,  0.05 Threshold , -0.40 Top reward\n",
            "[194, 3, 3]\n",
            "158 episode , 200 step , 48.36 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.56 Top reward\n",
            "[195, 1, 4]\n",
            "159 episode , 200 step , 48.88 Actor Loss, 0.23 Critic Loss,  0.05 Threshold , -0.54 Top reward\n",
            "[190, 5, 5]\n",
            "160 episode , 200 step , 49.60 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.39 Top reward\n",
            "[184, 5, 11]\n",
            "161 episode , 200 step , 50.72 Actor Loss, 0.28 Critic Loss,  0.05 Threshold , -0.58 Top reward\n",
            "[194, 2, 4]\n",
            "162 episode , 200 step , 51.27 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , -0.42 Top reward\n",
            "[172, 8, 20]\n",
            "163 episode , 200 step , 51.83 Actor Loss, 0.24 Critic Loss,  0.05 Threshold , -0.50 Top reward\n",
            "[156, 5, 39]\n",
            "164 episode , 200 step , 52.45 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , 0.04 Top reward\n",
            "[113, 16, 71]\n",
            "165 episode , 200 step , 52.93 Actor Loss, 0.27 Critic Loss,  0.05 Threshold , 0.33 Top reward\n",
            "[73, 44, 83]\n",
            "166 episode , 200 step , 53.29 Actor Loss, 0.26 Critic Loss,  0.05 Threshold , 0.38 Top reward\n",
            "[78, 53, 69]\n",
            "success\n",
            "167 episode , 171 step , 53.52 Actor Loss, 0.31 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[41, 38, 92]\n",
            "success\n",
            "168 episode , 187 step , 53.75 Actor Loss, 0.29 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[37, 45, 105]\n",
            "success\n",
            "169 episode , 121 step , 53.82 Actor Loss, 0.31 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[34, 18, 69]\n",
            "170 episode , 200 step , 53.70 Actor Loss, 0.31 Critic Loss,  0.05 Threshold , -0.01 Top reward\n",
            "[85, 46, 69]\n",
            "171 episode , 200 step , 54.25 Actor Loss, 0.34 Critic Loss,  0.05 Threshold , -0.38 Top reward\n",
            "[2, 137, 61]\n",
            "172 episode , 200 step , 54.36 Actor Loss, 0.29 Critic Loss,  0.05 Threshold , -0.25 Top reward\n",
            "[8, 4, 188]\n",
            "173 episode , 200 step , 54.52 Actor Loss, 0.32 Critic Loss,  0.05 Threshold , -0.32 Top reward\n",
            "[4, 3, 193]\n",
            "174 episode , 200 step , 54.37 Actor Loss, 0.34 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[2, 2, 196]\n",
            "175 episode , 200 step , 54.37 Actor Loss, 0.30 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[4, 3, 193]\n",
            "176 episode , 200 step , 54.24 Actor Loss, 0.34 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[5, 3, 192]\n",
            "177 episode , 200 step , 54.14 Actor Loss, 0.33 Critic Loss,  0.05 Threshold , -0.32 Top reward\n",
            "[2, 5, 193]\n",
            "success\n",
            "178 episode , 122 step , 54.25 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[33, 14, 75]\n",
            "179 episode , 200 step , 53.93 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[9, 2, 189]\n",
            "180 episode , 200 step , 53.93 Actor Loss, 0.32 Critic Loss,  0.05 Threshold , -0.32 Top reward\n",
            "[1, 4, 195]\n",
            "181 episode , 200 step , 53.80 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 0.27 Top reward\n",
            "[37, 13, 150]\n",
            "182 episode , 200 step , 53.50 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , -0.33 Top reward\n",
            "[2, 4, 194]\n",
            "183 episode , 200 step , 53.25 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 0.16 Top reward\n",
            "[27, 29, 144]\n",
            "184 episode , 200 step , 52.96 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , -0.30 Top reward\n",
            "[3, 3, 194]\n",
            "185 episode , 200 step , 52.98 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[7, 1, 192]\n",
            "186 episode , 200 step , 52.60 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , -0.17 Top reward\n",
            "[4, 6, 190]\n",
            "187 episode , 200 step , 51.83 Actor Loss, 0.40 Critic Loss,  0.05 Threshold , -0.31 Top reward\n",
            "[7, 3, 190]\n",
            "188 episode , 200 step , 51.58 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , -0.27 Top reward\n",
            "[6, 1, 193]\n",
            "189 episode , 200 step , 51.26 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , 0.05 Top reward\n",
            "[4, 24, 172]\n",
            "190 episode , 200 step , 51.08 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , -0.23 Top reward\n",
            "[1, 3, 196]\n",
            "191 episode , 200 step , 51.00 Actor Loss, 0.41 Critic Loss,  0.05 Threshold , 0.02 Top reward\n",
            "[3, 43, 154]\n",
            "192 episode , 200 step , 50.98 Actor Loss, 0.36 Critic Loss,  0.05 Threshold , -0.09 Top reward\n",
            "[4, 61, 135]\n",
            "193 episode , 200 step , 50.81 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 0.06 Top reward\n",
            "[2, 77, 121]\n",
            "194 episode , 200 step , 50.59 Actor Loss, 0.37 Critic Loss,  0.05 Threshold , -0.02 Top reward\n",
            "[1, 66, 133]\n",
            "195 episode , 200 step , 50.08 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , -0.01 Top reward\n",
            "[0, 64, 136]\n",
            "196 episode , 200 step , 49.90 Actor Loss, 0.34 Critic Loss,  0.05 Threshold , 0.15 Top reward\n",
            "[5, 40, 155]\n",
            "197 episode , 200 step , 49.53 Actor Loss, 0.33 Critic Loss,  0.05 Threshold , -0.30 Top reward\n",
            "[3, 3, 194]\n",
            "198 episode , 200 step , 49.36 Actor Loss, 0.34 Critic Loss,  0.05 Threshold , 0.07 Top reward\n",
            "[3, 26, 171]\n",
            "199 episode , 200 step , 49.00 Actor Loss, 0.33 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[4, 2, 194]\n",
            "200 episode , 200 step , 49.09 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , -0.26 Top reward\n",
            "[6, 3, 191]\n",
            "201 episode , 200 step , 49.33 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 0.16 Top reward\n",
            "[1, 39, 160]\n",
            "202 episode , 200 step , 49.25 Actor Loss, 0.32 Critic Loss,  0.05 Threshold , 0.17 Top reward\n",
            "[4, 37, 159]\n",
            "203 episode , 200 step , 49.07 Actor Loss, 0.32 Critic Loss,  0.05 Threshold , -0.12 Top reward\n",
            "[5, 27, 168]\n",
            "204 episode , 200 step , 48.63 Actor Loss, 0.35 Critic Loss,  0.05 Threshold , 0.12 Top reward\n",
            "[10, 65, 125]\n",
            "205 episode , 200 step , 48.02 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 0.30 Top reward\n",
            "[16, 43, 141]\n",
            "206 episode , 200 step , 47.64 Actor Loss, 0.39 Critic Loss,  0.05 Threshold , -0.45 Top reward\n",
            "[107, 52, 41]\n",
            "207 episode , 200 step , 47.42 Actor Loss, 0.38 Critic Loss,  0.05 Threshold , 0.18 Top reward\n",
            "[51, 44, 105]\n",
            "208 episode , 200 step , 46.67 Actor Loss, 0.45 Critic Loss,  0.05 Threshold , -0.47 Top reward\n",
            "[2, 196, 2]\n",
            "209 episode , 200 step , 44.78 Actor Loss, 0.61 Critic Loss,  0.05 Threshold , -0.45 Top reward\n",
            "[5, 193, 2]\n",
            "210 episode , 200 step , 43.60 Actor Loss, 0.88 Critic Loss,  0.05 Threshold , -0.44 Top reward\n",
            "[1, 197, 2]\n",
            "211 episode , 200 step , 42.85 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , -0.43 Top reward\n",
            "[5, 159, 36]\n",
            "212 episode , 200 step , 43.02 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , -0.13 Top reward\n",
            "[52, 63, 85]\n",
            "213 episode , 200 step , 42.36 Actor Loss, 0.47 Critic Loss,  0.05 Threshold , -0.07 Top reward\n",
            "[45, 108, 47]\n",
            "success\n",
            "214 episode , 166 step , 41.82 Actor Loss, 0.45 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[40, 22, 104]\n",
            "success\n",
            "215 episode , 159 step , 40.34 Actor Loss, 0.44 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[32, 15, 112]\n",
            "success\n",
            "216 episode , 168 step , 39.83 Actor Loss, 0.44 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 14, 117]\n",
            "success\n",
            "217 episode , 147 step , 39.29 Actor Loss, 0.44 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[21, 20, 106]\n",
            "success\n",
            "218 episode , 181 step , 38.66 Actor Loss, 0.41 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[37, 33, 111]\n",
            "success\n",
            "219 episode , 125 step , 38.11 Actor Loss, 0.41 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[32, 40, 53]\n",
            "success\n",
            "220 episode , 168 step , 36.78 Actor Loss, 0.51 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[78, 14, 76]\n",
            "221 episode , 200 step , 33.98 Actor Loss, 0.54 Critic Loss,  0.05 Threshold , 0.19 Top reward\n",
            "[141, 6, 53]\n",
            "success\n",
            "222 episode , 159 step , 33.58 Actor Loss, 0.43 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[66, 27, 66]\n",
            "success\n",
            "223 episode , 169 step , 32.32 Actor Loss, 0.47 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[94, 18, 57]\n",
            "success\n",
            "224 episode , 148 step , 31.50 Actor Loss, 0.45 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[66, 26, 56]\n",
            "success\n",
            "225 episode , 164 step , 29.53 Actor Loss, 0.45 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[63, 39, 62]\n",
            "success\n",
            "226 episode , 155 step , 28.29 Actor Loss, 0.47 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[77, 21, 57]\n",
            "success\n",
            "227 episode , 196 step , 26.66 Actor Loss, 0.47 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[72, 68, 56]\n",
            "success\n",
            "228 episode , 160 step , 24.68 Actor Loss, 0.48 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[71, 32, 57]\n",
            "success\n",
            "229 episode , 152 step , 23.22 Actor Loss, 0.49 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[76, 12, 64]\n",
            "success\n",
            "230 episode , 153 step , 21.60 Actor Loss, 0.54 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[69, 17, 67]\n",
            "success\n",
            "231 episode , 154 step , 18.11 Actor Loss, 0.62 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[70, 14, 70]\n",
            "success\n",
            "232 episode , 156 step , 16.66 Actor Loss, 0.60 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[79, 3, 74]\n",
            "success\n",
            "233 episode , 163 step , 15.02 Actor Loss, 0.56 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[73, 13, 77]\n",
            "success\n",
            "234 episode , 162 step , 12.49 Actor Loss, 0.62 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[76, 11, 75]\n",
            "success\n",
            "235 episode , 161 step , 10.68 Actor Loss, 0.61 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[72, 13, 76]\n",
            "success\n",
            "236 episode , 158 step , 9.18 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[72, 13, 73]\n",
            "success\n",
            "237 episode , 158 step , 6.34 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[68, 12, 78]\n",
            "238 episode , 200 step , 5.52 Actor Loss, 0.64 Critic Loss,  0.05 Threshold , -0.29 Top reward\n",
            "[158, 35, 7]\n",
            "success\n",
            "239 episode , 157 step , 4.45 Actor Loss, 0.62 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[72, 13, 72]\n",
            "success\n",
            "240 episode , 153 step , 3.06 Actor Loss, 0.63 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[69, 7, 77]\n",
            "success\n",
            "241 episode , 147 step , 0.77 Actor Loss, 0.70 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[69, 1, 77]\n",
            "success\n",
            "242 episode , 155 step , -0.97 Actor Loss, 0.70 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[76, 3, 76]\n",
            "success\n",
            "243 episode , 92 step , -2.75 Actor Loss, 0.73 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[34, 1, 57]\n",
            "success\n",
            "244 episode , 173 step , -4.60 Actor Loss, 0.78 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[65, 13, 95]\n",
            "success\n",
            "245 episode , 147 step , -6.21 Actor Loss, 0.69 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[71, 4, 72]\n",
            "success\n",
            "246 episode , 164 step , -7.67 Actor Loss, 0.80 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[67, 0, 97]\n",
            "success\n",
            "247 episode , 154 step , -9.93 Actor Loss, 0.79 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[69, 7, 78]\n",
            "success\n",
            "248 episode , 92 step , -11.47 Actor Loss, 0.88 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[37, 1, 54]\n",
            "success\n",
            "249 episode , 90 step , -12.31 Actor Loss, 0.78 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[36, 0, 54]\n",
            "success\n",
            "250 episode , 150 step , -13.95 Actor Loss, 0.89 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[72, 9, 69]\n",
            "success\n",
            "251 episode , 151 step , -16.60 Actor Loss, 0.99 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[81, 9, 61]\n",
            "success\n",
            "252 episode , 122 step , -17.60 Actor Loss, 0.96 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[28, 5, 89]\n",
            "success\n",
            "253 episode , 128 step , -19.23 Actor Loss, 1.04 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[31, 2, 95]\n",
            "success\n",
            "254 episode , 119 step , -20.46 Actor Loss, 0.98 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[33, 5, 81]\n",
            "success\n",
            "255 episode , 116 step , -22.54 Actor Loss, 1.16 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[32, 3, 81]\n",
            "success\n",
            "256 episode , 113 step , -24.22 Actor Loss, 1.05 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[29, 5, 79]\n",
            "success\n",
            "257 episode , 90 step , -25.88 Actor Loss, 1.04 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[35, 0, 55]\n",
            "success\n",
            "258 episode , 112 step , -26.46 Actor Loss, 1.07 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[35, 1, 76]\n",
            "success\n",
            "259 episode , 115 step , -28.03 Actor Loss, 1.13 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[35, 5, 75]\n",
            "success\n",
            "260 episode , 112 step , -29.47 Actor Loss, 1.10 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[33, 3, 76]\n",
            "success\n",
            "261 episode , 113 step , -33.25 Actor Loss, 1.27 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[38, 3, 72]\n",
            "success\n",
            "262 episode , 125 step , -35.45 Actor Loss, 1.25 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 0, 88]\n",
            "success\n",
            "263 episode , 116 step , -37.14 Actor Loss, 1.33 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[36, 2, 78]\n",
            "success\n",
            "264 episode , 124 step , -38.35 Actor Loss, 1.27 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[37, 1, 86]\n",
            "success\n",
            "265 episode , 120 step , -41.17 Actor Loss, 1.36 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[34, 3, 83]\n",
            "success\n",
            "266 episode , 112 step , -43.13 Actor Loss, 1.48 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[39, 0, 73]\n",
            "success\n",
            "267 episode , 111 step , -45.18 Actor Loss, 1.40 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[39, 3, 69]\n",
            "success\n",
            "268 episode , 153 step , -47.86 Actor Loss, 1.54 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[44, 22, 87]\n",
            "success\n",
            "269 episode , 114 step , -51.75 Actor Loss, 1.57 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[39, 6, 69]\n",
            "success\n",
            "270 episode , 114 step , -53.52 Actor Loss, 1.55 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[44, 2, 68]\n",
            "success\n",
            "271 episode , 116 step , -58.27 Actor Loss, 1.80 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[42, 3, 71]\n",
            "success\n",
            "272 episode , 117 step , -59.50 Actor Loss, 1.71 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[47, 2, 68]\n",
            "success\n",
            "273 episode , 92 step , -60.91 Actor Loss, 1.55 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[42, 0, 50]\n",
            "success\n",
            "274 episode , 122 step , -63.71 Actor Loss, 1.73 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[44, 2, 76]\n",
            "success\n",
            "275 episode , 118 step , -66.79 Actor Loss, 1.82 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[48, 5, 65]\n",
            "success\n",
            "276 episode , 118 step , -69.16 Actor Loss, 1.96 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[43, 4, 71]\n",
            "success\n",
            "277 episode , 111 step , -70.97 Actor Loss, 1.82 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[45, 0, 66]\n",
            "success\n",
            "278 episode , 114 step , -74.10 Actor Loss, 2.02 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[46, 1, 67]\n",
            "success\n",
            "279 episode , 109 step , -77.24 Actor Loss, 2.15 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[41, 4, 64]\n",
            "success\n",
            "280 episode , 113 step , -79.79 Actor Loss, 2.17 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[49, 1, 63]\n",
            "success\n",
            "281 episode , 108 step , -84.97 Actor Loss, 2.61 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[44, 4, 60]\n",
            "success\n",
            "282 episode , 109 step , -88.05 Actor Loss, 2.21 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[46, 1, 62]\n",
            "success\n",
            "283 episode , 111 step , -92.55 Actor Loss, 2.33 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[47, 1, 63]\n",
            "success\n",
            "284 episode , 110 step , -96.80 Actor Loss, 2.42 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[45, 1, 64]\n",
            "success\n",
            "285 episode , 113 step , -96.65 Actor Loss, 2.37 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[47, 4, 62]\n",
            "success\n",
            "286 episode , 113 step , -99.53 Actor Loss, 2.34 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[48, 1, 64]\n",
            "success\n",
            "287 episode , 110 step , -103.52 Actor Loss, 2.46 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[46, 4, 60]\n",
            "success\n",
            "288 episode , 89 step , -106.50 Actor Loss, 2.71 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 2, 47]\n",
            "success\n",
            "289 episode , 139 step , -109.49 Actor Loss, 2.67 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[55, 15, 69]\n",
            "success\n",
            "290 episode , 109 step , -111.18 Actor Loss, 2.75 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[43, 10, 56]\n",
            "success\n",
            "291 episode , 107 step , -116.80 Actor Loss, 3.27 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[43, 8, 56]\n",
            "success\n",
            "292 episode , 147 step , -119.81 Actor Loss, 2.99 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 32, 75]\n",
            "success\n",
            "293 episode , 95 step , -123.47 Actor Loss, 2.66 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 12, 46]\n",
            "success\n",
            "294 episode , 112 step , -123.57 Actor Loss, 2.78 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[31, 18, 63]\n",
            "success\n",
            "295 episode , 107 step , -124.33 Actor Loss, 2.85 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 9, 61]\n",
            "success\n",
            "296 episode , 112 step , -127.64 Actor Loss, 2.79 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[27, 19, 66]\n",
            "success\n",
            "297 episode , 114 step , -129.31 Actor Loss, 3.02 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[25, 21, 68]\n",
            "298 episode , 200 step , -132.60 Actor Loss, 3.00 Critic Loss,  0.05 Threshold , 0.26 Top reward\n",
            "[47, 44, 109]\n",
            "success\n",
            "299 episode , 114 step , -135.58 Actor Loss, 2.90 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[27, 18, 69]\n",
            "success\n",
            "300 episode , 196 step , -137.43 Actor Loss, 3.06 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[55, 37, 104]\n",
            "success\n",
            "301 episode , 145 step , -144.98 Actor Loss, 3.36 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[58, 26, 61]\n",
            "success\n",
            "302 episode , 145 step , -147.59 Actor Loss, 3.28 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[62, 19, 64]\n",
            "success\n",
            "303 episode , 150 step , -149.55 Actor Loss, 2.99 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[59, 30, 61]\n",
            "success\n",
            "304 episode , 165 step , -152.28 Actor Loss, 3.13 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[56, 16, 93]\n",
            "success\n",
            "305 episode , 161 step , -155.57 Actor Loss, 3.18 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[63, 10, 88]\n",
            "success\n",
            "306 episode , 145 step , -157.80 Actor Loss, 3.17 Critic Loss,  0.05 Threshold , 0.45 Top reward\n",
            "[68, 3, 74]\n",
            "success\n",
            "307 episode , 183 step , -160.59 Actor Loss, 3.10 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[62, 8, 113]\n",
            "success\n",
            "308 episode , 163 step , -164.55 Actor Loss, 3.03 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[66, 4, 93]\n",
            "success\n",
            "309 episode , 89 step , -166.39 Actor Loss, 3.42 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[41, 2, 46]\n",
            "310 episode , 200 step , -168.28 Actor Loss, 3.19 Critic Loss,  0.05 Threshold , -0.54 Top reward\n",
            "[195, 3, 2]\n",
            "success\n",
            "311 episode , 164 step , -173.65 Actor Loss, 3.62 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[76, 4, 84]\n",
            "success\n",
            "312 episode , 111 step , -176.72 Actor Loss, 3.12 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[44, 1, 66]\n",
            "313 episode , 200 step , -178.83 Actor Loss, 3.54 Critic Loss,  0.05 Threshold , -0.54 Top reward\n",
            "[193, 3, 4]\n",
            "314 episode , 200 step , -179.18 Actor Loss, 3.41 Critic Loss,  0.05 Threshold , -0.57 Top reward\n",
            "[192, 2, 6]\n",
            "315 episode , 200 step , -180.26 Actor Loss, 3.50 Critic Loss,  0.05 Threshold , -0.59 Top reward\n",
            "[191, 7, 2]\n",
            "success\n",
            "316 episode , 171 step , -184.09 Actor Loss, 3.75 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[90, 8, 73]\n",
            "success\n",
            "317 episode , 92 step , -185.38 Actor Loss, 3.86 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[36, 1, 55]\n",
            "success\n",
            "318 episode , 145 step , -188.84 Actor Loss, 3.82 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[71, 8, 66]\n",
            "success\n",
            "319 episode , 147 step , -191.18 Actor Loss, 3.82 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[78, 4, 65]\n",
            "success\n",
            "320 episode , 154 step , -194.23 Actor Loss, 3.86 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[64, 13, 77]\n",
            "success\n",
            "321 episode , 144 step , -201.90 Actor Loss, 4.09 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[65, 6, 73]\n",
            "success\n",
            "322 episode , 142 step , -204.23 Actor Loss, 3.96 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[64, 4, 74]\n",
            "success\n",
            "323 episode , 159 step , -208.37 Actor Loss, 3.73 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[63, 4, 92]\n",
            "success\n",
            "324 episode , 142 step , -210.95 Actor Loss, 4.05 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[78, 1, 63]\n",
            "success\n",
            "325 episode , 148 step , -213.74 Actor Loss, 3.96 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[81, 2, 65]\n",
            "success\n",
            "326 episode , 143 step , -215.85 Actor Loss, 4.23 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[79, 3, 61]\n",
            "success\n",
            "327 episode , 86 step , -219.92 Actor Loss, 4.39 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[36, 1, 49]\n",
            "success\n",
            "328 episode , 142 step , -221.33 Actor Loss, 3.99 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[80, 1, 61]\n",
            "329 episode , 200 step , -224.49 Actor Loss, 4.64 Critic Loss,  0.05 Threshold , -0.57 Top reward\n",
            "[189, 5, 6]\n",
            "success\n",
            "330 episode , 149 step , -224.99 Actor Loss, 4.16 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[77, 2, 70]\n",
            "success\n",
            "331 episode , 162 step , -230.28 Actor Loss, 4.42 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[89, 3, 70]\n",
            "success\n",
            "332 episode , 126 step , -234.25 Actor Loss, 4.71 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[30, 1, 95]\n",
            "success\n",
            "333 episode , 86 step , -235.84 Actor Loss, 4.50 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[31, 1, 54]\n",
            "334 episode , 200 step , -238.29 Actor Loss, 4.67 Critic Loss,  0.05 Threshold , -0.04 Top reward\n",
            "[149, 5, 46]\n",
            "success\n",
            "335 episode , 156 step , -241.73 Actor Loss, 4.45 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[65, 5, 86]\n",
            "success\n",
            "336 episode , 97 step , -243.72 Actor Loss, 4.44 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[32, 1, 64]\n",
            "success\n",
            "337 episode , 99 step , -244.99 Actor Loss, 4.53 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[32, 1, 66]\n",
            "success\n",
            "338 episode , 90 step , -248.23 Actor Loss, 4.50 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[32, 0, 58]\n",
            "success\n",
            "339 episode , 155 step , -250.07 Actor Loss, 4.89 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[70, 2, 83]\n",
            "success\n",
            "340 episode , 93 step , -253.87 Actor Loss, 4.71 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[36, 0, 57]\n",
            "success\n",
            "341 episode , 154 step , -259.81 Actor Loss, 4.50 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[72, 2, 80]\n",
            "success\n",
            "342 episode , 154 step , -262.01 Actor Loss, 4.95 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[74, 4, 76]\n",
            "success\n",
            "343 episode , 153 step , -267.05 Actor Loss, 4.71 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[65, 2, 86]\n",
            "success\n",
            "344 episode , 153 step , -267.88 Actor Loss, 4.77 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[66, 4, 83]\n",
            "success\n",
            "345 episode , 167 step , -272.15 Actor Loss, 4.50 Critic Loss,  0.05 Threshold , 0.45 Top reward\n",
            "[69, 3, 95]\n",
            "success\n",
            "346 episode , 151 step , -274.99 Actor Loss, 4.69 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[66, 2, 83]\n",
            "success\n",
            "347 episode , 95 step , -277.94 Actor Loss, 4.60 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[38, 3, 54]\n",
            "success\n",
            "348 episode , 145 step , -281.24 Actor Loss, 4.80 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[64, 13, 68]\n",
            "success\n",
            "349 episode , 95 step , -284.08 Actor Loss, 5.44 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 1, 57]\n",
            "success\n",
            "350 episode , 151 step , -285.22 Actor Loss, 5.36 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[69, 6, 76]\n",
            "success\n",
            "351 episode , 149 step , -295.36 Actor Loss, 5.82 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[72, 1, 76]\n",
            "success\n",
            "352 episode , 146 step , -297.86 Actor Loss, 5.69 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[71, 2, 73]\n",
            "success\n",
            "353 episode , 160 step , -300.85 Actor Loss, 5.59 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[76, 1, 83]\n",
            "success\n",
            "354 episode , 111 step , -305.03 Actor Loss, 5.39 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[44, 0, 67]\n",
            "success\n",
            "355 episode , 110 step , -308.41 Actor Loss, 6.41 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[43, 1, 66]\n",
            "success\n",
            "356 episode , 102 step , -310.80 Actor Loss, 5.78 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[41, 3, 58]\n",
            "success\n",
            "357 episode , 87 step , -311.95 Actor Loss, 5.78 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[39, 1, 47]\n",
            "success\n",
            "358 episode , 154 step , -314.99 Actor Loss, 5.69 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[77, 3, 74]\n",
            "success\n",
            "359 episode , 108 step , -319.84 Actor Loss, 6.06 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[40, 4, 64]\n",
            "success\n",
            "360 episode , 109 step , -321.71 Actor Loss, 5.95 Critic Loss,  0.05 Threshold , 0.45 Top reward\n",
            "[43, 0, 66]\n",
            "success\n",
            "361 episode , 109 step , -329.85 Actor Loss, 6.73 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[41, 2, 66]\n",
            "success\n",
            "362 episode , 158 step , -333.70 Actor Loss, 6.37 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[75, 7, 76]\n",
            "success\n",
            "363 episode , 111 step , -336.24 Actor Loss, 5.71 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[39, 2, 70]\n",
            "success\n",
            "364 episode , 108 step , -342.38 Actor Loss, 6.10 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[42, 1, 65]\n",
            "success\n",
            "365 episode , 86 step , -340.66 Actor Loss, 6.03 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[37, 0, 49]\n",
            "success\n",
            "366 episode , 107 step , -344.72 Actor Loss, 6.23 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[40, 1, 66]\n",
            "success\n",
            "367 episode , 111 step , -347.83 Actor Loss, 6.37 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[38, 2, 71]\n",
            "success\n",
            "368 episode , 107 step , -349.97 Actor Loss, 6.71 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[40, 2, 65]\n",
            "success\n",
            "369 episode , 110 step , -352.90 Actor Loss, 5.96 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[41, 2, 67]\n",
            "success\n",
            "370 episode , 107 step , -355.64 Actor Loss, 5.88 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[39, 0, 68]\n",
            "success\n",
            "371 episode , 106 step , -360.04 Actor Loss, 6.37 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[39, 3, 64]\n",
            "success\n",
            "372 episode , 109 step , -366.26 Actor Loss, 6.48 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[43, 0, 66]\n",
            "success\n",
            "373 episode , 108 step , -369.19 Actor Loss, 6.43 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[41, 0, 67]\n",
            "success\n",
            "374 episode , 95 step , -372.92 Actor Loss, 6.70 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 0, 55]\n",
            "success\n",
            "375 episode , 103 step , -374.60 Actor Loss, 6.33 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[38, 1, 64]\n",
            "success\n",
            "376 episode , 109 step , -377.87 Actor Loss, 6.18 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[42, 4, 63]\n",
            "success\n",
            "377 episode , 95 step , -379.74 Actor Loss, 7.26 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[40, 1, 54]\n",
            "success\n",
            "378 episode , 105 step , -383.75 Actor Loss, 6.95 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 1, 64]\n",
            "success\n",
            "379 episode , 109 step , -386.52 Actor Loss, 6.77 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[41, 2, 66]\n",
            "success\n",
            "380 episode , 93 step , -389.01 Actor Loss, 6.81 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 0, 53]\n",
            "success\n",
            "381 episode , 109 step , -399.73 Actor Loss, 7.87 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[42, 0, 67]\n",
            "success\n",
            "382 episode , 106 step , -402.17 Actor Loss, 7.51 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[40, 0, 66]\n",
            "success\n",
            "383 episode , 110 step , -403.90 Actor Loss, 6.76 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[43, 1, 66]\n",
            "success\n",
            "384 episode , 107 step , -410.36 Actor Loss, 7.14 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[38, 2, 67]\n",
            "success\n",
            "385 episode , 116 step , -412.97 Actor Loss, 7.45 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[39, 11, 66]\n",
            "success\n",
            "386 episode , 112 step , -419.39 Actor Loss, 7.63 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[41, 5, 66]\n",
            "success\n",
            "387 episode , 106 step , -419.54 Actor Loss, 7.27 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[38, 4, 64]\n",
            "success\n",
            "388 episode , 92 step , -425.02 Actor Loss, 8.08 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[34, 5, 53]\n",
            "success\n",
            "389 episode , 108 step , -428.76 Actor Loss, 7.47 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[42, 2, 64]\n",
            "success\n",
            "390 episode , 107 step , -431.33 Actor Loss, 7.96 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[38, 6, 63]\n",
            "success\n",
            "391 episode , 105 step , -437.91 Actor Loss, 7.90 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[39, 0, 66]\n",
            "success\n",
            "392 episode , 110 step , -445.65 Actor Loss, 8.08 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[43, 1, 66]\n",
            "success\n",
            "393 episode , 110 step , -451.93 Actor Loss, 7.62 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[43, 1, 66]\n",
            "success\n",
            "394 episode , 108 step , -456.39 Actor Loss, 8.42 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[42, 1, 65]\n",
            "success\n",
            "395 episode , 122 step , -463.56 Actor Loss, 8.00 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[41, 12, 69]\n",
            "success\n",
            "396 episode , 108 step , -471.32 Actor Loss, 7.85 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[43, 0, 65]\n",
            "success\n",
            "397 episode , 109 step , -478.57 Actor Loss, 8.24 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[42, 2, 65]\n",
            "success\n",
            "398 episode , 107 step , -482.44 Actor Loss, 8.34 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[43, 0, 64]\n",
            "success\n",
            "399 episode , 107 step , -487.20 Actor Loss, 9.63 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[42, 0, 65]\n",
            "success\n",
            "400 episode , 111 step , -490.18 Actor Loss, 8.79 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[44, 2, 65]\n",
            "success\n",
            "401 episode , 110 step , -500.36 Actor Loss, 9.04 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[41, 6, 63]\n",
            "success\n",
            "402 episode , 87 step , -504.20 Actor Loss, 8.54 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[37, 1, 49]\n",
            "success\n",
            "403 episode , 86 step , -506.79 Actor Loss, 8.32 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[36, 1, 49]\n",
            "success\n",
            "404 episode , 108 step , -512.90 Actor Loss, 9.63 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[40, 1, 67]\n",
            "success\n",
            "405 episode , 160 step , -516.69 Actor Loss, 8.98 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[63, 7, 90]\n",
            "success\n",
            "406 episode , 91 step , -521.77 Actor Loss, 8.31 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[37, 3, 51]\n",
            "success\n",
            "407 episode , 110 step , -522.61 Actor Loss, 9.33 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[40, 1, 69]\n",
            "success\n",
            "408 episode , 88 step , -528.18 Actor Loss, 9.55 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[34, 2, 52]\n",
            "success\n",
            "409 episode , 144 step , -530.96 Actor Loss, 9.50 Critic Loss,  0.05 Threshold , 0.46 Top reward\n",
            "[75, 0, 69]\n",
            "success\n",
            "410 episode , 143 step , -536.08 Actor Loss, 9.23 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[69, 3, 71]\n",
            "success\n",
            "411 episode , 111 step , -551.34 Actor Loss, 9.82 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[40, 3, 68]\n",
            "success\n",
            "412 episode , 137 step , -552.77 Actor Loss, 9.29 Critic Loss,  0.05 Threshold , 0.49 Top reward\n",
            "[69, 1, 67]\n",
            "success\n",
            "413 episode , 112 step , -555.84 Actor Loss, 9.53 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[38, 1, 73]\n",
            "success\n",
            "414 episode , 118 step , -565.82 Actor Loss, 9.04 Critic Loss,  0.05 Threshold , 0.50 Top reward\n",
            "[34, 7, 77]\n",
            "success\n",
            "415 episode , 114 step , -574.95 Actor Loss, 9.94 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[38, 2, 74]\n",
            "success\n",
            "416 episode , 113 step , -575.30 Actor Loss, 9.91 Critic Loss,  0.05 Threshold , 0.47 Top reward\n",
            "[39, 2, 72]\n",
            "success\n",
            "417 episode , 110 step , -581.60 Actor Loss, 9.10 Critic Loss,  0.05 Threshold , 0.48 Top reward\n",
            "[38, 2, 70]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "!pip install JSAnimation\n",
        "from matplotlib import animation\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        plt.gcf(), animate, frames = len(frames), interval=50\n",
        "    )\n",
        "    display(display_animation(anim, default_mode='loop'))\n",
        "    \n",
        "        \n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8LGoLHib8F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}