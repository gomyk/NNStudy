{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden_size = 5\n",
    "num_classes = 5\n",
    "input_size = 5\n",
    "seq_size = 1\n",
    "num_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_idx(text):\n",
    "    l = []\n",
    "    for t in text:\n",
    "        l.append(idxchar.index(t))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxchar = ['e','h','i','l','o']\n",
    "one_hot_lookup = np.eye(num_classes, num_classes, dtype=np.int32).tolist()\n",
    "\n",
    "input_text = \"hihell\"\n",
    "output_text = \"ihello\"\n",
    "input_one_hot = []\n",
    "output_one_hot = []\n",
    "\n",
    "inputs= torch.Tensor([one_hot_lookup[l] for l in text_to_idx(input_text)])\n",
    "labels= torch.LongTensor(text_to_idx(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x=x.view(batch_size, seq_size, input_size)\n",
    "        out,hidden = self.rnn(x,hidden)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(num_layer,batch_size,hidden_size).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myModel().cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llllll, epoch: 1, loss: 9.809\n",
      "lhllll, epoch: 2, loss: 9.075\n",
      "eoelll, epoch: 3, loss: 8.497\n",
      "eoelll, epoch: 4, loss: 7.999\n",
      "eoelll, epoch: 5, loss: 7.536\n",
      "ehelll, epoch: 6, loss: 7.081\n",
      "ehelll, epoch: 7, loss: 6.635\n",
      "ehelll, epoch: 8, loss: 6.221\n",
      "ehelll, epoch: 9, loss: 5.868\n",
      "ehelll, epoch: 10, loss: 5.597\n",
      "ihilll, epoch: 11, loss: 5.398\n",
      "ihilll, epoch: 12, loss: 5.249\n",
      "ihilll, epoch: 13, loss: 5.131\n",
      "ihilll, epoch: 14, loss: 5.031\n",
      "ehelll, epoch: 15, loss: 4.944\n",
      "ehelll, epoch: 16, loss: 4.868\n",
      "ehelll, epoch: 17, loss: 4.802\n",
      "ehelll, epoch: 18, loss: 4.746\n",
      "ehelll, epoch: 19, loss: 4.699\n",
      "ehelll, epoch: 20, loss: 4.660\n",
      "ehelll, epoch: 21, loss: 4.628\n",
      "ehelll, epoch: 22, loss: 4.602\n",
      "ehelll, epoch: 23, loss: 4.580\n",
      "ehelll, epoch: 24, loss: 4.562\n",
      "ehelll, epoch: 25, loss: 4.547\n",
      "ehelll, epoch: 26, loss: 4.534\n",
      "ehelll, epoch: 27, loss: 4.523\n",
      "ehelll, epoch: 28, loss: 4.513\n",
      "ehelll, epoch: 29, loss: 4.504\n",
      "ehelll, epoch: 30, loss: 4.497\n",
      "ehelll, epoch: 31, loss: 4.490\n",
      "ehelll, epoch: 32, loss: 4.484\n",
      "ehelll, epoch: 33, loss: 4.479\n",
      "ehelll, epoch: 34, loss: 4.474\n",
      "ehelll, epoch: 35, loss: 4.470\n",
      "ehelll, epoch: 36, loss: 4.466\n",
      "ehelll, epoch: 37, loss: 4.462\n",
      "ehelll, epoch: 38, loss: 4.459\n",
      "ehelll, epoch: 39, loss: 4.456\n",
      "ehelll, epoch: 40, loss: 4.453\n",
      "ehelll, epoch: 41, loss: 4.450\n",
      "ihilll, epoch: 42, loss: 4.448\n",
      "ihilll, epoch: 43, loss: 4.446\n",
      "ihilll, epoch: 44, loss: 4.444\n",
      "ihilll, epoch: 45, loss: 4.442\n",
      "ihilll, epoch: 46, loss: 4.440\n",
      "ihilll, epoch: 47, loss: 4.439\n",
      "ihilll, epoch: 48, loss: 4.437\n",
      "ihilll, epoch: 49, loss: 4.436\n",
      "ihilll, epoch: 50, loss: 4.434\n",
      "ihilll, epoch: 51, loss: 4.433\n",
      "ihilll, epoch: 52, loss: 4.432\n",
      "ihilll, epoch: 53, loss: 4.431\n",
      "ihilll, epoch: 54, loss: 4.430\n",
      "ihilll, epoch: 55, loss: 4.429\n",
      "ihilll, epoch: 56, loss: 4.428\n",
      "ihilll, epoch: 57, loss: 4.427\n",
      "ihilll, epoch: 58, loss: 4.426\n",
      "ihilll, epoch: 59, loss: 4.426\n",
      "ihilll, epoch: 60, loss: 4.425\n",
      "ihilll, epoch: 61, loss: 4.424\n",
      "ihilll, epoch: 62, loss: 4.424\n",
      "ihilll, epoch: 63, loss: 4.423\n",
      "ihilll, epoch: 64, loss: 4.422\n",
      "ihilll, epoch: 65, loss: 4.422\n",
      "ihilll, epoch: 66, loss: 4.421\n",
      "ihilll, epoch: 67, loss: 4.421\n",
      "ihilll, epoch: 68, loss: 4.420\n",
      "ihilll, epoch: 69, loss: 4.420\n",
      "ihilll, epoch: 70, loss: 4.419\n",
      "ihilll, epoch: 71, loss: 4.419\n",
      "ihilll, epoch: 72, loss: 4.418\n",
      "ihilll, epoch: 73, loss: 4.418\n",
      "ihilll, epoch: 74, loss: 4.417\n",
      "ihilll, epoch: 75, loss: 4.417\n",
      "ihilll, epoch: 76, loss: 4.416\n",
      "ehelll, epoch: 77, loss: 4.416\n",
      "ehelll, epoch: 78, loss: 4.415\n",
      "ehelll, epoch: 79, loss: 4.415\n",
      "ehelll, epoch: 80, loss: 4.415\n",
      "ehelll, epoch: 81, loss: 4.414\n",
      "ehelll, epoch: 82, loss: 4.414\n",
      "ehelll, epoch: 83, loss: 4.413\n",
      "ehelll, epoch: 84, loss: 4.413\n",
      "ehelll, epoch: 85, loss: 4.413\n",
      "ehelll, epoch: 86, loss: 4.412\n",
      "ehelll, epoch: 87, loss: 4.412\n",
      "ehelll, epoch: 88, loss: 4.412\n",
      "ehelll, epoch: 89, loss: 4.411\n",
      "ehelll, epoch: 90, loss: 4.411\n",
      "ehelll, epoch: 91, loss: 4.411\n",
      "ehelll, epoch: 92, loss: 4.410\n",
      "ehelll, epoch: 93, loss: 4.410\n",
      "ehelll, epoch: 94, loss: 4.410\n",
      "ehelll, epoch: 95, loss: 4.409\n",
      "ehelll, epoch: 96, loss: 4.409\n",
      "ehelll, epoch: 97, loss: 4.409\n",
      "ehelll, epoch: 98, loss: 4.408\n",
      "ehelll, epoch: 99, loss: 4.408\n",
      "ehelll, epoch: 100, loss: 4.408\n",
      "ehelll, epoch: 101, loss: 4.408\n",
      "ehelll, epoch: 102, loss: 4.407\n",
      "ehelll, epoch: 103, loss: 4.407\n",
      "ehelll, epoch: 104, loss: 4.407\n",
      "ehelll, epoch: 105, loss: 4.407\n",
      "ehelll, epoch: 106, loss: 4.406\n",
      "ehelll, epoch: 107, loss: 4.406\n",
      "ehelll, epoch: 108, loss: 4.406\n",
      "ehelll, epoch: 109, loss: 4.406\n",
      "ehelll, epoch: 110, loss: 4.405\n",
      "ehelll, epoch: 111, loss: 4.405\n",
      "ehelll, epoch: 112, loss: 4.405\n",
      "ehelll, epoch: 113, loss: 4.405\n",
      "ehelll, epoch: 114, loss: 4.404\n",
      "ehelll, epoch: 115, loss: 4.404\n",
      "ehelll, epoch: 116, loss: 4.404\n",
      "ehelll, epoch: 117, loss: 4.404\n",
      "ehelll, epoch: 118, loss: 4.403\n",
      "ehelll, epoch: 119, loss: 4.403\n",
      "ehelll, epoch: 120, loss: 4.403\n",
      "ehelll, epoch: 121, loss: 4.403\n",
      "ehelll, epoch: 122, loss: 4.403\n",
      "ehelll, epoch: 123, loss: 4.402\n",
      "ehelll, epoch: 124, loss: 4.402\n",
      "ehelll, epoch: 125, loss: 4.402\n",
      "ehelll, epoch: 126, loss: 4.402\n",
      "ehelll, epoch: 127, loss: 4.402\n",
      "ehelll, epoch: 128, loss: 4.401\n",
      "ehelll, epoch: 129, loss: 4.401\n",
      "ehelll, epoch: 130, loss: 4.401\n",
      "ehelll, epoch: 131, loss: 4.401\n",
      "ehelll, epoch: 132, loss: 4.401\n",
      "ehelll, epoch: 133, loss: 4.400\n",
      "ehelll, epoch: 134, loss: 4.400\n",
      "ehelll, epoch: 135, loss: 4.400\n",
      "ehelll, epoch: 136, loss: 4.400\n",
      "ehelll, epoch: 137, loss: 4.400\n",
      "ehelll, epoch: 138, loss: 4.400\n",
      "ehelll, epoch: 139, loss: 4.399\n",
      "ehelll, epoch: 140, loss: 4.399\n",
      "ehelll, epoch: 141, loss: 4.399\n",
      "ehelll, epoch: 142, loss: 4.399\n",
      "ehelll, epoch: 143, loss: 4.399\n",
      "ehelll, epoch: 144, loss: 4.399\n",
      "ehelll, epoch: 145, loss: 4.398\n",
      "ehelll, epoch: 146, loss: 4.398\n",
      "ehelll, epoch: 147, loss: 4.398\n",
      "ehelll, epoch: 148, loss: 4.398\n",
      "ehelll, epoch: 149, loss: 4.398\n",
      "ehelll, epoch: 150, loss: 4.398\n",
      "ehelll, epoch: 151, loss: 4.397\n",
      "ehelll, epoch: 152, loss: 4.397\n",
      "ehelll, epoch: 153, loss: 4.397\n",
      "ehelll, epoch: 154, loss: 4.397\n",
      "ehelll, epoch: 155, loss: 4.397\n",
      "ehelll, epoch: 156, loss: 4.397\n",
      "ehelll, epoch: 157, loss: 4.397\n",
      "ehelll, epoch: 158, loss: 4.396\n",
      "ehelll, epoch: 159, loss: 4.396\n",
      "ehelll, epoch: 160, loss: 4.396\n",
      "ehelll, epoch: 161, loss: 4.396\n",
      "ehelll, epoch: 162, loss: 4.396\n",
      "ehelll, epoch: 163, loss: 4.396\n",
      "ehelll, epoch: 164, loss: 4.396\n",
      "ehelll, epoch: 165, loss: 4.396\n",
      "ehelll, epoch: 166, loss: 4.395\n",
      "ehelll, epoch: 167, loss: 4.395\n",
      "ehelll, epoch: 168, loss: 4.395\n",
      "ehelll, epoch: 169, loss: 4.395\n",
      "ehelll, epoch: 170, loss: 4.395\n",
      "ehelll, epoch: 171, loss: 4.395\n",
      "ehelll, epoch: 172, loss: 4.395\n",
      "ehelll, epoch: 173, loss: 4.395\n",
      "ehelll, epoch: 174, loss: 4.394\n",
      "ehelll, epoch: 175, loss: 4.394\n",
      "ehelll, epoch: 176, loss: 4.394\n",
      "ehelll, epoch: 177, loss: 4.394\n",
      "ehelll, epoch: 178, loss: 4.394\n",
      "ehelll, epoch: 179, loss: 4.394\n",
      "ehelll, epoch: 180, loss: 4.394\n",
      "ehelll, epoch: 181, loss: 4.394\n",
      "ehelll, epoch: 182, loss: 4.394\n",
      "ehelll, epoch: 183, loss: 4.393\n",
      "ehelll, epoch: 184, loss: 4.393\n",
      "ehelll, epoch: 185, loss: 4.393\n",
      "ehelll, epoch: 186, loss: 4.393\n",
      "ehelll, epoch: 187, loss: 4.393\n",
      "ehelll, epoch: 188, loss: 4.393\n",
      "ehelll, epoch: 189, loss: 4.393\n",
      "ehelll, epoch: 190, loss: 4.393\n",
      "ehelll, epoch: 191, loss: 4.393\n",
      "ehelll, epoch: 192, loss: 4.393\n",
      "ehelll, epoch: 193, loss: 4.392\n",
      "ehelll, epoch: 194, loss: 4.392\n",
      "ehelll, epoch: 195, loss: 4.392\n",
      "ehelll, epoch: 196, loss: 4.392\n",
      "ehelll, epoch: 197, loss: 4.392\n",
      "ehelll, epoch: 198, loss: 4.392\n",
      "ehelll, epoch: 199, loss: 4.392\n",
      "ehelll, epoch: 200, loss: 4.392\n"
     ]
    }
   ],
   "source": [
    "labels = labels.cuda()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    for i,l in zip(inputs,labels):\n",
    "        i=i.cuda()\n",
    "        l=l.cuda()\n",
    "        \n",
    "        _,out = model(i,hidden)\n",
    "        _,idx = out.max(1)\n",
    "        print(idxchar[idx.data[0]],end=\"\") \n",
    "        loss += criterion(out,l.reshape(1))\n",
    "        \n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch+1,loss.item())) \n",
    "    loss.backward() \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myModel1, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x=x.view(batch_size, seq_size, input_size)\n",
    "        out,hidden = self.rnn(x,hidden)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(num_layer,batch_size,hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.625\n",
      "Predicted string:  ililii\n",
      "epoch: 2, loss: 1.435\n",
      "Predicted string:  ililll\n",
      "epoch: 3, loss: 1.269\n",
      "Predicted string:  ililll\n",
      "epoch: 4, loss: 1.132\n",
      "Predicted string:  ililll\n",
      "epoch: 5, loss: 1.058\n",
      "Predicted string:  ihelll\n",
      "epoch: 6, loss: 0.999\n",
      "Predicted string:  ihelll\n",
      "epoch: 7, loss: 0.942\n",
      "Predicted string:  ihello\n",
      "epoch: 8, loss: 0.890\n",
      "Predicted string:  ihelll\n",
      "epoch: 9, loss: 0.850\n",
      "Predicted string:  ihelll\n",
      "epoch: 10, loss: 0.810\n",
      "Predicted string:  ihelll\n",
      "epoch: 11, loss: 0.780\n",
      "Predicted string:  ihello\n",
      "epoch: 12, loss: 0.761\n",
      "Predicted string:  ihello\n",
      "epoch: 13, loss: 0.736\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.708\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.684\n",
      "Predicted string:  ihello\n",
      "epoch: 16, loss: 0.666\n",
      "Predicted string:  ihello\n",
      "epoch: 17, loss: 0.649\n",
      "Predicted string:  ihello\n",
      "epoch: 18, loss: 0.631\n",
      "Predicted string:  ihello\n",
      "epoch: 19, loss: 0.617\n",
      "Predicted string:  ihello\n",
      "epoch: 20, loss: 0.608\n",
      "Predicted string:  ihello\n",
      "epoch: 21, loss: 0.598\n",
      "Predicted string:  ihello\n",
      "epoch: 22, loss: 0.595\n",
      "Predicted string:  ihello\n",
      "epoch: 23, loss: 0.593\n",
      "Predicted string:  ihello\n",
      "epoch: 24, loss: 0.583\n",
      "Predicted string:  ihello\n",
      "epoch: 25, loss: 0.580\n",
      "Predicted string:  ihello\n",
      "epoch: 26, loss: 0.576\n",
      "Predicted string:  ihello\n",
      "epoch: 27, loss: 0.571\n",
      "Predicted string:  ihello\n",
      "epoch: 28, loss: 0.565\n",
      "Predicted string:  ihello\n",
      "epoch: 29, loss: 0.559\n",
      "Predicted string:  ihello\n",
      "epoch: 30, loss: 0.554\n",
      "Predicted string:  ihello\n",
      "epoch: 31, loss: 0.550\n",
      "Predicted string:  ihello\n",
      "epoch: 32, loss: 0.546\n",
      "Predicted string:  ihello\n",
      "epoch: 33, loss: 0.540\n",
      "Predicted string:  ihello\n",
      "epoch: 34, loss: 0.535\n",
      "Predicted string:  ihello\n",
      "epoch: 35, loss: 0.532\n",
      "Predicted string:  ihello\n",
      "epoch: 36, loss: 0.528\n",
      "Predicted string:  ihello\n",
      "epoch: 37, loss: 0.524\n",
      "Predicted string:  ihello\n",
      "epoch: 38, loss: 0.521\n",
      "Predicted string:  ihello\n",
      "epoch: 39, loss: 0.517\n",
      "Predicted string:  ihello\n",
      "epoch: 40, loss: 0.514\n",
      "Predicted string:  ihello\n",
      "epoch: 41, loss: 0.511\n",
      "Predicted string:  ihello\n",
      "epoch: 42, loss: 0.508\n",
      "Predicted string:  ihello\n",
      "epoch: 43, loss: 0.506\n",
      "Predicted string:  ihello\n",
      "epoch: 44, loss: 0.504\n",
      "Predicted string:  ihello\n",
      "epoch: 45, loss: 0.501\n",
      "Predicted string:  ihello\n",
      "epoch: 46, loss: 0.500\n",
      "Predicted string:  ihello\n",
      "epoch: 47, loss: 0.498\n",
      "Predicted string:  ihello\n",
      "epoch: 48, loss: 0.496\n",
      "Predicted string:  ihello\n",
      "epoch: 49, loss: 0.495\n",
      "Predicted string:  ihello\n",
      "epoch: 50, loss: 0.494\n",
      "Predicted string:  ihello\n",
      "epoch: 51, loss: 0.493\n",
      "Predicted string:  ihello\n",
      "epoch: 52, loss: 0.491\n",
      "Predicted string:  ihello\n",
      "epoch: 53, loss: 0.490\n",
      "Predicted string:  ihello\n",
      "epoch: 54, loss: 0.489\n",
      "Predicted string:  ihello\n",
      "epoch: 55, loss: 0.488\n",
      "Predicted string:  ihello\n",
      "epoch: 56, loss: 0.487\n",
      "Predicted string:  ihello\n",
      "epoch: 57, loss: 0.486\n",
      "Predicted string:  ihello\n",
      "epoch: 58, loss: 0.485\n",
      "Predicted string:  ihello\n",
      "epoch: 59, loss: 0.485\n",
      "Predicted string:  ihello\n",
      "epoch: 60, loss: 0.484\n",
      "Predicted string:  ihello\n",
      "epoch: 61, loss: 0.483\n",
      "Predicted string:  ihello\n",
      "epoch: 62, loss: 0.483\n",
      "Predicted string:  ihello\n",
      "epoch: 63, loss: 0.482\n",
      "Predicted string:  ihello\n",
      "epoch: 64, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 65, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 66, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 67, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 68, loss: 0.479\n",
      "Predicted string:  ihello\n",
      "epoch: 69, loss: 0.479\n",
      "Predicted string:  ihello\n",
      "epoch: 70, loss: 0.478\n",
      "Predicted string:  ihello\n",
      "epoch: 71, loss: 0.478\n",
      "Predicted string:  ihello\n",
      "epoch: 72, loss: 0.477\n",
      "Predicted string:  ihello\n",
      "epoch: 73, loss: 0.477\n",
      "Predicted string:  ihello\n",
      "epoch: 74, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 75, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 76, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 77, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 78, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 79, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 80, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 81, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 82, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 83, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 84, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 85, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 86, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 87, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 88, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 89, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 90, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 91, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 92, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 93, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 94, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 95, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 96, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 97, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 98, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 99, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 100, loss: 0.469\n",
      "Predicted string:  ihello\n"
     ]
    }
   ],
   "source": [
    "model = myModel1().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "inputs= inputs.cuda()\n",
    "labels= labels.cuda()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    out = model(inputs, hidden)\n",
    "    _,idx = out.max(1)\n",
    "    loss = criterion(out, labels)\n",
    "    \n",
    "    result_str = [idxchar[c] for c in idx.squeeze()]\n",
    "    \n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch+1,loss.item())) \n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 3, 3, 4], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
