{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOu/3KJJL7kMh9XwSxPGME0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20BipedalWalker-v2%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "666ffaa5-33ae-48b4-f2fd-36ca50e8fa7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"BipedalWalker-v2\")\n",
        "#env._max_episode_steps = 1600\n",
        "#env.seed(10)\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_512 = 512\n",
        "HIDDEN_256 = 256\n",
        "HIDDEN_128 = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, outputs)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear2.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = F.relu(self.bn256(self.linear2(x)))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.seed = torch.manual_seed(0)\n",
        "\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_512)\n",
        "        self.linear2 = nn.Linear(HIDDEN_512, HIDDEN_256)\n",
        "        self.linear3 = nn.Linear(action_size, HIDDEN_256)\n",
        "        self.head = nn.Linear(HIDDEN_256, 1)\n",
        "\n",
        "        self.bn512 = nn.BatchNorm1d(HIDDEN_512)\n",
        "        self.bn256 = nn.BatchNorm1d(HIDDEN_256)\n",
        "        self.bn128 = nn.BatchNorm1d(HIDDEN_128)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear3.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
        "        self.head.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        #x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.bn512(self.linear(state)))\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        y = self.linear3(action)\n",
        "    \n",
        "        x = F.relu(torch.add(x, y))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "7c1b7ae1-e37e-4ebf-945f-8adff96274bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.00\n",
        "EPS_END = 0.00\n",
        "EPS_DECAY = 2000\n",
        "WEIGHT_DECAY = 0.001\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.0001\n",
        "CRITIC_LR = 0.001\n",
        "MEMORY_SIZE = 1000000\n",
        "EPISODE_SIZE = 1000\n",
        "TAU = 0.001\n",
        "ou_noise_theta = 0.4\n",
        "ou_noise_sigma = 0.2\n",
        "\n",
        "RECORD_INTERVAL = 100\n",
        "\n",
        "seed = random.seed(0)\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = env.action_space.shape[0]\n",
        "n_obvs = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "#actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "#actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "#critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "#critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR, weight_decay=WEIGHT_DECAY)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = np.random.uniform(-1,1,n_actions)\n",
        "    else:\n",
        "        actor.eval()\n",
        "        with torch.no_grad():\n",
        "          selected_action = actor(\n",
        "              state.to(device)\n",
        "           )[0].detach().cpu().numpy()\n",
        "          \n",
        "    _noise = noise.sample()\n",
        "    actor.train()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvU9RgzCYsW",
        "colab_type": "text"
      },
      "source": [
        "##Normalizer for obv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0, 0\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + (GAMMA * next_value * masks)\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.mse_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    \n",
        "    # train actor\n",
        "\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "d7e7ab2d-833d-4d29-9eaf-5440d41f28d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "batch_count = 0\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        \n",
        "        \n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(total_reward)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Total reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward))\n",
        "            #print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    #if i_episode % TARGET_UPDATE == 0:\n",
        "    #   actor_target.load_state_dict(actor.state_dict())\n",
        "    #   critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 118 step , 0.00 Actor Loss, 0.00 Critic Loss,  0.00 Threshold , 0.38 Top reward, -92.60 Total reward\n",
            "1 episode , 117 step , 0.49 Actor Loss, 37.62 Critic Loss,  0.00 Threshold , 0.40 Top reward, -93.83 Total reward\n",
            "2 episode , 47 step , 0.87 Actor Loss, 23.67 Critic Loss,  0.00 Threshold , -0.03 Top reward, -127.90 Total reward\n",
            "3 episode , 1600 step , 0.32 Actor Loss, 3.23 Critic Loss,  0.00 Threshold , 0.21 Top reward, -143.28 Total reward\n",
            "4 episode , 46 step , 0.07 Actor Loss, 0.44 Critic Loss,  0.00 Threshold , -0.09 Top reward, -128.08 Total reward\n",
            "5 episode , 45 step , 0.13 Actor Loss, 2.11 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.51 Total reward\n",
            "6 episode , 45 step , 0.18 Actor Loss, 3.00 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.51 Total reward\n",
            "7 episode , 45 step , 0.45 Actor Loss, 9.12 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.55 Total reward\n",
            "8 episode , 45 step , 0.41 Actor Loss, 6.86 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.52 Total reward\n",
            "9 episode , 45 step , 0.58 Actor Loss, 5.17 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.51 Total reward\n",
            "10 episode , 46 step , 0.53 Actor Loss, 3.14 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.96 Total reward\n",
            "11 episode , 45 step , 0.71 Actor Loss, 2.32 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.50 Total reward\n",
            "12 episode , 45 step , 0.93 Actor Loss, 4.20 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.40 Total reward\n",
            "13 episode , 46 step , 0.78 Actor Loss, 3.08 Critic Loss,  0.00 Threshold , -0.10 Top reward, -128.00 Total reward\n",
            "14 episode , 45 step , 1.11 Actor Loss, 4.58 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.58 Total reward\n",
            "15 episode , 46 step , 0.95 Actor Loss, 3.90 Critic Loss,  0.00 Threshold , -0.10 Top reward, -128.03 Total reward\n",
            "16 episode , 45 step , 1.12 Actor Loss, 2.77 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.37 Total reward\n",
            "17 episode , 45 step , 1.29 Actor Loss, 3.39 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.26 Total reward\n",
            "18 episode , 45 step , 1.45 Actor Loss, 2.59 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.45 Total reward\n",
            "19 episode , 45 step , 1.59 Actor Loss, 3.53 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.41 Total reward\n",
            "20 episode , 45 step , 1.68 Actor Loss, 2.25 Critic Loss,  0.00 Threshold , -0.10 Top reward, -127.48 Total reward\n",
            "21 episode , 47 step , 1.94 Actor Loss, 2.05 Critic Loss,  0.00 Threshold , -0.08 Top reward, -127.89 Total reward\n",
            "22 episode , 48 step , 1.74 Actor Loss, 4.58 Critic Loss,  0.00 Threshold , -0.08 Top reward, -125.37 Total reward\n",
            "23 episode , 1600 step , 2.08 Actor Loss, 4.06 Critic Loss,  0.00 Threshold , 0.01 Top reward, -187.41 Total reward\n",
            "24 episode , 1567 step , 2.17 Actor Loss, 1.98 Critic Loss,  0.00 Threshold , 0.10 Top reward, -255.89 Total reward\n",
            "25 episode , 70 step , 2.36 Actor Loss, 2.37 Critic Loss,  0.00 Threshold , 0.02 Top reward, -114.69 Total reward\n",
            "26 episode , 1600 step , 2.42 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.35 Top reward, -166.30 Total reward\n",
            "27 episode , 147 step , 2.59 Actor Loss, 0.61 Critic Loss,  0.00 Threshold , 0.11 Top reward, -116.22 Total reward\n",
            "28 episode , 1600 step , 2.65 Actor Loss, 1.03 Critic Loss,  0.00 Threshold , 0.15 Top reward, -168.33 Total reward\n",
            "29 episode , 104 step , 2.87 Actor Loss, 0.47 Critic Loss,  0.00 Threshold , 0.09 Top reward, -122.02 Total reward\n",
            "30 episode , 41 step , 3.03 Actor Loss, 0.40 Critic Loss,  0.00 Threshold , 0.19 Top reward, -110.78 Total reward\n",
            "31 episode , 39 step , 3.07 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.24 Top reward, -109.71 Total reward\n",
            "32 episode , 51 step , 2.79 Actor Loss, 2.71 Critic Loss,  0.00 Threshold , 0.16 Top reward, -116.53 Total reward\n",
            "33 episode , 122 step , 2.73 Actor Loss, 2.46 Critic Loss,  0.00 Threshold , 0.21 Top reward, -120.78 Total reward\n",
            "34 episode , 53 step , 2.93 Actor Loss, 2.27 Critic Loss,  0.00 Threshold , 0.18 Top reward, -114.91 Total reward\n",
            "35 episode , 76 step , 2.88 Actor Loss, 3.24 Critic Loss,  0.00 Threshold , 0.01 Top reward, -109.43 Total reward\n",
            "36 episode , 63 step , 2.85 Actor Loss, 1.84 Critic Loss,  0.00 Threshold , 0.00 Top reward, -119.89 Total reward\n",
            "37 episode , 460 step , 2.97 Actor Loss, 1.78 Critic Loss,  0.00 Threshold , 0.22 Top reward, -129.25 Total reward\n",
            "38 episode , 99 step , 2.87 Actor Loss, 1.21 Critic Loss,  0.00 Threshold , -0.03 Top reward, -127.02 Total reward\n",
            "39 episode , 1019 step , 2.98 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.17 Top reward, -208.24 Total reward\n",
            "40 episode , 39 step , 2.77 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , -0.19 Top reward, -117.74 Total reward\n",
            "41 episode , 38 step , 3.28 Actor Loss, 0.96 Critic Loss,  0.00 Threshold , -0.19 Top reward, -116.69 Total reward\n",
            "42 episode , 89 step , 3.23 Actor Loss, 0.93 Critic Loss,  0.00 Threshold , -0.06 Top reward, -129.72 Total reward\n",
            "43 episode , 68 step , 3.08 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.24 Top reward, -117.47 Total reward\n",
            "44 episode , 62 step , 3.21 Actor Loss, 0.94 Critic Loss,  0.00 Threshold , 0.20 Top reward, -114.83 Total reward\n",
            "45 episode , 74 step , 3.37 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.13 Top reward, -121.64 Total reward\n",
            "46 episode , 60 step , 3.40 Actor Loss, 0.80 Critic Loss,  0.00 Threshold , -0.06 Top reward, -114.92 Total reward\n",
            "47 episode , 139 step , 3.28 Actor Loss, 1.02 Critic Loss,  0.00 Threshold , 0.17 Top reward, -141.46 Total reward\n",
            "48 episode , 73 step , 3.58 Actor Loss, 2.59 Critic Loss,  0.00 Threshold , 0.11 Top reward, -129.92 Total reward\n",
            "49 episode , 78 step , 3.72 Actor Loss, 2.76 Critic Loss,  0.00 Threshold , -0.07 Top reward, -134.21 Total reward\n",
            "50 episode , 76 step , 3.45 Actor Loss, 2.28 Critic Loss,  0.00 Threshold , 0.10 Top reward, -128.92 Total reward\n",
            "51 episode , 68 step , 3.63 Actor Loss, 2.50 Critic Loss,  0.00 Threshold , -0.01 Top reward, -131.23 Total reward\n",
            "52 episode , 62 step , 3.59 Actor Loss, 2.56 Critic Loss,  0.00 Threshold , 0.15 Top reward, -129.19 Total reward\n",
            "53 episode , 140 step , 3.85 Actor Loss, 2.52 Critic Loss,  0.00 Threshold , 0.15 Top reward, -138.41 Total reward\n",
            "54 episode , 292 step , 3.86 Actor Loss, 1.62 Critic Loss,  0.00 Threshold , 0.05 Top reward, -143.91 Total reward\n",
            "55 episode , 1600 step , 3.88 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.17 Top reward, -136.98 Total reward\n",
            "56 episode , 1600 step , 3.79 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.28 Top reward, -127.33 Total reward\n",
            "57 episode , 1600 step , 3.70 Actor Loss, 0.73 Critic Loss,  0.00 Threshold , 0.28 Top reward, -114.87 Total reward\n",
            "58 episode , 1600 step , 3.52 Actor Loss, 0.67 Critic Loss,  0.00 Threshold , -0.00 Top reward, -164.16 Total reward\n",
            "59 episode , 458 step , 3.39 Actor Loss, 0.53 Critic Loss,  0.00 Threshold , 0.51 Top reward, -136.79 Total reward\n",
            "60 episode , 217 step , 3.44 Actor Loss, 0.86 Critic Loss,  0.00 Threshold , 0.51 Top reward, -125.89 Total reward\n",
            "61 episode , 219 step , 3.44 Actor Loss, 0.93 Critic Loss,  0.00 Threshold , 0.37 Top reward, -120.34 Total reward\n",
            "62 episode , 84 step , 3.38 Actor Loss, 0.75 Critic Loss,  0.00 Threshold , 0.19 Top reward, -104.39 Total reward\n",
            "63 episode , 105 step , 3.56 Actor Loss, 0.68 Critic Loss,  0.00 Threshold , 0.26 Top reward, -103.14 Total reward\n",
            "64 episode , 205 step , 3.43 Actor Loss, 0.63 Critic Loss,  0.00 Threshold , 0.24 Top reward, -150.18 Total reward\n",
            "65 episode , 76 step , 3.30 Actor Loss, 0.51 Critic Loss,  0.00 Threshold , 0.19 Top reward, -104.68 Total reward\n",
            "66 episode , 133 step , 3.54 Actor Loss, 1.03 Critic Loss,  0.00 Threshold , 0.33 Top reward, -110.89 Total reward\n",
            "67 episode , 74 step , 3.33 Actor Loss, 0.82 Critic Loss,  0.00 Threshold , 0.14 Top reward, -107.20 Total reward\n",
            "68 episode , 97 step , 3.48 Actor Loss, 1.19 Critic Loss,  0.00 Threshold , 0.19 Top reward, -102.25 Total reward\n",
            "69 episode , 361 step , 3.54 Actor Loss, 0.80 Critic Loss,  0.00 Threshold , 0.36 Top reward, -134.00 Total reward\n",
            "70 episode , 87 step , 3.50 Actor Loss, 0.69 Critic Loss,  0.00 Threshold , 0.17 Top reward, -102.51 Total reward\n",
            "71 episode , 75 step , 3.56 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.18 Top reward, -104.02 Total reward\n",
            "72 episode , 114 step , 3.62 Actor Loss, 0.76 Critic Loss,  0.00 Threshold , 0.20 Top reward, -104.08 Total reward\n",
            "73 episode , 78 step , 3.38 Actor Loss, 0.64 Critic Loss,  0.00 Threshold , 0.22 Top reward, -103.12 Total reward\n",
            "74 episode , 119 step , 3.61 Actor Loss, 0.85 Critic Loss,  0.00 Threshold , 0.12 Top reward, -103.14 Total reward\n",
            "75 episode , 116 step , 3.64 Actor Loss, 0.77 Critic Loss,  0.00 Threshold , 0.24 Top reward, -103.78 Total reward\n",
            "76 episode , 122 step , 3.61 Actor Loss, 0.60 Critic Loss,  0.00 Threshold , 0.14 Top reward, -103.83 Total reward\n",
            "77 episode , 81 step , 3.90 Actor Loss, 0.76 Critic Loss,  0.00 Threshold , 0.09 Top reward, -105.22 Total reward\n",
            "78 episode , 220 step , 3.78 Actor Loss, 0.64 Critic Loss,  0.00 Threshold , 0.31 Top reward, -147.33 Total reward\n",
            "79 episode , 79 step , 3.71 Actor Loss, 0.50 Critic Loss,  0.00 Threshold , 0.17 Top reward, -103.64 Total reward\n",
            "80 episode , 58 step , 3.87 Actor Loss, 0.79 Critic Loss,  0.00 Threshold , 0.11 Top reward, -103.89 Total reward\n",
            "81 episode , 51 step , 3.55 Actor Loss, 0.61 Critic Loss,  0.00 Threshold , 0.27 Top reward, -105.88 Total reward\n",
            "82 episode , 48 step , 3.87 Actor Loss, 0.49 Critic Loss,  0.00 Threshold , 0.23 Top reward, -106.60 Total reward\n",
            "83 episode , 215 step , 3.85 Actor Loss, 0.77 Critic Loss,  0.00 Threshold , 0.27 Top reward, -115.99 Total reward\n",
            "84 episode , 76 step , 4.21 Actor Loss, 0.71 Critic Loss,  0.00 Threshold , 0.16 Top reward, -103.73 Total reward\n",
            "85 episode , 89 step , 3.91 Actor Loss, 0.89 Critic Loss,  0.00 Threshold , 0.14 Top reward, -104.22 Total reward\n",
            "86 episode , 264 step , 4.00 Actor Loss, 0.72 Critic Loss,  0.00 Threshold , 0.39 Top reward, -123.16 Total reward\n",
            "87 episode , 54 step , 4.12 Actor Loss, 0.56 Critic Loss,  0.00 Threshold , 0.24 Top reward, -110.21 Total reward\n",
            "88 episode , 255 step , 4.11 Actor Loss, 0.77 Critic Loss,  0.00 Threshold , 0.16 Top reward, -140.62 Total reward\n",
            "89 episode , 55 step , 4.20 Actor Loss, 0.76 Critic Loss,  0.00 Threshold , 0.23 Top reward, -109.36 Total reward\n",
            "90 episode , 55 step , 4.25 Actor Loss, 0.69 Critic Loss,  0.00 Threshold , 0.25 Top reward, -109.02 Total reward\n",
            "91 episode , 83 step , 4.37 Actor Loss, 0.65 Critic Loss,  0.00 Threshold , 0.16 Top reward, -121.73 Total reward\n",
            "92 episode , 56 step , 4.57 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.24 Top reward, -110.63 Total reward\n",
            "93 episode , 50 step , 4.25 Actor Loss, 0.68 Critic Loss,  0.00 Threshold , 0.24 Top reward, -108.06 Total reward\n",
            "94 episode , 57 step , 4.44 Actor Loss, 0.56 Critic Loss,  0.00 Threshold , 0.23 Top reward, -111.46 Total reward\n",
            "95 episode , 67 step , 4.45 Actor Loss, 0.78 Critic Loss,  0.00 Threshold , 0.25 Top reward, -117.49 Total reward\n",
            "96 episode , 60 step , 4.37 Actor Loss, 0.60 Critic Loss,  0.00 Threshold , 0.29 Top reward, -115.55 Total reward\n",
            "97 episode , 49 step , 4.38 Actor Loss, 0.66 Critic Loss,  0.00 Threshold , 0.25 Top reward, -109.15 Total reward\n",
            "98 episode , 299 step , 4.45 Actor Loss, 0.90 Critic Loss,  0.00 Threshold , 0.17 Top reward, -135.58 Total reward\n",
            "99 episode , 51 step , 4.50 Actor Loss, 0.89 Critic Loss,  0.00 Threshold , -0.01 Top reward, -114.44 Total reward\n",
            "100 episode , 91 step , 4.26 Actor Loss, 0.70 Critic Loss,  0.00 Threshold , 0.17 Top reward, -132.16 Total reward\n",
            "101 episode , 55 step , 4.59 Actor Loss, 0.91 Critic Loss,  0.00 Threshold , 0.13 Top reward, -120.82 Total reward\n",
            "102 episode , 60 step , 4.46 Actor Loss, 0.78 Critic Loss,  0.00 Threshold , 0.26 Top reward, -115.78 Total reward\n",
            "103 episode , 48 step , 4.95 Actor Loss, 0.88 Critic Loss,  0.00 Threshold , 0.30 Top reward, -107.69 Total reward\n",
            "104 episode , 47 step , 4.40 Actor Loss, 1.16 Critic Loss,  0.00 Threshold , 0.30 Top reward, -108.40 Total reward\n",
            "105 episode , 46 step , 4.62 Actor Loss, 0.89 Critic Loss,  0.00 Threshold , 0.19 Top reward, -106.33 Total reward\n",
            "106 episode , 44 step , 4.49 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.16 Top reward, -107.58 Total reward\n",
            "107 episode , 44 step , 4.89 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.14 Top reward, -107.91 Total reward\n",
            "108 episode , 45 step , 4.87 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , 0.28 Top reward, -107.62 Total reward\n",
            "109 episode , 58 step , 4.87 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , 0.07 Top reward, -117.75 Total reward\n",
            "110 episode , 72 step , 4.63 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.04 Top reward, -122.02 Total reward\n",
            "111 episode , 78 step , 4.72 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , -0.03 Top reward, -122.35 Total reward\n",
            "112 episode , 69 step , 5.13 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , -0.03 Top reward, -120.21 Total reward\n",
            "113 episode , 65 step , 4.89 Actor Loss, 1.10 Critic Loss,  0.00 Threshold , -0.03 Top reward, -116.21 Total reward\n",
            "114 episode , 57 step , 5.20 Actor Loss, 1.01 Critic Loss,  0.00 Threshold , -0.03 Top reward, -123.80 Total reward\n",
            "115 episode , 73 step , 5.16 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.13 Top reward, -109.85 Total reward\n",
            "116 episode , 83 step , 5.33 Actor Loss, 1.22 Critic Loss,  0.00 Threshold , 0.10 Top reward, -126.12 Total reward\n",
            "117 episode , 98 step , 5.13 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.11 Top reward, -124.65 Total reward\n",
            "118 episode , 71 step , 5.13 Actor Loss, 1.14 Critic Loss,  0.00 Threshold , 0.15 Top reward, -123.26 Total reward\n",
            "119 episode , 51 step , 5.40 Actor Loss, 0.94 Critic Loss,  0.00 Threshold , 0.11 Top reward, -118.59 Total reward\n",
            "120 episode , 70 step , 5.57 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , -0.02 Top reward, -125.13 Total reward\n",
            "121 episode , 54 step , 5.31 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.02 Top reward, -118.85 Total reward\n",
            "122 episode , 81 step , 5.51 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , 0.16 Top reward, -127.98 Total reward\n",
            "123 episode , 63 step , 5.55 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.12 Top reward, -121.26 Total reward\n",
            "124 episode , 64 step , 5.32 Actor Loss, 0.92 Critic Loss,  0.00 Threshold , -0.01 Top reward, -126.70 Total reward\n",
            "125 episode , 89 step , 5.60 Actor Loss, 1.08 Critic Loss,  0.00 Threshold , 0.06 Top reward, -134.94 Total reward\n",
            "126 episode , 205 step , 5.35 Actor Loss, 1.14 Critic Loss,  0.00 Threshold , 0.09 Top reward, -147.80 Total reward\n",
            "127 episode , 72 step , 5.64 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.04 Top reward, -129.75 Total reward\n",
            "128 episode , 435 step , 5.65 Actor Loss, 1.07 Critic Loss,  0.00 Threshold , 0.17 Top reward, -169.16 Total reward\n",
            "129 episode , 89 step , 5.46 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , 0.11 Top reward, -123.75 Total reward\n",
            "130 episode , 112 step , 5.78 Actor Loss, 0.97 Critic Loss,  0.00 Threshold , 0.15 Top reward, -125.75 Total reward\n",
            "131 episode , 93 step , 5.90 Actor Loss, 0.93 Critic Loss,  0.00 Threshold , 0.19 Top reward, -113.65 Total reward\n",
            "132 episode , 1600 step , 5.70 Actor Loss, 0.99 Critic Loss,  0.00 Threshold , 0.21 Top reward, -159.87 Total reward\n",
            "133 episode , 43 step , 5.67 Actor Loss, 0.99 Critic Loss,  0.00 Threshold , -0.12 Top reward, -115.22 Total reward\n",
            "134 episode , 43 step , 5.49 Actor Loss, 0.76 Critic Loss,  0.00 Threshold , -0.12 Top reward, -115.14 Total reward\n",
            "135 episode , 1600 step , 5.68 Actor Loss, 0.96 Critic Loss,  0.00 Threshold , 0.44 Top reward, -175.99 Total reward\n",
            "136 episode , 1600 step , 5.40 Actor Loss, 0.95 Critic Loss,  0.00 Threshold , 0.08 Top reward, -182.43 Total reward\n",
            "137 episode , 1600 step , 5.27 Actor Loss, 1.04 Critic Loss,  0.00 Threshold , 0.12 Top reward, -173.12 Total reward\n",
            "138 episode , 77 step , 5.21 Actor Loss, 0.97 Critic Loss,  0.00 Threshold , 0.17 Top reward, -120.70 Total reward\n",
            "139 episode , 168 step , 5.00 Actor Loss, 0.91 Critic Loss,  0.00 Threshold , 0.13 Top reward, -117.15 Total reward\n",
            "140 episode , 1600 step , 5.23 Actor Loss, 1.09 Critic Loss,  0.00 Threshold , 0.09 Top reward, -181.98 Total reward\n",
            "141 episode , 1600 step , 5.08 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.15 Top reward, -144.00 Total reward\n",
            "142 episode , 46 step , 5.31 Actor Loss, 1.19 Critic Loss,  0.00 Threshold , -0.07 Top reward, -124.20 Total reward\n",
            "143 episode , 1600 step , 4.90 Actor Loss, 1.08 Critic Loss,  0.00 Threshold , 0.24 Top reward, -151.46 Total reward\n",
            "144 episode , 45 step , 4.87 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , -0.08 Top reward, -127.14 Total reward\n",
            "145 episode , 1600 step , 4.80 Actor Loss, 1.07 Critic Loss,  0.00 Threshold , 0.33 Top reward, -176.21 Total reward\n",
            "146 episode , 1600 step , 4.75 Actor Loss, 1.13 Critic Loss,  0.00 Threshold , 0.25 Top reward, -170.24 Total reward\n",
            "147 episode , 1600 step , 4.54 Actor Loss, 1.07 Critic Loss,  0.00 Threshold , 0.26 Top reward, -145.75 Total reward\n",
            "148 episode , 78 step , 4.63 Actor Loss, 0.90 Critic Loss,  0.00 Threshold , 0.27 Top reward, -111.05 Total reward\n",
            "149 episode , 722 step , 4.51 Actor Loss, 1.10 Critic Loss,  0.00 Threshold , 0.22 Top reward, -188.84 Total reward\n",
            "150 episode , 478 step , 4.42 Actor Loss, 1.05 Critic Loss,  0.00 Threshold , 0.23 Top reward, -168.58 Total reward\n",
            "151 episode , 665 step , 4.43 Actor Loss, 1.11 Critic Loss,  0.00 Threshold , 0.16 Top reward, -184.16 Total reward\n",
            "152 episode , 228 step , 4.38 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.08 Top reward, -144.27 Total reward\n",
            "153 episode , 1195 step , 4.42 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.22 Top reward, -243.56 Total reward\n",
            "154 episode , 618 step , 4.26 Actor Loss, 1.72 Critic Loss,  0.00 Threshold , 0.27 Top reward, -183.06 Total reward\n",
            "155 episode , 511 step , 4.35 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.14 Top reward, -174.48 Total reward\n",
            "156 episode , 270 step , 4.14 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.14 Top reward, -149.54 Total reward\n",
            "157 episode , 699 step , 4.23 Actor Loss, 1.70 Critic Loss,  0.00 Threshold , 0.14 Top reward, -192.48 Total reward\n",
            "158 episode , 1600 step , 4.21 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.16 Top reward, -183.96 Total reward\n",
            "159 episode , 1600 step , 4.02 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.03 Top reward, -185.96 Total reward\n",
            "160 episode , 411 step , 3.99 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.24 Top reward, -163.16 Total reward\n",
            "161 episode , 170 step , 4.05 Actor Loss, 1.20 Critic Loss,  0.00 Threshold , 0.38 Top reward, -112.35 Total reward\n",
            "162 episode , 261 step , 3.99 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.09 Top reward, -152.35 Total reward\n",
            "163 episode , 1531 step , 3.98 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , 0.12 Top reward, -272.64 Total reward\n",
            "164 episode , 1600 step , 3.92 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.38 Top reward, -167.06 Total reward\n",
            "165 episode , 328 step , 3.83 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.65 Top reward, -120.90 Total reward\n",
            "166 episode , 1600 step , 3.75 Actor Loss, 1.09 Critic Loss,  0.00 Threshold , -0.09 Top reward, -183.78 Total reward\n",
            "167 episode , 88 step , 3.84 Actor Loss, 1.38 Critic Loss,  0.00 Threshold , 0.20 Top reward, -132.23 Total reward\n",
            "168 episode , 67 step , 3.65 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.29 Top reward, -122.78 Total reward\n",
            "169 episode , 43 step , 4.13 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , -0.03 Top reward, -109.06 Total reward\n",
            "170 episode , 179 step , 3.80 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , 0.03 Top reward, -147.78 Total reward\n",
            "171 episode , 86 step , 3.58 Actor Loss, 1.16 Critic Loss,  0.00 Threshold , 0.64 Top reward, -107.67 Total reward\n",
            "172 episode , 106 step , 3.76 Actor Loss, 1.08 Critic Loss,  0.00 Threshold , 0.04 Top reward, -114.12 Total reward\n",
            "173 episode , 1600 step , 3.80 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.15 Top reward, -152.79 Total reward\n",
            "174 episode , 1600 step , 3.64 Actor Loss, 1.13 Critic Loss,  0.00 Threshold , 0.23 Top reward, -139.32 Total reward\n",
            "175 episode , 191 step , 3.42 Actor Loss, 1.21 Critic Loss,  0.00 Threshold , 0.18 Top reward, -135.75 Total reward\n",
            "176 episode , 64 step , 3.38 Actor Loss, 1.18 Critic Loss,  0.00 Threshold , 0.24 Top reward, -112.82 Total reward\n",
            "177 episode , 1331 step , 3.56 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , 0.37 Top reward, -224.91 Total reward\n",
            "178 episode , 458 step , 3.50 Actor Loss, 1.11 Critic Loss,  0.00 Threshold , 0.37 Top reward, -144.36 Total reward\n",
            "179 episode , 49 step , 3.36 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , -0.02 Top reward, -117.53 Total reward\n",
            "180 episode , 135 step , 3.37 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.12 Top reward, -127.35 Total reward\n",
            "181 episode , 81 step , 3.35 Actor Loss, 0.94 Critic Loss,  0.00 Threshold , -0.02 Top reward, -127.92 Total reward\n",
            "182 episode , 1350 step , 3.55 Actor Loss, 1.10 Critic Loss,  0.00 Threshold , 0.54 Top reward, -234.05 Total reward\n",
            "183 episode , 437 step , 3.45 Actor Loss, 1.09 Critic Loss,  0.00 Threshold , 0.34 Top reward, -129.47 Total reward\n",
            "184 episode , 38 step , 3.58 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , -0.06 Top reward, -118.51 Total reward\n",
            "185 episode , 160 step , 3.53 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , 0.25 Top reward, -129.86 Total reward\n",
            "186 episode , 1600 step , 3.50 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , 0.15 Top reward, -150.61 Total reward\n",
            "187 episode , 1600 step , 3.45 Actor Loss, 1.09 Critic Loss,  0.00 Threshold , 0.33 Top reward, -132.19 Total reward\n",
            "188 episode , 1600 step , 3.28 Actor Loss, 1.02 Critic Loss,  0.00 Threshold , 0.41 Top reward, -122.51 Total reward\n",
            "189 episode , 69 step , 2.96 Actor Loss, 1.06 Critic Loss,  0.00 Threshold , 0.02 Top reward, -109.30 Total reward\n",
            "190 episode , 239 step , 3.31 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.29 Top reward, -141.16 Total reward\n",
            "191 episode , 55 step , 2.89 Actor Loss, 1.17 Critic Loss,  0.00 Threshold , -0.00 Top reward, -111.00 Total reward\n",
            "192 episode , 117 step , 3.32 Actor Loss, 0.94 Critic Loss,  0.00 Threshold , 0.10 Top reward, -113.68 Total reward\n",
            "193 episode , 57 step , 3.24 Actor Loss, 0.99 Critic Loss,  0.00 Threshold , -0.02 Top reward, -108.62 Total reward\n",
            "194 episode , 185 step , 3.31 Actor Loss, 1.02 Critic Loss,  0.00 Threshold , 0.32 Top reward, -106.38 Total reward\n",
            "195 episode , 140 step , 3.42 Actor Loss, 1.12 Critic Loss,  0.00 Threshold , 0.06 Top reward, -136.00 Total reward\n",
            "196 episode , 48 step , 3.56 Actor Loss, 1.56 Critic Loss,  0.00 Threshold , -0.01 Top reward, -111.23 Total reward\n",
            "197 episode , 1600 step , 3.37 Actor Loss, 1.07 Critic Loss,  0.00 Threshold , 0.30 Top reward, -157.22 Total reward\n",
            "198 episode , 44 step , 3.11 Actor Loss, 1.09 Critic Loss,  0.00 Threshold , -0.12 Top reward, -113.73 Total reward\n",
            "199 episode , 46 step , 3.33 Actor Loss, 0.99 Critic Loss,  0.00 Threshold , -0.03 Top reward, -111.52 Total reward\n",
            "200 episode , 45 step , 2.89 Actor Loss, 1.13 Critic Loss,  0.00 Threshold , -0.01 Top reward, -109.83 Total reward\n",
            "201 episode , 57 step , 3.39 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , -0.02 Top reward, -108.41 Total reward\n",
            "202 episode , 1600 step , 3.37 Actor Loss, 1.10 Critic Loss,  0.00 Threshold , 0.34 Top reward, -170.94 Total reward\n",
            "203 episode , 61 step , 3.23 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.01 Top reward, -110.02 Total reward\n",
            "204 episode , 167 step , 3.15 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.16 Top reward, -134.51 Total reward\n",
            "205 episode , 68 step , 3.51 Actor Loss, 1.12 Critic Loss,  0.00 Threshold , 0.07 Top reward, -110.71 Total reward\n",
            "206 episode , 1600 step , 3.33 Actor Loss, 1.17 Critic Loss,  0.00 Threshold , 0.34 Top reward, -123.78 Total reward\n",
            "207 episode , 90 step , 3.49 Actor Loss, 0.96 Critic Loss,  0.00 Threshold , 0.36 Top reward, -102.03 Total reward\n",
            "208 episode , 114 step , 3.43 Actor Loss, 0.92 Critic Loss,  0.00 Threshold , 0.08 Top reward, -115.32 Total reward\n",
            "209 episode , 134 step , 3.22 Actor Loss, 1.00 Critic Loss,  0.00 Threshold , 0.11 Top reward, -128.13 Total reward\n",
            "210 episode , 103 step , 3.71 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.47 Top reward, -105.24 Total reward\n",
            "211 episode , 72 step , 3.39 Actor Loss, 1.05 Critic Loss,  0.00 Threshold , 0.29 Top reward, -103.25 Total reward\n",
            "212 episode , 1406 step , 3.45 Actor Loss, 1.16 Critic Loss,  0.00 Threshold , 0.44 Top reward, -159.93 Total reward\n",
            "213 episode , 60 step , 3.70 Actor Loss, 1.08 Critic Loss,  0.00 Threshold , -0.00 Top reward, -116.23 Total reward\n",
            "214 episode , 1600 step , 3.50 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.28 Top reward, -162.30 Total reward\n",
            "215 episode , 1589 step , 3.46 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.52 Top reward, -187.51 Total reward\n",
            "216 episode , 1600 step , 3.44 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.22 Top reward, -126.87 Total reward\n",
            "217 episode , 558 step , 3.44 Actor Loss, 1.56 Critic Loss,  0.00 Threshold , 0.30 Top reward, -123.29 Total reward\n",
            "218 episode , 180 step , 3.25 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.46 Top reward, -101.51 Total reward\n",
            "219 episode , 194 step , 3.55 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.11 Top reward, -134.68 Total reward\n",
            "220 episode , 201 step , 3.50 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.35 Top reward, -101.82 Total reward\n",
            "221 episode , 201 step , 3.63 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.30 Top reward, -103.49 Total reward\n",
            "222 episode , 301 step , 3.51 Actor Loss, 1.20 Critic Loss,  0.00 Threshold , 0.30 Top reward, -124.59 Total reward\n",
            "223 episode , 181 step , 3.56 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.65 Top reward, -106.99 Total reward\n",
            "224 episode , 193 step , 3.61 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.57 Top reward, -110.53 Total reward\n",
            "225 episode , 252 step , 3.58 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.58 Top reward, -102.81 Total reward\n",
            "226 episode , 311 step , 3.67 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.52 Top reward, -114.78 Total reward\n",
            "227 episode , 179 step , 3.50 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.06 Top reward, -141.42 Total reward\n",
            "228 episode , 194 step , 3.67 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.16 Top reward, -139.19 Total reward\n",
            "229 episode , 249 step , 3.58 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.63 Top reward, -107.69 Total reward\n",
            "230 episode , 477 step , 3.62 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.63 Top reward, -119.43 Total reward\n",
            "231 episode , 120 step , 3.49 Actor Loss, 1.37 Critic Loss,  0.00 Threshold , 0.29 Top reward, -114.99 Total reward\n",
            "232 episode , 270 step , 3.66 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.33 Top reward, -134.84 Total reward\n",
            "233 episode , 687 step , 3.65 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.32 Top reward, -159.71 Total reward\n",
            "234 episode , 263 step , 3.65 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.52 Top reward, -128.51 Total reward\n",
            "235 episode , 536 step , 3.75 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.55 Top reward, -142.63 Total reward\n",
            "236 episode , 1449 step , 3.69 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.44 Top reward, -230.22 Total reward\n",
            "237 episode , 51 step , 3.56 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.06 Top reward, -117.11 Total reward\n",
            "238 episode , 178 step , 3.88 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.48 Top reward, -122.22 Total reward\n",
            "239 episode , 479 step , 3.71 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.56 Top reward, -125.77 Total reward\n",
            "240 episode , 562 step , 3.57 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.57 Top reward, -135.89 Total reward\n",
            "241 episode , 1600 step , 3.66 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.47 Top reward, -134.48 Total reward\n",
            "242 episode , 1417 step , 3.54 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.60 Top reward, -239.68 Total reward\n",
            "243 episode , 221 step , 3.47 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.26 Top reward, -132.31 Total reward\n",
            "244 episode , 564 step , 3.52 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.65 Top reward, -125.47 Total reward\n",
            "245 episode , 225 step , 3.60 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.26 Top reward, -106.02 Total reward\n",
            "246 episode , 176 step , 3.50 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.29 Top reward, -109.04 Total reward\n",
            "247 episode , 454 step , 3.58 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.35 Top reward, -119.82 Total reward\n",
            "248 episode , 1428 step , 3.57 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.63 Top reward, -218.78 Total reward\n",
            "249 episode , 371 step , 3.51 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.51 Top reward, -118.06 Total reward\n",
            "250 episode , 114 step , 3.49 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.15 Top reward, -118.24 Total reward\n",
            "251 episode , 188 step , 3.70 Actor Loss, 1.50 Critic Loss,  0.00 Threshold , 0.21 Top reward, -119.06 Total reward\n",
            "252 episode , 62 step , 3.74 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.10 Top reward, -121.11 Total reward\n",
            "253 episode , 225 step , 3.66 Actor Loss, 1.64 Critic Loss,  0.00 Threshold , 0.30 Top reward, -113.42 Total reward\n",
            "254 episode , 1600 step , 3.63 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.22 Top reward, -124.53 Total reward\n",
            "255 episode , 82 step , 3.77 Actor Loss, 1.61 Critic Loss,  0.00 Threshold , 0.23 Top reward, -126.98 Total reward\n",
            "256 episode , 1600 step , 3.59 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.29 Top reward, -115.31 Total reward\n",
            "257 episode , 1600 step , 3.54 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.34 Top reward, -111.63 Total reward\n",
            "258 episode , 135 step , 3.27 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.14 Top reward, -129.31 Total reward\n",
            "259 episode , 85 step , 3.23 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.26 Top reward, -120.13 Total reward\n",
            "260 episode , 112 step , 3.53 Actor Loss, 1.15 Critic Loss,  0.00 Threshold , 0.11 Top reward, -126.04 Total reward\n",
            "261 episode , 329 step , 3.47 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.37 Top reward, -116.57 Total reward\n",
            "262 episode , 306 step , 3.60 Actor Loss, 1.38 Critic Loss,  0.00 Threshold , 0.30 Top reward, -101.62 Total reward\n",
            "263 episode , 690 step , 3.47 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.43 Top reward, -160.29 Total reward\n",
            "264 episode , 459 step , 3.52 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.30 Top reward, -120.56 Total reward\n",
            "265 episode , 124 step , 3.55 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.05 Top reward, -122.84 Total reward\n",
            "266 episode , 425 step , 3.44 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.39 Top reward, -121.89 Total reward\n",
            "267 episode , 46 step , 3.72 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , -0.15 Top reward, -113.52 Total reward\n",
            "268 episode , 130 step , 3.40 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.14 Top reward, -131.96 Total reward\n",
            "269 episode , 1600 step , 3.47 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.36 Top reward, -109.13 Total reward\n",
            "270 episode , 284 step , 3.53 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.24 Top reward, -97.18 Total reward\n",
            "271 episode , 1600 step , 3.38 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.39 Top reward, -97.78 Total reward\n",
            "272 episode , 172 step , 3.33 Actor Loss, 1.16 Critic Loss,  0.00 Threshold , 0.52 Top reward, -98.94 Total reward\n",
            "273 episode , 122 step , 3.18 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.54 Top reward, -93.93 Total reward\n",
            "274 episode , 116 step , 3.42 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , 0.34 Top reward, -98.40 Total reward\n",
            "275 episode , 138 step , 3.10 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.21 Top reward, -104.22 Total reward\n",
            "276 episode , 154 step , 3.33 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.18 Top reward, -126.34 Total reward\n",
            "277 episode , 566 step , 3.47 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.30 Top reward, -138.34 Total reward\n",
            "278 episode , 215 step , 3.17 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.16 Top reward, -124.17 Total reward\n",
            "279 episode , 442 step , 3.33 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.15 Top reward, -156.19 Total reward\n",
            "280 episode , 132 step , 3.12 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.30 Top reward, -100.23 Total reward\n",
            "281 episode , 1407 step , 3.27 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.23 Top reward, -226.44 Total reward\n",
            "282 episode , 178 step , 3.46 Actor Loss, 1.72 Critic Loss,  0.00 Threshold , 0.20 Top reward, -111.26 Total reward\n",
            "283 episode , 1144 step , 3.26 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.28 Top reward, -213.54 Total reward\n",
            "284 episode , 155 step , 3.54 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.23 Top reward, -134.03 Total reward\n",
            "285 episode , 97 step , 3.43 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.18 Top reward, -110.02 Total reward\n",
            "286 episode , 515 step , 3.31 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.35 Top reward, -177.98 Total reward\n",
            "287 episode , 197 step , 3.22 Actor Loss, 1.63 Critic Loss,  0.00 Threshold , 0.00 Top reward, -147.83 Total reward\n",
            "288 episode , 438 step , 3.45 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.41 Top reward, -136.81 Total reward\n",
            "289 episode , 257 step , 3.30 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.12 Top reward, -143.46 Total reward\n",
            "290 episode , 173 step , 3.47 Actor Loss, 1.99 Critic Loss,  0.00 Threshold , 0.32 Top reward, -114.66 Total reward\n",
            "291 episode , 400 step , 3.30 Actor Loss, 1.62 Critic Loss,  0.00 Threshold , 0.33 Top reward, -153.24 Total reward\n",
            "292 episode , 194 step , 3.23 Actor Loss, 1.95 Critic Loss,  0.00 Threshold , 0.13 Top reward, -115.95 Total reward\n",
            "293 episode , 85 step , 3.19 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.09 Top reward, -109.09 Total reward\n",
            "294 episode , 520 step , 3.38 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.28 Top reward, -165.68 Total reward\n",
            "295 episode , 319 step , 3.27 Actor Loss, 1.50 Critic Loss,  0.00 Threshold , 0.25 Top reward, -149.37 Total reward\n",
            "296 episode , 306 step , 3.17 Actor Loss, 1.68 Critic Loss,  0.00 Threshold , 0.22 Top reward, -147.67 Total reward\n",
            "297 episode , 163 step , 3.32 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.12 Top reward, -115.43 Total reward\n",
            "298 episode , 216 step , 3.33 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.19 Top reward, -125.80 Total reward\n",
            "299 episode , 1340 step , 3.35 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.29 Top reward, -228.61 Total reward\n",
            "300 episode , 584 step , 3.33 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.34 Top reward, -129.64 Total reward\n",
            "301 episode , 591 step , 3.26 Actor Loss, 1.64 Critic Loss,  0.00 Threshold , 0.35 Top reward, -140.64 Total reward\n",
            "302 episode , 653 step , 3.49 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.41 Top reward, -175.17 Total reward\n",
            "303 episode , 737 step , 3.29 Actor Loss, 1.68 Critic Loss,  0.00 Threshold , 0.32 Top reward, -176.19 Total reward\n",
            "304 episode , 1600 step , 3.22 Actor Loss, 1.66 Critic Loss,  0.00 Threshold , 0.29 Top reward, -132.02 Total reward\n",
            "305 episode , 1600 step , 3.15 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.31 Top reward, -109.87 Total reward\n",
            "306 episode , 85 step , 3.41 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.11 Top reward, -119.96 Total reward\n",
            "307 episode , 1600 step , 3.05 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.52 Top reward, -89.78 Total reward\n",
            "308 episode , 1600 step , 2.92 Actor Loss, 1.62 Critic Loss,  0.00 Threshold , 0.32 Top reward, -119.07 Total reward\n",
            "309 episode , 1600 step , 2.76 Actor Loss, 1.72 Critic Loss,  0.00 Threshold , 0.30 Top reward, -130.30 Total reward\n",
            "310 episode , 1600 step , 2.69 Actor Loss, 1.61 Critic Loss,  0.00 Threshold , 0.49 Top reward, -112.31 Total reward\n",
            "311 episode , 1600 step , 2.60 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.26 Top reward, -142.40 Total reward\n",
            "312 episode , 75 step , 2.38 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.03 Top reward, -122.48 Total reward\n",
            "313 episode , 1600 step , 2.48 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.15 Top reward, -153.32 Total reward\n",
            "314 episode , 1600 step , 2.39 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.29 Top reward, -121.63 Total reward\n",
            "315 episode , 1600 step , 2.20 Actor Loss, 1.59 Critic Loss,  0.00 Threshold , 0.47 Top reward, -72.39 Total reward\n",
            "316 episode , 1600 step , 2.12 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.48 Top reward, -67.25 Total reward\n",
            "317 episode , 1600 step , 2.04 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.42 Top reward, -16.35 Total reward\n",
            "318 episode , 1600 step , 1.93 Actor Loss, 1.56 Critic Loss,  0.00 Threshold , 0.49 Top reward, -47.89 Total reward\n",
            "319 episode , 519 step , 1.85 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.54 Top reward, -120.98 Total reward\n",
            "320 episode , 334 step , 1.82 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.24 Top reward, -126.77 Total reward\n",
            "321 episode , 139 step , 1.99 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.30 Top reward, -106.99 Total reward\n",
            "322 episode , 887 step , 1.77 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.48 Top reward, -187.86 Total reward\n",
            "323 episode , 551 step , 1.81 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.49 Top reward, -167.32 Total reward\n",
            "324 episode , 201 step , 1.77 Actor Loss, 1.85 Critic Loss,  0.00 Threshold , 0.46 Top reward, -126.86 Total reward\n",
            "325 episode , 747 step , 1.75 Actor Loss, 1.62 Critic Loss,  0.00 Threshold , 0.41 Top reward, -157.43 Total reward\n",
            "326 episode , 1600 step , 1.74 Actor Loss, 1.62 Critic Loss,  0.00 Threshold , 0.53 Top reward, -85.98 Total reward\n",
            "327 episode , 1205 step , 1.60 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.42 Top reward, -169.84 Total reward\n",
            "328 episode , 1600 step , 1.56 Actor Loss, 1.68 Critic Loss,  0.00 Threshold , 0.53 Top reward, -59.64 Total reward\n",
            "329 episode , 491 step , 1.46 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.38 Top reward, -165.49 Total reward\n",
            "330 episode , 237 step , 1.25 Actor Loss, 1.69 Critic Loss,  0.00 Threshold , 0.08 Top reward, -121.97 Total reward\n",
            "331 episode , 1600 step , 1.42 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.50 Top reward, -109.60 Total reward\n",
            "332 episode , 106 step , 1.13 Actor Loss, 1.69 Critic Loss,  0.00 Threshold , 0.16 Top reward, -111.96 Total reward\n",
            "333 episode , 1600 step , 1.28 Actor Loss, 1.71 Critic Loss,  0.00 Threshold , 0.49 Top reward, -52.28 Total reward\n",
            "334 episode , 725 step , 1.22 Actor Loss, 1.65 Critic Loss,  0.00 Threshold , 0.40 Top reward, -171.16 Total reward\n",
            "335 episode , 181 step , 1.37 Actor Loss, 1.38 Critic Loss,  0.00 Threshold , 0.15 Top reward, -115.44 Total reward\n",
            "336 episode , 1600 step , 1.19 Actor Loss, 1.70 Critic Loss,  0.00 Threshold , 0.54 Top reward, -9.09 Total reward\n",
            "337 episode , 1171 step , 1.01 Actor Loss, 1.64 Critic Loss,  0.00 Threshold , 0.46 Top reward, -127.07 Total reward\n",
            "338 episode , 1600 step , 1.01 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.49 Top reward, -16.70 Total reward\n",
            "339 episode , 130 step , 1.11 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.12 Top reward, -111.47 Total reward\n",
            "340 episode , 104 step , 1.15 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.04 Top reward, -111.98 Total reward\n",
            "341 episode , 104 step , 1.26 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.17 Top reward, -112.47 Total reward\n",
            "342 episode , 464 step , 0.95 Actor Loss, 1.72 Critic Loss,  0.00 Threshold , 0.41 Top reward, -105.79 Total reward\n",
            "343 episode , 1600 step , 0.99 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.43 Top reward, 31.28 Total reward\n",
            "344 episode , 1600 step , 0.95 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.35 Top reward, -12.81 Total reward\n",
            "345 episode , 1431 step , 0.81 Actor Loss, 1.61 Critic Loss,  0.00 Threshold , 0.52 Top reward, -69.93 Total reward\n",
            "346 episode , 1600 step , 0.69 Actor Loss, 1.68 Critic Loss,  0.00 Threshold , 0.34 Top reward, 27.98 Total reward\n",
            "347 episode , 535 step , 0.81 Actor Loss, 1.77 Critic Loss,  0.00 Threshold , 0.35 Top reward, -127.35 Total reward\n",
            "348 episode , 1600 step , 0.64 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.47 Top reward, 8.59 Total reward\n",
            "349 episode , 1600 step , 0.60 Actor Loss, 1.69 Critic Loss,  0.00 Threshold , 0.47 Top reward, -53.45 Total reward\n",
            "350 episode , 1600 step , 0.46 Actor Loss, 1.64 Critic Loss,  0.00 Threshold , 0.54 Top reward, 25.24 Total reward\n",
            "351 episode , 603 step , 0.35 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.34 Top reward, -132.26 Total reward\n",
            "352 episode , 1340 step , 0.49 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.43 Top reward, -155.24 Total reward\n",
            "353 episode , 169 step , 0.42 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.41 Top reward, -103.79 Total reward\n",
            "354 episode , 1600 step , 0.41 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.43 Top reward, -67.38 Total reward\n",
            "355 episode , 1600 step , 0.32 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.45 Top reward, 5.91 Total reward\n",
            "356 episode , 614 step , 0.26 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.38 Top reward, -97.40 Total reward\n",
            "357 episode , 172 step , 0.37 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.42 Top reward, -105.13 Total reward\n",
            "358 episode , 1600 step , 0.28 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.48 Top reward, 1.55 Total reward\n",
            "359 episode , 1600 step , 0.13 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.52 Top reward, 43.77 Total reward\n",
            "360 episode , 1600 step , 0.17 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.45 Top reward, -117.32 Total reward\n",
            "361 episode , 1600 step , 0.05 Actor Loss, 1.63 Critic Loss,  0.00 Threshold , 0.42 Top reward, 7.67 Total reward\n",
            "362 episode , 1600 step , 0.02 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.48 Top reward, 14.88 Total reward\n",
            "363 episode , 1087 step , -0.08 Actor Loss, 1.66 Critic Loss,  0.00 Threshold , 0.42 Top reward, -44.39 Total reward\n",
            "364 episode , 1180 step , -0.01 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.42 Top reward, -76.14 Total reward\n",
            "365 episode , 1600 step , -0.08 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.44 Top reward, 83.45 Total reward\n",
            "366 episode , 1366 step , -0.15 Actor Loss, 1.63 Critic Loss,  0.00 Threshold , 0.47 Top reward, 0.20 Total reward\n",
            "367 episode , 428 step , -0.06 Actor Loss, 1.65 Critic Loss,  0.00 Threshold , 0.40 Top reward, -87.02 Total reward\n",
            "368 episode , 1600 step , -0.23 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.47 Top reward, 91.93 Total reward\n",
            "369 episode , 1600 step , -0.27 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.53 Top reward, 83.46 Total reward\n",
            "370 episode , 1584 step , -0.29 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.55 Top reward, -47.93 Total reward\n",
            "371 episode , 101 step , -0.29 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.14 Top reward, -105.35 Total reward\n",
            "372 episode , 1600 step , -0.35 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.57 Top reward, 77.67 Total reward\n",
            "373 episode , 169 step , -0.15 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.15 Top reward, -111.07 Total reward\n",
            "374 episode , 1600 step , -0.37 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.47 Top reward, 17.83 Total reward\n",
            "375 episode , 136 step , -0.34 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.29 Top reward, -99.71 Total reward\n",
            "376 episode , 1177 step , -0.38 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.54 Top reward, -76.97 Total reward\n",
            "377 episode , 882 step , -0.34 Actor Loss, 1.63 Critic Loss,  0.00 Threshold , 0.39 Top reward, -132.67 Total reward\n",
            "378 episode , 323 step , -0.59 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.32 Top reward, -88.04 Total reward\n",
            "379 episode , 658 step , -0.45 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.36 Top reward, -149.07 Total reward\n",
            "380 episode , 202 step , -0.53 Actor Loss, 1.65 Critic Loss,  0.00 Threshold , 0.30 Top reward, -105.44 Total reward\n",
            "381 episode , 378 step , -0.40 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.29 Top reward, -133.06 Total reward\n",
            "382 episode , 138 step , -0.43 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.07 Top reward, -111.69 Total reward\n",
            "383 episode , 137 step , -0.40 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.34 Top reward, -101.92 Total reward\n",
            "384 episode , 251 step , -0.45 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.40 Top reward, -97.84 Total reward\n",
            "385 episode , 272 step , -0.54 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.36 Top reward, -105.08 Total reward\n",
            "386 episode , 919 step , -0.42 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.33 Top reward, -147.70 Total reward\n",
            "387 episode , 124 step , -0.29 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.36 Top reward, -101.71 Total reward\n",
            "388 episode , 756 step , -0.45 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.44 Top reward, -60.94 Total reward\n",
            "389 episode , 1600 step , -0.41 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.46 Top reward, 105.70 Total reward\n",
            "390 episode , 644 step , -0.53 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.34 Top reward, -123.58 Total reward\n",
            "391 episode , 1427 step , -0.58 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.66 Top reward, -51.59 Total reward\n",
            "392 episode , 568 step , -0.44 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.43 Top reward, -84.00 Total reward\n",
            "393 episode , 435 step , -0.58 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.47 Top reward, -101.61 Total reward\n",
            "394 episode , 1215 step , -0.45 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.48 Top reward, -101.63 Total reward\n",
            "395 episode , 409 step , -0.60 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.34 Top reward, -89.53 Total reward\n",
            "396 episode , 383 step , -0.60 Actor Loss, 1.57 Critic Loss,  0.00 Threshold , 0.36 Top reward, -93.16 Total reward\n",
            "397 episode , 1600 step , -0.66 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.55 Top reward, 110.42 Total reward\n",
            "398 episode , 1600 step , -0.65 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.52 Top reward, 131.83 Total reward\n",
            "399 episode , 1068 step , -0.70 Actor Loss, 1.56 Critic Loss,  0.00 Threshold , 0.42 Top reward, -71.78 Total reward\n",
            "400 episode , 824 step , -0.81 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.55 Top reward, -51.30 Total reward\n",
            "401 episode , 1020 step , -0.73 Actor Loss, 1.50 Critic Loss,  0.00 Threshold , 0.56 Top reward, -89.46 Total reward\n",
            "402 episode , 100 step , -0.70 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.39 Top reward, -103.90 Total reward\n",
            "403 episode , 78 step , -0.65 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.09 Top reward, -111.04 Total reward\n",
            "404 episode , 1600 step , -0.70 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.55 Top reward, 94.90 Total reward\n",
            "405 episode , 750 step , -0.85 Actor Loss, 1.60 Critic Loss,  0.00 Threshold , 0.30 Top reward, -170.67 Total reward\n",
            "406 episode , 1600 step , -0.88 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.55 Top reward, 88.30 Total reward\n",
            "407 episode , 1600 step , -0.93 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.47 Top reward, 94.94 Total reward\n",
            "408 episode , 1600 step , -0.98 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.47 Top reward, 113.75 Total reward\n",
            "409 episode , 1600 step , -1.16 Actor Loss, 1.61 Critic Loss,  0.00 Threshold , 0.56 Top reward, 69.79 Total reward\n",
            "410 episode , 1099 step , -1.18 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.49 Top reward, -115.74 Total reward\n",
            "411 episode , 649 step , -1.29 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.40 Top reward, -90.62 Total reward\n",
            "412 episode , 182 step , -1.12 Actor Loss, 1.76 Critic Loss,  0.00 Threshold , 0.26 Top reward, -131.01 Total reward\n",
            "413 episode , 484 step , -1.36 Actor Loss, 1.63 Critic Loss,  0.00 Threshold , 0.40 Top reward, -88.45 Total reward\n",
            "414 episode , 1015 step , -1.27 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.47 Top reward, -106.68 Total reward\n",
            "415 episode , 1078 step , -1.29 Actor Loss, 1.58 Critic Loss,  0.00 Threshold , 0.49 Top reward, -75.27 Total reward\n",
            "416 episode , 1600 step , -1.34 Actor Loss, 1.56 Critic Loss,  0.00 Threshold , 0.49 Top reward, 51.26 Total reward\n",
            "417 episode , 1600 step , -1.47 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.50 Top reward, 4.41 Total reward\n",
            "418 episode , 1599 step , -1.51 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.44 Top reward, -38.52 Total reward\n",
            "419 episode , 1026 step , -1.52 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.44 Top reward, -84.32 Total reward\n",
            "420 episode , 1600 step , -1.60 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.39 Top reward, 90.96 Total reward\n",
            "421 episode , 1600 step , -1.74 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.35 Top reward, 85.68 Total reward\n",
            "422 episode , 1600 step , -1.82 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.46 Top reward, 54.34 Total reward\n",
            "423 episode , 1600 step , -1.93 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.52 Top reward, 73.01 Total reward\n",
            "424 episode , 1600 step , -2.01 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.63 Top reward, 81.24 Total reward\n",
            "425 episode , 1600 step , -2.12 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.55 Top reward, 36.58 Total reward\n",
            "426 episode , 1600 step , -2.16 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.54 Top reward, 23.40 Total reward\n",
            "427 episode , 1600 step , -2.35 Actor Loss, 1.45 Critic Loss,  0.00 Threshold , 0.52 Top reward, 21.57 Total reward\n",
            "428 episode , 1600 step , -2.38 Actor Loss, 1.55 Critic Loss,  0.00 Threshold , 0.54 Top reward, 24.36 Total reward\n",
            "429 episode , 1600 step , -2.53 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.64 Top reward, 49.35 Total reward\n",
            "430 episode , 1600 step , -2.64 Actor Loss, 1.54 Critic Loss,  0.00 Threshold , 0.58 Top reward, -30.75 Total reward\n",
            "431 episode , 725 step , -2.60 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.55 Top reward, -107.70 Total reward\n",
            "432 episode , 1600 step , -2.66 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.59 Top reward, 23.73 Total reward\n",
            "433 episode , 1600 step , -2.73 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.66 Top reward, 43.80 Total reward\n",
            "434 episode , 1600 step , -2.80 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.55 Top reward, -27.15 Total reward\n",
            "435 episode , 1208 step , -2.89 Actor Loss, 1.42 Critic Loss,  0.00 Threshold , 0.62 Top reward, -99.35 Total reward\n",
            "436 episode , 304 step , -2.75 Actor Loss, 1.25 Critic Loss,  0.00 Threshold , 0.46 Top reward, -93.50 Total reward\n",
            "437 episode , 613 step , -2.83 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.59 Top reward, -71.81 Total reward\n",
            "438 episode , 1600 step , -2.94 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.60 Top reward, 35.62 Total reward\n",
            "439 episode , 1600 step , -2.96 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.60 Top reward, 62.72 Total reward\n",
            "440 episode , 1600 step , -3.09 Actor Loss, 1.49 Critic Loss,  0.00 Threshold , 0.61 Top reward, 71.28 Total reward\n",
            "441 episode , 1324 step , -3.03 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.52 Top reward, -31.82 Total reward\n",
            "442 episode , 1600 step , -3.13 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.54 Top reward, 96.13 Total reward\n",
            "443 episode , 1600 step , -3.18 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.63 Top reward, 92.40 Total reward\n",
            "444 episode , 1376 step , -3.24 Actor Loss, 1.43 Critic Loss,  0.00 Threshold , 0.51 Top reward, 21.37 Total reward\n",
            "445 episode , 112 step , -3.04 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.37 Top reward, -99.18 Total reward\n",
            "446 episode , 245 step , -3.24 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.31 Top reward, -85.69 Total reward\n",
            "447 episode , 1600 step , -3.36 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.46 Top reward, 139.51 Total reward\n",
            "448 episode , 1600 step , -3.40 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.58 Top reward, 126.15 Total reward\n",
            "449 episode , 772 step , -3.41 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.58 Top reward, -71.18 Total reward\n",
            "450 episode , 791 step , -3.36 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.48 Top reward, -83.19 Total reward\n",
            "451 episode , 1600 step , -3.46 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.48 Top reward, 172.32 Total reward\n",
            "452 episode , 1600 step , -3.57 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.47 Top reward, 137.46 Total reward\n",
            "453 episode , 1600 step , -3.60 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.46 Top reward, 168.75 Total reward\n",
            "454 episode , 1600 step , -3.63 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.46 Top reward, 127.30 Total reward\n",
            "455 episode , 1600 step , -3.68 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.50 Top reward, 179.69 Total reward\n",
            "456 episode , 1600 step , -3.77 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.44 Top reward, 159.80 Total reward\n",
            "457 episode , 1600 step , -3.81 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.48 Top reward, 182.72 Total reward\n",
            "458 episode , 1600 step , -3.91 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.42 Top reward, 171.51 Total reward\n",
            "459 episode , 199 step , -3.94 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.48 Top reward, -92.51 Total reward\n",
            "460 episode , 409 step , -4.01 Actor Loss, 1.37 Critic Loss,  0.00 Threshold , 0.41 Top reward, -67.61 Total reward\n",
            "461 episode , 1128 step , -3.95 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.48 Top reward, 7.81 Total reward\n",
            "462 episode , 212 step , -4.05 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.38 Top reward, -78.79 Total reward\n",
            "463 episode , 1600 step , -3.99 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.46 Top reward, 178.39 Total reward\n",
            "464 episode , 1600 step , -4.08 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.45 Top reward, 158.26 Total reward\n",
            "465 episode , 1237 step , -4.24 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.43 Top reward, 24.59 Total reward\n",
            "466 episode , 535 step , -4.14 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.37 Top reward, -66.16 Total reward\n",
            "467 episode , 795 step , -4.17 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.50 Top reward, -29.00 Total reward\n",
            "468 episode , 366 step , -4.19 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.38 Top reward, -74.03 Total reward\n",
            "469 episode , 1600 step , -4.20 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.48 Top reward, 129.04 Total reward\n",
            "470 episode , 266 step , -4.39 Actor Loss, 1.47 Critic Loss,  0.00 Threshold , 0.37 Top reward, -82.77 Total reward\n",
            "471 episode , 1308 step , -4.42 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.41 Top reward, 8.20 Total reward\n",
            "472 episode , 1600 step , -4.44 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.38 Top reward, -54.96 Total reward\n",
            "473 episode , 582 step , -4.59 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.43 Top reward, -73.60 Total reward\n",
            "474 episode , 1600 step , -4.50 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.48 Top reward, 59.66 Total reward\n",
            "475 episode , 1600 step , -4.60 Actor Loss, 1.40 Critic Loss,  0.00 Threshold , 0.50 Top reward, 106.23 Total reward\n",
            "476 episode , 1600 step , -4.70 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.43 Top reward, 19.65 Total reward\n",
            "477 episode , 1600 step , -4.79 Actor Loss, 1.44 Critic Loss,  0.00 Threshold , 0.47 Top reward, -80.21 Total reward\n",
            "478 episode , 1600 step , -4.92 Actor Loss, 1.51 Critic Loss,  0.00 Threshold , 0.57 Top reward, 121.51 Total reward\n",
            "479 episode , 1600 step , -5.07 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.47 Top reward, 37.58 Total reward\n",
            "480 episode , 1600 step , -5.13 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.58 Top reward, 80.76 Total reward\n",
            "481 episode , 1600 step , -5.14 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.60 Top reward, 121.56 Total reward\n",
            "482 episode , 1600 step , -5.24 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.65 Top reward, 129.92 Total reward\n",
            "483 episode , 1600 step , -5.40 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.73 Top reward, 82.27 Total reward\n",
            "484 episode , 1600 step , -5.41 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.62 Top reward, 150.33 Total reward\n",
            "485 episode , 1600 step , -5.45 Actor Loss, 1.36 Critic Loss,  0.00 Threshold , 0.61 Top reward, 139.21 Total reward\n",
            "486 episode , 1600 step , -5.54 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.57 Top reward, 169.72 Total reward\n",
            "487 episode , 1600 step , -5.67 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.68 Top reward, 152.07 Total reward\n",
            "488 episode , 313 step , -5.88 Actor Loss, 1.20 Critic Loss,  0.00 Threshold , 0.47 Top reward, -102.88 Total reward\n",
            "489 episode , 1600 step , -5.68 Actor Loss, 1.38 Critic Loss,  0.00 Threshold , 0.51 Top reward, 127.85 Total reward\n",
            "490 episode , 777 step , -5.86 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.54 Top reward, -70.14 Total reward\n",
            "491 episode , 1600 step , -5.79 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.53 Top reward, 158.94 Total reward\n",
            "492 episode , 1600 step , -5.80 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.55 Top reward, 126.73 Total reward\n",
            "493 episode , 220 step , -5.95 Actor Loss, 1.23 Critic Loss,  0.00 Threshold , 0.38 Top reward, -100.51 Total reward\n",
            "494 episode , 1166 step , -5.94 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.55 Top reward, -29.38 Total reward\n",
            "495 episode , 616 step , -5.87 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.59 Top reward, -54.22 Total reward\n",
            "496 episode , 1600 step , -5.98 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.49 Top reward, 140.41 Total reward\n",
            "497 episode , 581 step , -6.09 Actor Loss, 1.20 Critic Loss,  0.00 Threshold , 0.43 Top reward, -99.79 Total reward\n",
            "498 episode , 188 step , -6.02 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.45 Top reward, -91.22 Total reward\n",
            "499 episode , 242 step , -6.25 Actor Loss, 1.52 Critic Loss,  0.00 Threshold , 0.32 Top reward, -96.95 Total reward\n",
            "500 episode , 1010 step , -6.05 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.50 Top reward, -48.72 Total reward\n",
            "501 episode , 189 step , -6.06 Actor Loss, 1.65 Critic Loss,  0.00 Threshold , 0.36 Top reward, -94.06 Total reward\n",
            "502 episode , 487 step , -6.08 Actor Loss, 1.37 Critic Loss,  0.00 Threshold , 0.37 Top reward, -46.08 Total reward\n",
            "503 episode , 547 step , -6.05 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.39 Top reward, -58.81 Total reward\n",
            "504 episode , 1273 step , -6.05 Actor Loss, 1.48 Critic Loss,  0.00 Threshold , 0.54 Top reward, -2.43 Total reward\n",
            "505 episode , 260 step , -6.00 Actor Loss, 1.53 Critic Loss,  0.00 Threshold , 0.70 Top reward, -111.45 Total reward\n",
            "506 episode , 1600 step , -6.09 Actor Loss, 1.34 Critic Loss,  0.00 Threshold , 0.56 Top reward, 137.75 Total reward\n",
            "507 episode , 106 step , -6.22 Actor Loss, 1.21 Critic Loss,  0.00 Threshold , 0.50 Top reward, -104.81 Total reward\n",
            "508 episode , 1600 step , -6.14 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.55 Top reward, 136.84 Total reward\n",
            "509 episode , 1600 step , -6.27 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , 0.63 Top reward, 150.06 Total reward\n",
            "510 episode , 118 step , -6.25 Actor Loss, 1.02 Critic Loss,  0.00 Threshold , 0.57 Top reward, -106.85 Total reward\n",
            "511 episode , 1600 step , -6.32 Actor Loss, 1.31 Critic Loss,  0.00 Threshold , 0.39 Top reward, 139.91 Total reward\n",
            "512 episode , 1600 step , -6.38 Actor Loss, 1.41 Critic Loss,  0.00 Threshold , 0.54 Top reward, 162.13 Total reward\n",
            "513 episode , 614 step , -6.38 Actor Loss, 1.32 Critic Loss,  0.00 Threshold , 0.65 Top reward, -39.64 Total reward\n",
            "514 episode , 1600 step , -6.46 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.62 Top reward, 197.76 Total reward\n",
            "515 episode , 325 step , -6.45 Actor Loss, 1.20 Critic Loss,  0.00 Threshold , 0.50 Top reward, -81.36 Total reward\n",
            "516 episode , 1414 step , -6.49 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.58 Top reward, 70.42 Total reward\n",
            "517 episode , 276 step , -6.54 Actor Loss, 1.13 Critic Loss,  0.00 Threshold , 0.52 Top reward, -61.62 Total reward\n",
            "518 episode , 1350 step , -6.54 Actor Loss, 1.37 Critic Loss,  0.00 Threshold , 0.59 Top reward, 69.70 Total reward\n",
            "519 episode , 1600 step , -6.57 Actor Loss, 1.35 Critic Loss,  0.00 Threshold , 0.61 Top reward, 210.56 Total reward\n",
            "520 episode , 1600 step , -6.63 Actor Loss, 1.39 Critic Loss,  0.00 Threshold , 0.60 Top reward, 190.47 Total reward\n",
            "521 episode , 1600 step , -6.74 Actor Loss, 1.38 Critic Loss,  0.00 Threshold , 0.53 Top reward, 195.04 Total reward\n",
            "522 episode , 1600 step , -6.75 Actor Loss, 1.27 Critic Loss,  0.00 Threshold , 0.65 Top reward, 188.25 Total reward\n",
            "523 episode , 1600 step , -6.86 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.64 Top reward, 120.65 Total reward\n",
            "524 episode , 1600 step , -6.89 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.56 Top reward, 156.75 Total reward\n",
            "525 episode , 1600 step , -6.95 Actor Loss, 1.21 Critic Loss,  0.00 Threshold , 0.61 Top reward, 177.67 Total reward\n",
            "526 episode , 1600 step , -7.04 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.61 Top reward, 161.53 Total reward\n",
            "527 episode , 1600 step , -7.03 Actor Loss, 1.29 Critic Loss,  0.00 Threshold , 0.54 Top reward, 182.39 Total reward\n",
            "528 episode , 1496 step , -7.09 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.54 Top reward, 8.83 Total reward\n",
            "529 episode , 1600 step , -7.16 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.65 Top reward, 134.44 Total reward\n",
            "530 episode , 1600 step , -7.31 Actor Loss, 1.28 Critic Loss,  0.00 Threshold , 0.67 Top reward, 141.46 Total reward\n",
            "531 episode , 1600 step , -7.35 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.60 Top reward, 139.93 Total reward\n",
            "532 episode , 1600 step , -7.39 Actor Loss, 1.25 Critic Loss,  0.00 Threshold , 0.50 Top reward, 173.19 Total reward\n",
            "533 episode , 1600 step , -7.46 Actor Loss, 1.33 Critic Loss,  0.00 Threshold , 0.52 Top reward, 184.81 Total reward\n",
            "534 episode , 1600 step , -7.53 Actor Loss, 1.26 Critic Loss,  0.00 Threshold , 0.45 Top reward, 169.82 Total reward\n",
            "535 episode , 1600 step , -7.57 Actor Loss, 1.30 Critic Loss,  0.00 Threshold , 0.54 Top reward, 151.27 Total reward\n",
            "536 episode , 1600 step , -7.59 Actor Loss, 1.24 Critic Loss,  0.00 Threshold , 0.58 Top reward, 177.36 Total reward\n",
            "537 episode , 1600 step , -7.74 Actor Loss, 1.25 Critic Loss,  0.00 Threshold , 0.48 Top reward, 186.67 Total reward\n",
            "538 episode , 812 step , -7.77 Actor Loss, 1.46 Critic Loss,  0.00 Threshold , 0.39 Top reward, -24.90 Total reward\n",
            "539 episode , 1600 step , -7.79 Actor Loss, 1.19 Critic Loss,  0.00 Threshold , 0.44 Top reward, 166.04 Total reward\n",
            "540 episode , 1600 step , -7.82 Actor Loss, 1.21 Critic Loss,  0.00 Threshold , 0.39 Top reward, 161.72 Total reward\n",
            "541 episode , 1600 step , -7.91 Actor Loss, 1.18 Critic Loss,  0.00 Threshold , 0.57 Top reward, 161.02 Total reward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lg-4834irdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}