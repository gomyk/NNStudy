{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIhZw58FrjIcSsjvbkWcIn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20BipedalWalker-v2%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "7afa3c6d-c8d4-43d8-cc4c-d32b7df03c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"BipedalWalker-v2\")\n",
        "env._max_episode_steps = 500\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 256\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.linear(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size + action_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, 1)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "cbddaaa0-76ce-4742-a827-67d71678f802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 1000\n",
        "TARGET_UPDATE = 5\n",
        "ACTOR_LR = 0.0001\n",
        "CRITIC_LR = 0.0002\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 2000\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "\n",
        "RECORD_INTERVAL = 49\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = 4\n",
        "n_obvs = 24\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = np.random.uniform(-1,1,n_actions)\n",
        "    else:\n",
        "        selected_action = actor(\n",
        "             torch.FloatTensor(state).to(device)\n",
        "         ).detach().cpu().numpy()\n",
        "    _noise = noise.sample()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1 , -1\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + GAMMA * next_value * masks\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.smooth_l1_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    # train actor\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "e26a1b98-39dc-4cbd-cfe4-d756db6ec493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Avg reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward/(t+1)))\n",
        "            print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        actor_target.load_state_dict(actor.state_dict())\n",
        "        critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 98 step , -1.00 Actor Loss, -1.00 Critic Loss,  0.82 Threshold , 0.14 Top reward, -1.07 Avg reward\n",
            "[0, 0, 0]\n",
            "1 episode , 78 step , -0.37 Actor Loss, 0.05 Critic Loss,  0.76 Threshold , 0.46 Top reward, -1.27 Avg reward\n",
            "[0, 0, 0]\n",
            "2 episode , 58 step , 0.02 Actor Loss, 1.02 Critic Loss,  0.71 Threshold , -0.04 Top reward, -1.91 Avg reward\n",
            "[0, 0, 0]\n",
            "3 episode , 71 step , 0.06 Actor Loss, 1.04 Critic Loss,  0.67 Threshold , 0.37 Top reward, -1.41 Avg reward\n",
            "[0, 0, 0]\n",
            "4 episode , 75 step , 0.09 Actor Loss, 1.25 Critic Loss,  0.62 Threshold , 0.32 Top reward, -1.36 Avg reward\n",
            "[0, 0, 0]\n",
            "5 episode , 500 step , 0.11 Actor Loss, 0.84 Critic Loss,  0.38 Threshold , 0.18 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "6 episode , 169 step , 0.14 Actor Loss, 0.49 Critic Loss,  0.32 Threshold , 0.14 Top reward, -0.70 Avg reward\n",
            "[0, 0, 0]\n",
            "7 episode , 220 step , 0.13 Actor Loss, 0.48 Critic Loss,  0.26 Threshold , 0.35 Top reward, -0.44 Avg reward\n",
            "[0, 0, 0]\n",
            "8 episode , 500 step , -0.01 Actor Loss, 0.49 Critic Loss,  0.16 Threshold , 0.32 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "9 episode , 179 step , -0.14 Actor Loss, 0.35 Critic Loss,  0.14 Threshold , 0.36 Top reward, -0.54 Avg reward\n",
            "[0, 0, 0]\n",
            "10 episode , 157 step , -0.21 Actor Loss, 0.28 Critic Loss,  0.12 Threshold , 0.29 Top reward, -0.61 Avg reward\n",
            "[0, 0, 0]\n",
            "11 episode , 135 step , -0.31 Actor Loss, 0.36 Critic Loss,  0.10 Threshold , 0.04 Top reward, -0.87 Avg reward\n",
            "[0, 0, 0]\n",
            "12 episode , 170 step , -0.33 Actor Loss, 0.40 Critic Loss,  0.09 Threshold , 0.40 Top reward, -0.56 Avg reward\n",
            "[0, 0, 0]\n",
            "13 episode , 171 step , -0.39 Actor Loss, 0.46 Critic Loss,  0.08 Threshold , 0.34 Top reward, -0.54 Avg reward\n",
            "[0, 0, 0]\n",
            "14 episode , 53 step , -0.44 Actor Loss, 0.40 Critic Loss,  0.07 Threshold , -0.11 Top reward, -2.12 Avg reward\n",
            "[0, 0, 0]\n",
            "15 episode , 500 step , -0.56 Actor Loss, 0.48 Critic Loss,  0.05 Threshold , 0.20 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "16 episode , 500 step , -0.66 Actor Loss, 0.37 Critic Loss,  0.03 Threshold , 0.07 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "17 episode , 500 step , -0.77 Actor Loss, 0.32 Critic Loss,  0.02 Threshold , 0.10 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "18 episode , 500 step , -0.94 Actor Loss, 0.30 Critic Loss,  0.02 Threshold , 0.17 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "19 episode , 70 step , -1.09 Actor Loss, 0.35 Critic Loss,  0.02 Threshold , 0.47 Top reward, -1.40 Avg reward\n",
            "[0, 0, 0]\n",
            "20 episode , 56 step , -1.12 Actor Loss, 0.29 Critic Loss,  0.02 Threshold , 0.38 Top reward, -1.83 Avg reward\n",
            "[0, 0, 0]\n",
            "21 episode , 65 step , -1.25 Actor Loss, 0.37 Critic Loss,  0.02 Threshold , 0.35 Top reward, -1.50 Avg reward\n",
            "[0, 0, 0]\n",
            "22 episode , 63 step , -1.30 Actor Loss, 0.22 Critic Loss,  0.02 Threshold , 0.44 Top reward, -1.55 Avg reward\n",
            "[0, 0, 0]\n",
            "23 episode , 58 step , -1.35 Actor Loss, 0.40 Critic Loss,  0.02 Threshold , 0.27 Top reward, -1.69 Avg reward\n",
            "[0, 0, 0]\n",
            "24 episode , 62 step , -1.39 Actor Loss, 0.33 Critic Loss,  0.02 Threshold , 0.38 Top reward, -1.58 Avg reward\n",
            "[0, 0, 0]\n",
            "25 episode , 59 step , -1.46 Actor Loss, 0.34 Critic Loss,  0.02 Threshold , 0.46 Top reward, -1.66 Avg reward\n",
            "[0, 0, 0]\n",
            "26 episode , 60 step , -1.48 Actor Loss, 0.42 Critic Loss,  0.02 Threshold , 0.38 Top reward, -1.63 Avg reward\n",
            "[0, 0, 0]\n",
            "27 episode , 62 step , -1.59 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.34 Top reward, -1.59 Avg reward\n",
            "[0, 0, 0]\n",
            "28 episode , 59 step , -1.59 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.38 Top reward, -1.66 Avg reward\n",
            "[0, 0, 0]\n",
            "29 episode , 63 step , -1.70 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.54 Top reward, -1.54 Avg reward\n",
            "[0, 0, 0]\n",
            "30 episode , 68 step , -1.72 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.42 Top reward, -1.42 Avg reward\n",
            "[0, 0, 0]\n",
            "31 episode , 66 step , -1.85 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.42 Top reward, -1.47 Avg reward\n",
            "[0, 0, 0]\n",
            "32 episode , 61 step , -1.89 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.52 Top reward, -1.59 Avg reward\n",
            "[0, 0, 0]\n",
            "33 episode , 71 step , -1.99 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.40 Top reward, -1.38 Avg reward\n",
            "[0, 0, 0]\n",
            "34 episode , 74 step , -2.02 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.34 Top reward, -1.31 Avg reward\n",
            "[0, 0, 0]\n",
            "35 episode , 63 step , -2.04 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.49 Top reward, -1.53 Avg reward\n",
            "[0, 0, 0]\n",
            "36 episode , 64 step , -2.06 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.49 Top reward, -1.50 Avg reward\n",
            "[0, 0, 0]\n",
            "37 episode , 65 step , -2.17 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.45 Top reward, -1.48 Avg reward\n",
            "[0, 0, 0]\n",
            "38 episode , 64 step , -2.23 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.35 Top reward, -1.52 Avg reward\n",
            "[0, 0, 0]\n",
            "39 episode , 58 step , -2.21 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.44 Top reward, -1.70 Avg reward\n",
            "[0, 0, 0]\n",
            "40 episode , 54 step , -2.20 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.53 Top reward, -1.89 Avg reward\n",
            "[0, 0, 0]\n",
            "41 episode , 53 step , -2.37 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.49 Top reward, -1.88 Avg reward\n",
            "[0, 0, 0]\n",
            "42 episode , 52 step , -2.38 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.47 Top reward, -1.92 Avg reward\n",
            "[0, 0, 0]\n",
            "43 episode , 57 step , -2.41 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.57 Top reward, -1.74 Avg reward\n",
            "[0, 0, 0]\n",
            "44 episode , 87 step , -2.41 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.35 Top reward, -1.11 Avg reward\n",
            "[0, 0, 0]\n",
            "45 episode , 132 step , -2.47 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.73 Avg reward\n",
            "[0, 0, 0]\n",
            "46 episode , 74 step , -2.55 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.38 Top reward, -1.32 Avg reward\n",
            "[0, 0, 0]\n",
            "47 episode , 72 step , -2.55 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.31 Top reward, -1.37 Avg reward\n",
            "[0, 0, 0]\n",
            "48 episode , 79 step , -2.59 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.29 Top reward, -1.28 Avg reward\n",
            "[0, 0, 0]\n",
            "49 episode , 91 step , -2.58 Actor Loss, 0.65 Critic Loss,  0.01 Threshold , 0.36 Top reward, -1.06 Avg reward\n",
            "[0, 0, 0]\n",
            "50 episode , 282 step , -2.69 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.42 Top reward, -0.34 Avg reward\n",
            "[0, 0, 0]\n",
            "51 episode , 500 step , -3.13 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.22 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "52 episode , 70 step , -3.44 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , -0.03 Top reward, -1.60 Avg reward\n",
            "[0, 0, 0]\n",
            "53 episode , 53 step , -3.42 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.25 Avg reward\n",
            "[0, 0, 0]\n",
            "54 episode , 52 step , -3.39 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.28 Avg reward\n",
            "[0, 0, 0]\n",
            "55 episode , 55 step , -3.35 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.20 Avg reward\n",
            "[0, 0, 0]\n",
            "56 episode , 56 step , -3.25 Actor Loss, 0.70 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.19 Avg reward\n",
            "[0, 0, 0]\n",
            "57 episode , 55 step , -3.23 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.23 Avg reward\n",
            "[0, 0, 0]\n",
            "58 episode , 53 step , -3.19 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.39 Avg reward\n",
            "[0, 0, 0]\n",
            "59 episode , 55 step , -3.16 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.24 Avg reward\n",
            "[0, 0, 0]\n",
            "60 episode , 51 step , -3.10 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , -0.10 Top reward, -2.39 Avg reward\n",
            "[0, 0, 0]\n",
            "61 episode , 500 step , -3.14 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.05 Top reward, -0.10 Avg reward\n",
            "[0, 0, 0]\n",
            "62 episode , 53 step , -3.24 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , -0.07 Top reward, -2.39 Avg reward\n",
            "[0, 0, 0]\n",
            "63 episode , 333 step , -3.07 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.28 Top reward, -0.37 Avg reward\n",
            "[0, 0, 0]\n",
            "64 episode , 148 step , -3.16 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.25 Top reward, -0.70 Avg reward\n",
            "[0, 0, 0]\n",
            "65 episode , 158 step , -3.20 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.10 Top reward, -0.75 Avg reward\n",
            "[0, 0, 0]\n",
            "66 episode , 109 step , -3.09 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.06 Top reward, -1.14 Avg reward\n",
            "[0, 0, 0]\n",
            "67 episode , 74 step , -3.17 Actor Loss, 0.66 Critic Loss,  0.01 Threshold , -0.02 Top reward, -1.59 Avg reward\n",
            "[0, 0, 0]\n",
            "68 episode , 105 step , -3.11 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.09 Top reward, -1.16 Avg reward\n",
            "[0, 0, 0]\n",
            "69 episode , 62 step , -3.28 Actor Loss, 0.66 Critic Loss,  0.01 Threshold , 0.46 Top reward, -1.66 Avg reward\n",
            "[0, 0, 0]\n",
            "70 episode , 103 step , -3.18 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.03 Top reward, -1.13 Avg reward\n",
            "[0, 0, 0]\n",
            "71 episode , 67 step , -3.03 Actor Loss, 0.73 Critic Loss,  0.01 Threshold , 0.06 Top reward, -1.54 Avg reward\n",
            "[0, 0, 0]\n",
            "72 episode , 71 step , -3.06 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.17 Top reward, -1.43 Avg reward\n",
            "[0, 0, 0]\n",
            "73 episode , 67 step , -2.93 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.00 Top reward, -1.71 Avg reward\n",
            "[0, 0, 0]\n",
            "74 episode , 59 step , -2.99 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , -0.03 Top reward, -1.94 Avg reward\n",
            "[0, 0, 0]\n",
            "75 episode , 70 step , -3.08 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , -0.02 Top reward, -1.67 Avg reward\n",
            "[0, 0, 0]\n",
            "76 episode , 500 step , -3.11 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "77 episode , 500 step , -3.27 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.28 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "78 episode , 53 step , -3.40 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.29 Top reward, -2.09 Avg reward\n",
            "[0, 0, 0]\n",
            "79 episode , 48 step , -3.36 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.25 Top reward, -2.26 Avg reward\n",
            "[0, 0, 0]\n",
            "80 episode , 58 step , -3.22 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.19 Top reward, -1.78 Avg reward\n",
            "[0, 0, 0]\n",
            "81 episode , 62 step , -2.99 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.23 Top reward, -1.72 Avg reward\n",
            "[0, 0, 0]\n",
            "82 episode , 55 step , -3.27 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.23 Top reward, -1.86 Avg reward\n",
            "[0, 0, 0]\n",
            "83 episode , 52 step , -3.25 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.29 Top reward, -2.09 Avg reward\n",
            "[0, 0, 0]\n",
            "84 episode , 54 step , -2.92 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.29 Top reward, -2.00 Avg reward\n",
            "[0, 0, 0]\n",
            "85 episode , 62 step , -3.17 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.23 Top reward, -1.70 Avg reward\n",
            "[0, 0, 0]\n",
            "86 episode , 63 step , -3.05 Actor Loss, 0.64 Critic Loss,  0.01 Threshold , 0.25 Top reward, -1.66 Avg reward\n",
            "[0, 0, 0]\n",
            "87 episode , 63 step , -2.84 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.19 Top reward, -1.65 Avg reward\n",
            "[0, 0, 0]\n",
            "88 episode , 77 step , -2.69 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.51 Top reward, -1.33 Avg reward\n",
            "[0, 0, 0]\n",
            "89 episode , 91 step , -2.93 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.65 Top reward, -1.11 Avg reward\n",
            "[0, 0, 0]\n",
            "90 episode , 170 step , -2.73 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , -0.06 Top reward, -0.82 Avg reward\n",
            "[0, 0, 0]\n",
            "91 episode , 74 step , -2.73 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.62 Top reward, -1.34 Avg reward\n",
            "[0, 0, 0]\n",
            "92 episode , 68 step , -2.66 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.58 Top reward, -1.45 Avg reward\n",
            "[0, 0, 0]\n",
            "93 episode , 76 step , -2.67 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.54 Top reward, -1.31 Avg reward\n",
            "[0, 0, 0]\n",
            "94 episode , 84 step , -2.53 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.46 Top reward, -1.19 Avg reward\n",
            "[0, 0, 0]\n",
            "95 episode , 500 step , -2.80 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.43 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "96 episode , 500 step , -3.08 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.18 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "97 episode , 84 step , -3.25 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.06 Top reward, -1.23 Avg reward\n",
            "[0, 0, 0]\n",
            "98 episode , 60 step , -3.07 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.16 Top reward, -1.73 Avg reward\n",
            "[0, 0, 0]\n",
            "99 episode , 67 step , -2.89 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.11 Top reward, -1.54 Avg reward\n",
            "[0, 0, 0]\n",
            "100 episode , 500 step , -2.90 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.12 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "101 episode , 500 step , -2.68 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.09 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "102 episode , 500 step , -2.73 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.12 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "103 episode , 500 step , -3.07 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "104 episode , 129 step , -3.32 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.21 Top reward, -0.82 Avg reward\n",
            "[0, 0, 0]\n",
            "105 episode , 71 step , -3.40 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.43 Top reward, -1.41 Avg reward\n",
            "[0, 0, 0]\n",
            "106 episode , 68 step , -3.54 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.20 Top reward, -1.50 Avg reward\n",
            "[0, 0, 0]\n",
            "107 episode , 71 step , -3.53 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.36 Top reward, -1.40 Avg reward\n",
            "[0, 0, 0]\n",
            "108 episode , 64 step , -3.73 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.27 Top reward, -1.59 Avg reward\n",
            "[0, 0, 0]\n",
            "109 episode , 52 step , -3.27 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.46 Top reward, -1.92 Avg reward\n",
            "[0, 0, 0]\n",
            "110 episode , 50 step , -3.48 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.43 Top reward, -2.05 Avg reward\n",
            "[0, 0, 0]\n",
            "111 episode , 96 step , -3.48 Actor Loss, 0.62 Critic Loss,  0.01 Threshold , 0.40 Top reward, -1.00 Avg reward\n",
            "[0, 0, 0]\n",
            "112 episode , 85 step , -3.75 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.34 Top reward, -1.21 Avg reward\n",
            "[0, 0, 0]\n",
            "113 episode , 78 step , -3.63 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.43 Top reward, -1.30 Avg reward\n",
            "[0, 0, 0]\n",
            "114 episode , 70 step , -3.69 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.39 Top reward, -1.41 Avg reward\n",
            "[0, 0, 0]\n",
            "115 episode , 100 step , -3.62 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.38 Top reward, -1.01 Avg reward\n",
            "[0, 0, 0]\n",
            "116 episode , 58 step , -3.38 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.33 Top reward, -1.81 Avg reward\n",
            "[0, 0, 0]\n",
            "117 episode , 57 step , -3.57 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.39 Top reward, -1.86 Avg reward\n",
            "[0, 0, 0]\n",
            "118 episode , 68 step , -3.46 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.40 Top reward, -1.59 Avg reward\n",
            "[0, 0, 0]\n",
            "119 episode , 68 step , -3.43 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.36 Top reward, -1.65 Avg reward\n",
            "[0, 0, 0]\n",
            "120 episode , 58 step , -3.37 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.18 Top reward, -1.99 Avg reward\n",
            "[0, 0, 0]\n",
            "121 episode , 500 step , -3.48 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.23 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "122 episode , 286 step , -3.71 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.28 Avg reward\n",
            "[0, 0, 0]\n",
            "123 episode , 129 step , -3.79 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.74 Avg reward\n",
            "[0, 0, 0]\n",
            "124 episode , 181 step , -4.44 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.65 Top reward, -0.47 Avg reward\n",
            "[0, 0, 0]\n",
            "125 episode , 264 step , -5.57 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.53 Top reward, -0.32 Avg reward\n",
            "[0, 0, 0]\n",
            "126 episode , 252 step , -6.41 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.32 Top reward, -0.36 Avg reward\n",
            "[0, 0, 0]\n",
            "127 episode , 119 step , -6.41 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.52 Top reward, -0.79 Avg reward\n",
            "[0, 0, 0]\n",
            "128 episode , 95 step , -6.38 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.45 Top reward, -1.01 Avg reward\n",
            "[0, 0, 0]\n",
            "129 episode , 84 step , -6.35 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.33 Top reward, -1.10 Avg reward\n",
            "[0, 0, 0]\n",
            "130 episode , 88 step , -6.61 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.29 Top reward, -1.07 Avg reward\n",
            "[0, 0, 0]\n",
            "131 episode , 114 step , -6.73 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.04 Top reward, -1.09 Avg reward\n",
            "[0, 0, 0]\n",
            "132 episode , 108 step , -6.75 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , -0.02 Top reward, -1.19 Avg reward\n",
            "[0, 0, 0]\n",
            "133 episode , 112 step , -6.89 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.44 Top reward, -0.94 Avg reward\n",
            "[0, 0, 0]\n",
            "134 episode , 111 step , -6.75 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.34 Top reward, -0.93 Avg reward\n",
            "[0, 0, 0]\n",
            "135 episode , 80 step , -6.70 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.25 Top reward, -1.30 Avg reward\n",
            "[0, 0, 0]\n",
            "136 episode , 135 step , -6.49 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.76 Avg reward\n",
            "[0, 0, 0]\n",
            "137 episode , 107 step , -6.56 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.96 Avg reward\n",
            "[0, 0, 0]\n",
            "138 episode , 202 step , -6.63 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.09 Top reward, -0.71 Avg reward\n",
            "[0, 0, 0]\n",
            "139 episode , 157 step , -6.54 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.19 Top reward, -0.89 Avg reward\n",
            "[0, 0, 0]\n",
            "140 episode , 500 step , -6.74 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.56 Top reward, -0.02 Avg reward\n",
            "[0, 0, 0]\n",
            "141 episode , 161 step , -6.08 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.72 Avg reward\n",
            "[0, 0, 0]\n",
            "142 episode , 116 step , -5.77 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.96 Avg reward\n",
            "[0, 0, 0]\n",
            "143 episode , 189 step , -5.51 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.03 Top reward, -0.63 Avg reward\n",
            "[0, 0, 0]\n",
            "144 episode , 240 step , -5.05 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.00 Top reward, -0.52 Avg reward\n",
            "[0, 0, 0]\n",
            "145 episode , 155 step , -4.54 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.01 Top reward, -0.89 Avg reward\n",
            "[0, 0, 0]\n",
            "146 episode , 194 step , -5.05 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.00 Top reward, -0.69 Avg reward\n",
            "[0, 0, 0]\n",
            "147 episode , 178 step , -5.11 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.01 Top reward, -0.78 Avg reward\n",
            "[0, 0, 0]\n",
            "148 episode , 164 step , -5.29 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.06 Top reward, -0.71 Avg reward\n",
            "[0, 0, 0]\n",
            "149 episode , 163 step , -5.16 Actor Loss, 0.36 Critic Loss,  0.01 Threshold , -0.02 Top reward, -0.82 Avg reward\n",
            "[0, 0, 0]\n",
            "150 episode , 100 step , -5.05 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , -0.00 Top reward, -1.12 Avg reward\n",
            "[0, 0, 0]\n",
            "151 episode , 500 step , -4.73 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.57 Top reward, -0.02 Avg reward\n",
            "[0, 0, 0]\n",
            "152 episode , 118 step , -4.36 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.07 Top reward, -0.94 Avg reward\n",
            "[0, 0, 0]\n",
            "153 episode , 500 step , -3.73 Actor Loss, 0.34 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "154 episode , 500 step , -3.60 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.27 Top reward, -0.08 Avg reward\n",
            "[0, 0, 0]\n",
            "155 episode , 500 step , -3.63 Actor Loss, 0.37 Critic Loss,  0.01 Threshold , 0.55 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "156 episode , 500 step , -2.97 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.21 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "157 episode , 500 step , -2.54 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.12 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "158 episode , 87 step , -1.97 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , -0.01 Top reward, -1.44 Avg reward\n",
            "[0, 0, 0]\n",
            "159 episode , 500 step , -1.79 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.15 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "160 episode , 80 step , -2.53 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.11 Top reward, -1.33 Avg reward\n",
            "[0, 0, 0]\n",
            "161 episode , 82 step , -1.66 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.06 Top reward, -1.35 Avg reward\n",
            "[0, 0, 0]\n",
            "162 episode , 60 step , -1.82 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.04 Top reward, -1.77 Avg reward\n",
            "[0, 0, 0]\n",
            "163 episode , 79 step , -2.49 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.04 Top reward, -1.34 Avg reward\n",
            "[0, 0, 0]\n",
            "164 episode , 72 step , -1.10 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.05 Top reward, -1.51 Avg reward\n",
            "[0, 0, 0]\n",
            "165 episode , 69 step , -0.90 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.03 Top reward, -1.61 Avg reward\n",
            "[0, 0, 0]\n",
            "166 episode , 396 step , -0.98 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.07 Top reward, -0.29 Avg reward\n",
            "[0, 0, 0]\n",
            "167 episode , 101 step , -1.29 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.10 Top reward, -1.00 Avg reward\n",
            "[0, 0, 0]\n",
            "168 episode , 168 step , -1.91 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.45 Top reward, -0.64 Avg reward\n",
            "[0, 0, 0]\n",
            "169 episode , 500 step , -1.67 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.19 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "170 episode , 72 step , -1.98 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.44 Top reward, -1.39 Avg reward\n",
            "[0, 0, 0]\n",
            "171 episode , 72 step , -0.73 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.78 Top reward, -1.37 Avg reward\n",
            "[0, 0, 0]\n",
            "172 episode , 65 step , -0.65 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.79 Top reward, -1.52 Avg reward\n",
            "[0, 0, 0]\n",
            "173 episode , 65 step , -0.88 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.60 Top reward, -1.53 Avg reward\n",
            "[0, 0, 0]\n",
            "174 episode , 66 step , -0.59 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.75 Top reward, -1.50 Avg reward\n",
            "[0, 0, 0]\n",
            "175 episode , 62 step , 0.41 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.73 Top reward, -1.61 Avg reward\n",
            "[0, 0, 0]\n",
            "176 episode , 76 step , 0.93 Actor Loss, 0.65 Critic Loss,  0.01 Threshold , 0.78 Top reward, -1.30 Avg reward\n",
            "[0, 0, 0]\n",
            "177 episode , 106 step , 0.86 Actor Loss, 0.64 Critic Loss,  0.01 Threshold , 0.25 Top reward, -0.97 Avg reward\n",
            "[0, 0, 0]\n",
            "178 episode , 102 step , 1.27 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.38 Top reward, -0.99 Avg reward\n",
            "[0, 0, 0]\n",
            "179 episode , 72 step , 1.46 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.24 Top reward, -1.41 Avg reward\n",
            "[0, 0, 0]\n",
            "180 episode , 85 step , 1.68 Actor Loss, 0.69 Critic Loss,  0.01 Threshold , 0.27 Top reward, -1.20 Avg reward\n",
            "[0, 0, 0]\n",
            "181 episode , 75 step , 3.22 Actor Loss, 0.84 Critic Loss,  0.01 Threshold , 0.23 Top reward, -1.39 Avg reward\n",
            "[0, 0, 0]\n",
            "182 episode , 90 step , 2.17 Actor Loss, 0.67 Critic Loss,  0.01 Threshold , 0.23 Top reward, -1.16 Avg reward\n",
            "[0, 0, 0]\n",
            "183 episode , 500 step , 1.69 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.55 Top reward, -0.02 Avg reward\n",
            "[0, 0, 0]\n",
            "184 episode , 120 step , 0.43 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.24 Top reward, -0.85 Avg reward\n",
            "[0, 0, 0]\n",
            "185 episode , 500 step , 0.38 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.23 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "186 episode , 381 step , -1.09 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.24 Top reward, -0.38 Avg reward\n",
            "[0, 0, 0]\n",
            "187 episode , 300 step , -1.59 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.21 Top reward, -0.46 Avg reward\n",
            "[0, 0, 0]\n",
            "188 episode , 84 step , -1.64 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.02 Top reward, -1.33 Avg reward\n",
            "[0, 0, 0]\n",
            "189 episode , 100 step , -0.01 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.05 Top reward, -1.14 Avg reward\n",
            "[0, 0, 0]\n",
            "190 episode , 88 step , -0.66 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.07 Top reward, -1.26 Avg reward\n",
            "[0, 0, 0]\n",
            "191 episode , 112 step , -0.48 Actor Loss, 0.64 Critic Loss,  0.01 Threshold , 0.14 Top reward, -0.94 Avg reward\n",
            "[0, 0, 0]\n",
            "192 episode , 117 step , -0.49 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.07 Top reward, -0.91 Avg reward\n",
            "[0, 0, 0]\n",
            "193 episode , 107 step , -0.62 Actor Loss, 0.64 Critic Loss,  0.01 Threshold , -0.00 Top reward, -1.03 Avg reward\n",
            "[0, 0, 0]\n",
            "194 episode , 224 step , -0.33 Actor Loss, 0.63 Critic Loss,  0.01 Threshold , 0.09 Top reward, -0.50 Avg reward\n",
            "[0, 0, 0]\n",
            "195 episode , 163 step , -0.71 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.08 Top reward, -0.68 Avg reward\n",
            "[0, 0, 0]\n",
            "196 episode , 98 step , -0.00 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.10 Top reward, -1.12 Avg reward\n",
            "[0, 0, 0]\n",
            "197 episode , 92 step , -0.15 Actor Loss, 0.67 Critic Loss,  0.01 Threshold , -0.01 Top reward, -1.21 Avg reward\n",
            "[0, 0, 0]\n",
            "198 episode , 89 step , -0.13 Actor Loss, 0.69 Critic Loss,  0.01 Threshold , 0.02 Top reward, -1.27 Avg reward\n",
            "[0, 0, 0]\n",
            "199 episode , 139 step , 0.37 Actor Loss, 0.70 Critic Loss,  0.01 Threshold , 0.12 Top reward, -0.81 Avg reward\n",
            "[0, 0, 0]\n",
            "200 episode , 144 step , 0.53 Actor Loss, 0.72 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.77 Avg reward\n",
            "[0, 0, 0]\n",
            "201 episode , 500 step , 0.07 Actor Loss, 0.76 Critic Loss,  0.01 Threshold , 0.47 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "202 episode , 111 step , -1.08 Actor Loss, 0.73 Critic Loss,  0.01 Threshold , 0.47 Top reward, -0.86 Avg reward\n",
            "[0, 0, 0]\n",
            "203 episode , 500 step , -1.10 Actor Loss, 0.73 Critic Loss,  0.01 Threshold , 0.41 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "204 episode , 84 step , -1.13 Actor Loss, 0.66 Critic Loss,  0.01 Threshold , 0.16 Top reward, -1.19 Avg reward\n",
            "[0, 0, 0]\n",
            "205 episode , 90 step , -1.46 Actor Loss, 0.66 Critic Loss,  0.01 Threshold , 0.13 Top reward, -1.14 Avg reward\n",
            "[0, 0, 0]\n",
            "206 episode , 120 step , -0.52 Actor Loss, 0.76 Critic Loss,  0.01 Threshold , 0.01 Top reward, -1.07 Avg reward\n",
            "[0, 0, 0]\n",
            "207 episode , 149 step , -0.39 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.64 Avg reward\n",
            "[0, 0, 0]\n",
            "208 episode , 500 step , -0.02 Actor Loss, 0.74 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "209 episode , 500 step , -0.02 Actor Loss, 0.73 Critic Loss,  0.01 Threshold , 0.41 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "210 episode , 500 step , 0.09 Actor Loss, 0.75 Critic Loss,  0.01 Threshold , 0.42 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "211 episode , 500 step , 0.54 Actor Loss, 0.80 Critic Loss,  0.01 Threshold , 0.44 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "212 episode , 500 step , 0.39 Actor Loss, 0.79 Critic Loss,  0.01 Threshold , 0.36 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "213 episode , 500 step , -1.74 Actor Loss, 0.71 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "214 episode , 500 step , -3.74 Actor Loss, 0.68 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "215 episode , 147 step , -5.43 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.53 Top reward, -0.70 Avg reward\n",
            "[0, 0, 0]\n",
            "216 episode , 500 step , -5.32 Actor Loss, 0.64 Critic Loss,  0.01 Threshold , 0.54 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "217 episode , 500 step , -6.23 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "218 episode , 500 step , -8.31 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.34 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "219 episode , 500 step , -9.09 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.49 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "220 episode , 139 step , -9.28 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.78 Avg reward\n",
            "[0, 0, 0]\n",
            "221 episode , 500 step , -9.96 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.55 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "222 episode , 500 step , -10.21 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.11 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "223 episode , 81 step , -10.48 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.11 Top reward, -1.34 Avg reward\n",
            "[0, 0, 0]\n",
            "224 episode , 500 step , -11.49 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.29 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "225 episode , 500 step , -14.80 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , -0.01 Top reward, -0.10 Avg reward\n",
            "[0, 0, 0]\n",
            "226 episode , 500 step , -18.05 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.08 Top reward, -0.10 Avg reward\n",
            "[0, 0, 0]\n",
            "227 episode , 500 step , -20.83 Actor Loss, 0.27 Critic Loss,  0.01 Threshold , 0.18 Top reward, -0.10 Avg reward\n",
            "[0, 0, 0]\n",
            "228 episode , 500 step , -21.52 Actor Loss, 0.27 Critic Loss,  0.01 Threshold , 0.34 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "229 episode , 500 step , -22.57 Actor Loss, 0.26 Critic Loss,  0.01 Threshold , 0.48 Top reward, 0.00 Avg reward\n",
            "[0, 0, 0]\n",
            "230 episode , 130 step , -23.53 Actor Loss, 0.24 Critic Loss,  0.01 Threshold , 0.41 Top reward, -0.79 Avg reward\n",
            "[0, 0, 0]\n",
            "231 episode , 500 step , -24.39 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.42 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "232 episode , 500 step , -24.53 Actor Loss, 0.29 Critic Loss,  0.01 Threshold , 0.43 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "233 episode , 500 step , -24.97 Actor Loss, 0.27 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "234 episode , 500 step , -25.17 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.07 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "235 episode , 500 step , -25.16 Actor Loss, 0.30 Critic Loss,  0.01 Threshold , 0.07 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "236 episode , 500 step , -25.47 Actor Loss, 0.29 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "237 episode , 500 step , -25.77 Actor Loss, 0.30 Critic Loss,  0.01 Threshold , 0.18 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "238 episode , 500 step , -26.37 Actor Loss, 0.25 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "239 episode , 500 step , -26.84 Actor Loss, 0.21 Critic Loss,  0.01 Threshold , 0.15 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "240 episode , 87 step , -27.06 Actor Loss, 0.22 Critic Loss,  0.01 Threshold , 0.19 Top reward, -1.19 Avg reward\n",
            "[0, 0, 0]\n",
            "241 episode , 65 step , -26.70 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.33 Top reward, -1.57 Avg reward\n",
            "[0, 0, 0]\n",
            "242 episode , 61 step , -26.35 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.27 Top reward, -1.66 Avg reward\n",
            "[0, 0, 0]\n",
            "243 episode , 61 step , -25.46 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.30 Top reward, -1.67 Avg reward\n",
            "[0, 0, 0]\n",
            "244 episode , 65 step , -25.10 Actor Loss, 0.32 Critic Loss,  0.01 Threshold , 0.28 Top reward, -1.54 Avg reward\n",
            "[0, 0, 0]\n",
            "245 episode , 57 step , -24.90 Actor Loss, 0.30 Critic Loss,  0.01 Threshold , 0.31 Top reward, -1.79 Avg reward\n",
            "[0, 0, 0]\n",
            "246 episode , 59 step , -23.66 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.31 Top reward, -1.71 Avg reward\n",
            "[0, 0, 0]\n",
            "247 episode , 500 step , -23.80 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.46 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "248 episode , 203 step , -24.25 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.36 Top reward, -0.51 Avg reward\n",
            "[0, 0, 0]\n",
            "249 episode , 219 step , -23.73 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.29 Top reward, -0.52 Avg reward\n",
            "[0, 0, 0]\n",
            "250 episode , 500 step , -23.24 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.19 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "251 episode , 500 step , -23.49 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.28 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "252 episode , 500 step , -24.32 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.30 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "253 episode , 500 step , -23.87 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "254 episode , 152 step , -24.38 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.38 Top reward, -0.69 Avg reward\n",
            "[0, 0, 0]\n",
            "255 episode , 500 step , -23.26 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.35 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "256 episode , 129 step , -22.80 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.40 Top reward, -0.77 Avg reward\n",
            "[0, 0, 0]\n",
            "257 episode , 263 step , -22.43 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.49 Top reward, -0.37 Avg reward\n",
            "[0, 0, 0]\n",
            "258 episode , 100 step , -21.45 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.21 Top reward, -1.03 Avg reward\n",
            "[0, 0, 0]\n",
            "259 episode , 86 step , -20.38 Actor Loss, 0.74 Critic Loss,  0.01 Threshold , 0.20 Top reward, -1.19 Avg reward\n",
            "[0, 0, 0]\n",
            "260 episode , 500 step , -19.03 Actor Loss, 0.72 Critic Loss,  0.01 Threshold , 0.08 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "261 episode , 151 step , -16.80 Actor Loss, 0.94 Critic Loss,  0.01 Threshold , 0.22 Top reward, -0.72 Avg reward\n",
            "[0, 0, 0]\n",
            "262 episode , 96 step , -15.93 Actor Loss, 0.94 Critic Loss,  0.01 Threshold , 0.27 Top reward, -1.09 Avg reward\n",
            "[0, 0, 0]\n",
            "263 episode , 91 step , -15.00 Actor Loss, 0.90 Critic Loss,  0.01 Threshold , 0.32 Top reward, -1.11 Avg reward\n",
            "[0, 0, 0]\n",
            "264 episode , 500 step , -14.27 Actor Loss, 0.94 Critic Loss,  0.01 Threshold , 0.70 Top reward, 0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "265 episode , 500 step , -13.51 Actor Loss, 1.10 Critic Loss,  0.01 Threshold , 0.46 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "266 episode , 500 step , -13.51 Actor Loss, 1.13 Critic Loss,  0.01 Threshold , 0.24 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "267 episode , 500 step , -13.00 Actor Loss, 1.26 Critic Loss,  0.01 Threshold , 0.22 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "268 episode , 500 step , -12.71 Actor Loss, 1.38 Critic Loss,  0.01 Threshold , 0.35 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "269 episode , 500 step , -13.10 Actor Loss, 1.38 Critic Loss,  0.01 Threshold , 0.42 Top reward, -0.03 Avg reward\n",
            "[0, 0, 0]\n",
            "270 episode , 500 step , -12.97 Actor Loss, 1.42 Critic Loss,  0.01 Threshold , 0.34 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "271 episode , 500 step , -11.66 Actor Loss, 1.54 Critic Loss,  0.01 Threshold , 0.34 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "272 episode , 500 step , -12.33 Actor Loss, 1.61 Critic Loss,  0.01 Threshold , 0.37 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "273 episode , 500 step , -15.61 Actor Loss, 1.38 Critic Loss,  0.01 Threshold , 0.36 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "274 episode , 500 step , -20.81 Actor Loss, 1.18 Critic Loss,  0.01 Threshold , 0.46 Top reward, -0.01 Avg reward\n",
            "[0, 0, 0]\n",
            "275 episode , 72 step , -20.51 Actor Loss, 1.13 Critic Loss,  0.01 Threshold , 0.19 Top reward, -1.49 Avg reward\n",
            "[0, 0, 0]\n",
            "276 episode , 70 step , -18.34 Actor Loss, 1.21 Critic Loss,  0.01 Threshold , 0.26 Top reward, -1.49 Avg reward\n",
            "[0, 0, 0]\n",
            "277 episode , 66 step , -18.25 Actor Loss, 1.55 Critic Loss,  0.01 Threshold , 0.24 Top reward, -1.56 Avg reward\n",
            "[0, 0, 0]\n",
            "278 episode , 500 step , -20.25 Actor Loss, 1.30 Critic Loss,  0.01 Threshold , 0.52 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "279 episode , 500 step , -21.37 Actor Loss, 1.32 Critic Loss,  0.01 Threshold , 0.44 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "280 episode , 500 step , -21.16 Actor Loss, 1.36 Critic Loss,  0.01 Threshold , 0.14 Top reward, -0.07 Avg reward\n",
            "[0, 0, 0]\n",
            "281 episode , 500 step , -22.04 Actor Loss, 1.43 Critic Loss,  0.01 Threshold , 0.26 Top reward, -0.04 Avg reward\n",
            "[0, 0, 0]\n",
            "282 episode , 500 step , -22.98 Actor Loss, 1.42 Critic Loss,  0.01 Threshold , 0.11 Top reward, -0.06 Avg reward\n",
            "[0, 0, 0]\n",
            "283 episode , 500 step , -24.43 Actor Loss, 1.40 Critic Loss,  0.01 Threshold , 0.05 Top reward, -0.09 Avg reward\n",
            "[0, 0, 0]\n",
            "284 episode , 500 step , -27.75 Actor Loss, 1.20 Critic Loss,  0.01 Threshold , 0.04 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "285 episode , 500 step , -32.42 Actor Loss, 0.99 Critic Loss,  0.01 Threshold , 0.02 Top reward, -0.09 Avg reward\n",
            "[0, 0, 0]\n",
            "286 episode , 500 step , -37.28 Actor Loss, 0.76 Critic Loss,  0.01 Threshold , 0.33 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}