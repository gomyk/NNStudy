{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRjm8DCVutRZp//uJC9+jc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20BipedalWalker-v2%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "057bff4d-d2e2-445d-ca04-d84913345c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"BipedalWalker-v2\")\n",
        "env._max_episode_steps = 1600\n",
        "#env.seed(10)\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 256\n",
        "HIDDEN_SIZE2 = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE2)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE2, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE2, outputs)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.linear(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        #x = F.relu(self.linear3(x))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size + action_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE2)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE2, 1)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        #x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "5815ff73-5df2-4084-ec65-248910adf052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDm4zs8z8fxnhxjNjO/w7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 9\n",
        "ACTOR_LR = 0.0001\n",
        "CRITIC_LR = 0.0003\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 1000\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 0.15\n",
        "ou_noise_sigma = 0.2\n",
        "\n",
        "RECORD_INTERVAL = 199\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = env.action_space.shape[0]\n",
        "n_obvs = env.observation_space.shape[0]\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR, weight_decay=0.0001)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = np.random.uniform(-1,1,n_actions)\n",
        "    else:\n",
        "        selected_action = actor(\n",
        "             torch.FloatTensor(state).to(device)\n",
        "         ).detach().cpu().numpy()\n",
        "    _noise = noise.sample()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvU9RgzCYsW",
        "colab_type": "text"
      },
      "source": [
        "##Normalizer for obv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1 , -1\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + GAMMA * next_value * masks\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.smooth_l1_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    # train actor\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "63522823-a022-44b2-8c00-f2335f86309a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frames = []\n",
        "\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "        reward = max(min(reward, 1), -1)\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(total_reward)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Total reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward))\n",
        "            #print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        actor_target.load_state_dict(actor.state_dict())\n",
        "        critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 1600 step , -0.10 Actor Loss, -0.08 Critic Loss,  0.41 Threshold , 0.19 Top reward, -70.74 Total reward\n",
            "1 episode , 1600 step , -0.06 Actor Loss, 0.00 Critic Loss,  0.19 Threshold , 0.23 Top reward, -54.84 Total reward\n",
            "2 episode , 49 step , -0.11 Actor Loss, 0.00 Critic Loss,  0.19 Threshold , 0.28 Top reward, -3.16 Total reward\n",
            "3 episode , 106 step , -0.14 Actor Loss, 0.01 Critic Loss,  0.18 Threshold , 0.37 Top reward, -2.42 Total reward\n",
            "4 episode , 63 step , -0.16 Actor Loss, 0.01 Critic Loss,  0.17 Threshold , 0.31 Top reward, -9.52 Total reward\n",
            "5 episode , 957 step , -0.18 Actor Loss, 0.01 Critic Loss,  0.11 Threshold , 0.31 Top reward, -71.53 Total reward\n",
            "6 episode , 67 step , -0.17 Actor Loss, 0.01 Critic Loss,  0.11 Threshold , 0.31 Top reward, -1.25 Total reward\n",
            "7 episode , 53 step , -0.17 Actor Loss, 0.01 Critic Loss,  0.10 Threshold , 0.23 Top reward, -3.27 Total reward\n",
            "8 episode , 62 step , -0.18 Actor Loss, 0.01 Critic Loss,  0.10 Threshold , 0.19 Top reward, -6.47 Total reward\n",
            "9 episode , 73 step , -0.18 Actor Loss, 0.01 Critic Loss,  0.10 Threshold , 0.25 Top reward, -4.50 Total reward\n",
            "10 episode , 51 step , -0.20 Actor Loss, 0.01 Critic Loss,  0.10 Threshold , 0.09 Top reward, -5.18 Total reward\n",
            "11 episode , 82 step , -0.21 Actor Loss, 0.01 Critic Loss,  0.09 Threshold , 0.17 Top reward, -3.84 Total reward\n",
            "12 episode , 68 step , -0.22 Actor Loss, 0.01 Critic Loss,  0.09 Threshold , 0.10 Top reward, -3.62 Total reward\n",
            "13 episode , 57 step , -0.21 Actor Loss, 0.01 Critic Loss,  0.09 Threshold , 0.16 Top reward, -4.17 Total reward\n",
            "14 episode , 113 step , -0.21 Actor Loss, 0.01 Critic Loss,  0.08 Threshold , 0.19 Top reward, -2.31 Total reward\n",
            "15 episode , 1600 step , -0.22 Actor Loss, 0.01 Critic Loss,  0.04 Threshold , 0.23 Top reward, -78.62 Total reward\n",
            "16 episode , 64 step , -0.17 Actor Loss, 0.01 Critic Loss,  0.04 Threshold , 0.20 Top reward, -2.72 Total reward\n",
            "17 episode , 171 step , -0.21 Actor Loss, 0.01 Critic Loss,  0.04 Threshold , 0.21 Top reward, -8.49 Total reward\n",
            "18 episode , 1600 step , -0.22 Actor Loss, 0.01 Critic Loss,  0.02 Threshold , 0.48 Top reward, -35.16 Total reward\n",
            "19 episode , 1031 step , -0.25 Actor Loss, 0.01 Critic Loss,  0.02 Threshold , 0.25 Top reward, -24.70 Total reward\n",
            "20 episode , 105 step , -0.28 Actor Loss, 0.01 Critic Loss,  0.02 Threshold , 0.10 Top reward, -15.42 Total reward\n",
            "21 episode , 1434 step , -0.30 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.31 Top reward, -28.76 Total reward\n",
            "22 episode , 199 step , -0.32 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.38 Top reward, 11.21 Total reward\n",
            "23 episode , 197 step , -0.33 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.21 Top reward, 7.78 Total reward\n",
            "24 episode , 74 step , -0.35 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.03 Top reward, -13.56 Total reward\n",
            "25 episode , 1600 step , -0.37 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.33 Top reward, -14.13 Total reward\n",
            "26 episode , 1600 step , -0.44 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.22 Top reward, -29.63 Total reward\n",
            "27 episode , 74 step , -0.49 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.14 Top reward, -5.15 Total reward\n",
            "28 episode , 90 step , -0.50 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.30 Top reward, -2.53 Total reward\n",
            "29 episode , 348 step , -0.51 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.17 Top reward, -6.99 Total reward\n",
            "30 episode , 687 step , -0.55 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.25 Top reward, -2.35 Total reward\n",
            "31 episode , 276 step , -0.60 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.21 Top reward, -1.17 Total reward\n",
            "32 episode , 72 step , -0.61 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.20 Top reward, 1.44 Total reward\n",
            "33 episode , 1600 step , -0.67 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.57 Top reward, -89.16 Total reward\n",
            "34 episode , 76 step , -0.72 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.20 Top reward, 2.46 Total reward\n",
            "35 episode , 175 step , -0.73 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.24 Top reward, -0.97 Total reward\n",
            "36 episode , 63 step , -0.74 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.14 Top reward, -1.54 Total reward\n",
            "37 episode , 252 step , -0.80 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.25 Top reward, 2.20 Total reward\n",
            "38 episode , 60 step , -0.81 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.28 Top reward, 3.31 Total reward\n",
            "39 episode , 524 step , -0.84 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.33 Top reward, 6.70 Total reward\n",
            "40 episode , 1600 step , -0.94 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.42 Top reward, -0.80 Total reward\n",
            "41 episode , 67 step , -1.02 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.07 Top reward, -11.07 Total reward\n",
            "42 episode , 1600 step , -1.02 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.35 Top reward, -1.62 Total reward\n",
            "43 episode , 72 step , -1.08 Actor Loss, 0.01 Critic Loss,  0.01 Threshold , 0.19 Top reward, -7.89 Total reward\n",
            "44 episode , 55 step , -1.09 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.03 Top reward, -13.32 Total reward\n",
            "45 episode , 68 step , -1.08 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.12 Top reward, -12.96 Total reward\n",
            "46 episode , 100 step , -1.08 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.24 Top reward, -1.49 Total reward\n",
            "47 episode , 83 step , -1.09 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.36 Top reward, -4.88 Total reward\n",
            "48 episode , 170 step , -1.10 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.19 Top reward, -4.23 Total reward\n",
            "49 episode , 77 step , -1.10 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.36 Top reward, -2.25 Total reward\n",
            "50 episode , 80 step , -1.13 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.56 Top reward, -1.23 Total reward\n",
            "51 episode , 229 step , -1.14 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.30 Top reward, -4.93 Total reward\n",
            "52 episode , 97 step , -1.18 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.25 Top reward, 2.26 Total reward\n",
            "53 episode , 67 step , -1.20 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.35 Top reward, 0.32 Total reward\n",
            "54 episode , 1600 step , -1.30 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.34 Top reward, 4.73 Total reward\n",
            "55 episode , 72 step , -1.43 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.19 Top reward, -6.00 Total reward\n",
            "56 episode , 82 step , -1.39 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.22 Top reward, -3.97 Total reward\n",
            "57 episode , 62 step , -1.40 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.14 Top reward, -12.59 Total reward\n",
            "58 episode , 74 step , -1.38 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.09 Top reward, -10.12 Total reward\n",
            "59 episode , 77 step , -1.38 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.39 Top reward, -7.81 Total reward\n",
            "60 episode , 79 step , -1.39 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.15 Top reward, -15.44 Total reward\n",
            "61 episode , 62 step , -1.37 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.14 Top reward, -2.82 Total reward\n",
            "62 episode , 53 step , -1.39 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.02 Top reward, -8.14 Total reward\n",
            "63 episode , 110 step , -1.38 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.32 Top reward, 5.33 Total reward\n",
            "64 episode , 60 step , -1.40 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.01 Top reward, -9.43 Total reward\n",
            "65 episode , 107 step , -1.40 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.21 Top reward, -2.57 Total reward\n",
            "66 episode , 79 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.43 Top reward, -5.74 Total reward\n",
            "67 episode , 64 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.05 Top reward, -5.81 Total reward\n",
            "68 episode , 59 step , -1.41 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.22 Top reward, 0.16 Total reward\n",
            "69 episode , 64 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.01 Top reward, -9.91 Total reward\n",
            "70 episode , 66 step , -1.40 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.11 Top reward, -7.29 Total reward\n",
            "71 episode , 1600 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.34 Top reward, -4.61 Total reward\n",
            "72 episode , 87 step , -1.44 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.12 Top reward, -0.75 Total reward\n",
            "73 episode , 1600 step , -1.44 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.26 Top reward, -12.01 Total reward\n",
            "74 episode , 1600 step , -1.45 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.33 Top reward, -7.67 Total reward\n",
            "75 episode , 177 step , -1.45 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.24 Top reward, -30.98 Total reward\n",
            "76 episode , 76 step , -1.47 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.06 Top reward, -8.36 Total reward\n",
            "77 episode , 78 step , -1.47 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.27 Top reward, -1.85 Total reward\n",
            "78 episode , 167 step , -1.46 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.30 Top reward, -6.26 Total reward\n",
            "79 episode , 60 step , -1.49 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.02 Top reward, -8.71 Total reward\n",
            "80 episode , 94 step , -1.48 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.33 Top reward, -1.75 Total reward\n",
            "81 episode , 80 step , -1.48 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.55 Top reward, -2.32 Total reward\n",
            "82 episode , 81 step , -1.45 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.10 Top reward, -2.61 Total reward\n",
            "83 episode , 85 step , -1.46 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.52 Top reward, -1.11 Total reward\n",
            "84 episode , 53 step , -1.48 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.18 Top reward, -5.14 Total reward\n",
            "85 episode , 71 step , -1.45 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.19 Top reward, -4.29 Total reward\n",
            "86 episode , 66 step , -1.41 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.09 Top reward, -8.75 Total reward\n",
            "87 episode , 70 step , -1.47 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.29 Top reward, 1.18 Total reward\n",
            "88 episode , 1600 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.33 Top reward, -15.15 Total reward\n",
            "89 episode , 66 step , -1.34 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.62 Top reward, 1.47 Total reward\n",
            "90 episode , 61 step , -1.37 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.01 Top reward, -7.86 Total reward\n",
            "91 episode , 85 step , -1.38 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.25 Top reward, 0.31 Total reward\n",
            "92 episode , 96 step , -1.39 Actor Loss, 0.02 Critic Loss,  0.01 Threshold , 0.30 Top reward, 0.90 Total reward\n",
            "93 episode , 67 step , -1.36 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.48 Top reward, -0.04 Total reward\n",
            "94 episode , 71 step , -1.37 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.38 Top reward, 3.89 Total reward\n",
            "95 episode , 65 step , -1.38 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.49 Top reward, 0.29 Total reward\n",
            "96 episode , 93 step , -1.40 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.35 Top reward, 7.01 Total reward\n",
            "97 episode , 115 step , -1.42 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.25 Top reward, 2.98 Total reward\n",
            "98 episode , 94 step , -1.43 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.29 Top reward, 5.65 Total reward\n",
            "99 episode , 91 step , -1.45 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.29 Top reward, 4.97 Total reward\n",
            "100 episode , 98 step , -1.46 Actor Loss, 0.03 Critic Loss,  0.01 Threshold , 0.35 Top reward, 7.58 Total reward\n",
            "101 episode , 77 step , -1.48 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.27 Top reward, 4.56 Total reward\n",
            "102 episode , 84 step , -1.50 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.29 Top reward, 1.49 Total reward\n",
            "103 episode , 81 step , -1.52 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.36 Top reward, 2.96 Total reward\n",
            "104 episode , 109 step , -1.54 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.29 Top reward, 5.79 Total reward\n",
            "105 episode , 106 step , -1.58 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.37 Top reward, 6.98 Total reward\n",
            "106 episode , 103 step , -1.60 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.34 Top reward, 7.55 Total reward\n",
            "107 episode , 77 step , -1.66 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.34 Top reward, 8.45 Total reward\n",
            "108 episode , 80 step , -1.67 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.38 Top reward, 8.02 Total reward\n",
            "109 episode , 84 step , -1.68 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.41 Top reward, 9.87 Total reward\n",
            "110 episode , 80 step , -1.70 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.33 Top reward, 7.61 Total reward\n",
            "111 episode , 90 step , -1.71 Actor Loss, 0.04 Critic Loss,  0.01 Threshold , 0.43 Top reward, 7.23 Total reward\n",
            "112 episode , 75 step , -1.75 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.30 Top reward, 5.86 Total reward\n",
            "113 episode , 71 step , -1.77 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.28 Top reward, 5.37 Total reward\n",
            "114 episode , 81 step , -1.80 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.33 Top reward, 6.77 Total reward\n",
            "115 episode , 86 step , -1.78 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.33 Top reward, 2.68 Total reward\n",
            "116 episode , 72 step , -1.83 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.38 Top reward, 3.05 Total reward\n",
            "117 episode , 87 step , -1.84 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.24 Top reward, 2.88 Total reward\n",
            "118 episode , 79 step , -1.85 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.31 Top reward, 1.62 Total reward\n",
            "119 episode , 75 step , -1.86 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.30 Top reward, 4.17 Total reward\n",
            "120 episode , 87 step , -1.88 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.38 Top reward, 4.81 Total reward\n",
            "121 episode , 81 step , -1.89 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.36 Top reward, 5.56 Total reward\n",
            "122 episode , 82 step , -1.91 Actor Loss, 0.05 Critic Loss,  0.01 Threshold , 0.27 Top reward, 6.74 Total reward\n",
            "123 episode , 73 step , -1.95 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.27 Top reward, 5.41 Total reward\n",
            "124 episode , 84 step , -1.96 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.37 Top reward, 4.36 Total reward\n",
            "125 episode , 71 step , -1.97 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.24 Top reward, 3.86 Total reward\n",
            "126 episode , 76 step , -1.99 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.36 Top reward, 5.75 Total reward\n",
            "127 episode , 71 step , -2.00 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.31 Top reward, 5.55 Total reward\n",
            "128 episode , 65 step , -2.03 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.27 Top reward, 5.27 Total reward\n",
            "129 episode , 70 step , -2.05 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.29 Top reward, 3.87 Total reward\n",
            "130 episode , 77 step , -2.06 Actor Loss, 0.06 Critic Loss,  0.01 Threshold , 0.32 Top reward, 3.53 Total reward\n",
            "131 episode , 150 step , -2.13 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.28 Top reward, 11.84 Total reward\n",
            "132 episode , 75 step , -2.13 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.23 Top reward, 3.41 Total reward\n",
            "133 episode , 86 step , -2.17 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.45 Top reward, 6.57 Total reward\n",
            "134 episode , 80 step , -2.18 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.38 Top reward, 4.62 Total reward\n",
            "135 episode , 99 step , -2.21 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.39 Top reward, 9.20 Total reward\n",
            "136 episode , 70 step , -2.28 Actor Loss, 0.08 Critic Loss,  0.01 Threshold , 0.38 Top reward, 5.60 Total reward\n",
            "137 episode , 80 step , -2.27 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.38 Top reward, 5.09 Total reward\n",
            "138 episode , 72 step , -2.27 Actor Loss, 0.07 Critic Loss,  0.01 Threshold , 0.40 Top reward, 5.86 Total reward\n",
            "139 episode , 81 step , -2.31 Actor Loss, 0.08 Critic Loss,  0.01 Threshold , 0.49 Top reward, 10.17 Total reward\n",
            "140 episode , 82 step , -2.36 Actor Loss, 0.08 Critic Loss,  0.01 Threshold , 0.51 Top reward, 9.07 Total reward\n",
            "141 episode , 79 step , -2.38 Actor Loss, 0.08 Critic Loss,  0.01 Threshold , 0.50 Top reward, 11.05 Total reward\n",
            "142 episode , 93 step , -2.39 Actor Loss, 0.08 Critic Loss,  0.01 Threshold , 0.47 Top reward, 13.91 Total reward\n",
            "143 episode , 201 step , -2.46 Actor Loss, 0.09 Critic Loss,  0.01 Threshold , 0.40 Top reward, 7.83 Total reward\n",
            "144 episode , 137 step , -2.51 Actor Loss, 0.09 Critic Loss,  0.01 Threshold , 0.30 Top reward, 11.06 Total reward\n",
            "145 episode , 132 step , -2.57 Actor Loss, 0.09 Critic Loss,  0.01 Threshold , 0.33 Top reward, 10.46 Total reward\n",
            "146 episode , 107 step , -2.60 Actor Loss, 0.09 Critic Loss,  0.01 Threshold , 0.39 Top reward, 12.23 Total reward\n",
            "147 episode , 121 step , -2.66 Actor Loss, 0.10 Critic Loss,  0.01 Threshold , 0.28 Top reward, 9.09 Total reward\n",
            "148 episode , 656 step , -2.78 Actor Loss, 0.10 Critic Loss,  0.01 Threshold , 0.23 Top reward, -5.25 Total reward\n",
            "149 episode , 131 step , -2.93 Actor Loss, 0.10 Critic Loss,  0.01 Threshold , 0.25 Top reward, 9.74 Total reward\n",
            "150 episode , 86 step , -2.99 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.44 Top reward, 10.76 Total reward\n",
            "151 episode , 101 step , -3.01 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.38 Top reward, 14.04 Total reward\n",
            "152 episode , 68 step , -3.07 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.48 Top reward, 7.45 Total reward\n",
            "153 episode , 107 step , -3.07 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.34 Top reward, 12.49 Total reward\n",
            "154 episode , 93 step , -3.14 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.40 Top reward, 12.23 Total reward\n",
            "155 episode , 111 step , -3.10 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.27 Top reward, 8.25 Total reward\n",
            "156 episode , 115 step , -3.18 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.29 Top reward, 9.33 Total reward\n",
            "157 episode , 94 step , -3.22 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.38 Top reward, 11.39 Total reward\n",
            "158 episode , 110 step , -3.27 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.30 Top reward, 7.25 Total reward\n",
            "159 episode , 103 step , -3.27 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.35 Top reward, 10.46 Total reward\n",
            "160 episode , 107 step , -3.29 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.35 Top reward, 9.55 Total reward\n",
            "161 episode , 111 step , -3.33 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.53 Total reward\n",
            "162 episode , 82 step , -3.37 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.32 Top reward, 8.99 Total reward\n",
            "163 episode , 118 step , -3.34 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 11.92 Total reward\n",
            "164 episode , 70 step , -3.36 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.41 Top reward, 4.96 Total reward\n",
            "165 episode , 129 step , -3.40 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.31 Top reward, 13.58 Total reward\n",
            "166 episode , 119 step , -3.43 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.28 Top reward, 7.05 Total reward\n",
            "167 episode , 93 step , -3.46 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.37 Top reward, 11.11 Total reward\n",
            "168 episode , 1412 step , -3.56 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.26 Top reward, -43.16 Total reward\n",
            "169 episode , 77 step , -3.67 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.44 Top reward, 6.01 Total reward\n",
            "170 episode , 111 step , -3.70 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.30 Top reward, 11.18 Total reward\n",
            "171 episode , 159 step , -3.73 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.28 Top reward, 12.90 Total reward\n",
            "172 episode , 94 step , -3.78 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.36 Top reward, 11.15 Total reward\n",
            "173 episode , 110 step , -3.86 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.43 Top reward, 16.60 Total reward\n",
            "174 episode , 175 step , -3.90 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.25 Top reward, 11.72 Total reward\n",
            "175 episode , 104 step , -3.94 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 12.16 Total reward\n",
            "176 episode , 120 step , -3.95 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.50 Top reward, 12.45 Total reward\n",
            "177 episode , 131 step , -4.00 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.36 Top reward, 14.18 Total reward\n",
            "178 episode , 108 step , -3.98 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.55 Top reward, 13.57 Total reward\n",
            "179 episode , 120 step , -4.01 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.34 Top reward, 12.21 Total reward\n",
            "180 episode , 92 step , -4.05 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.48 Top reward, 11.86 Total reward\n",
            "181 episode , 109 step , -4.07 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.37 Top reward, 12.43 Total reward\n",
            "182 episode , 120 step , -4.13 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.31 Top reward, 12.50 Total reward\n",
            "183 episode , 90 step , -4.12 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.54 Top reward, 11.99 Total reward\n",
            "184 episode , 106 step , -4.12 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.50 Top reward, 14.49 Total reward\n",
            "185 episode , 98 step , -4.13 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.31 Top reward, 7.65 Total reward\n",
            "186 episode , 98 step , -4.14 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.35 Top reward, 10.62 Total reward\n",
            "187 episode , 113 step , -4.13 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.28 Top reward, 10.15 Total reward\n",
            "188 episode , 107 step , -4.15 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.27 Total reward\n",
            "189 episode , 117 step , -4.19 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.27 Top reward, 8.66 Total reward\n",
            "190 episode , 101 step , -4.22 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.34 Top reward, 8.78 Total reward\n",
            "191 episode , 95 step , -4.22 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.41 Top reward, 12.99 Total reward\n",
            "192 episode , 98 step , -4.25 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.35 Top reward, 11.33 Total reward\n",
            "193 episode , 101 step , -4.20 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 10.85 Total reward\n",
            "194 episode , 125 step , -4.23 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.31 Top reward, 11.03 Total reward\n",
            "195 episode , 147 step , -4.26 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.29 Top reward, 10.82 Total reward\n",
            "196 episode , 103 step , -4.27 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.47 Top reward, 13.20 Total reward\n",
            "197 episode , 136 step , -4.32 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.38 Top reward, 14.75 Total reward\n",
            "198 episode , 122 step , -4.36 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.40 Top reward, 12.44 Total reward\n",
            "199 episode , 156 step , -4.32 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.27 Top reward, 13.99 Total reward\n",
            "200 episode , 137 step , -4.35 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.29 Top reward, 14.46 Total reward\n",
            "201 episode , 125 step , -4.36 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.34 Top reward, 13.98 Total reward\n",
            "202 episode , 152 step , -4.43 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.45 Top reward, 14.51 Total reward\n",
            "203 episode , 140 step , -4.42 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.39 Top reward, 13.97 Total reward\n",
            "204 episode , 114 step , -4.44 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.30 Top reward, 7.19 Total reward\n",
            "205 episode , 87 step , -4.45 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 8.47 Total reward\n",
            "206 episode , 92 step , -4.44 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.25 Top reward, 6.68 Total reward\n",
            "207 episode , 118 step , -4.44 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.28 Top reward, 10.38 Total reward\n",
            "208 episode , 108 step , -4.47 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.25 Top reward, 6.77 Total reward\n",
            "209 episode , 90 step , -4.45 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.22 Top reward, 5.73 Total reward\n",
            "210 episode , 121 step , -4.49 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.22 Top reward, 4.40 Total reward\n",
            "211 episode , 95 step , -4.49 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.22 Top reward, 6.08 Total reward\n",
            "212 episode , 118 step , -4.47 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 7.63 Total reward\n",
            "213 episode , 73 step , -4.50 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 2.32 Total reward\n",
            "214 episode , 76 step , -4.52 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.25 Top reward, 4.26 Total reward\n",
            "215 episode , 95 step , -4.45 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.24 Top reward, 5.05 Total reward\n",
            "216 episode , 138 step , -4.48 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.26 Top reward, 12.00 Total reward\n",
            "217 episode , 114 step , -4.49 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.30 Top reward, 11.57 Total reward\n",
            "218 episode , 121 step , -4.53 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 12.67 Total reward\n",
            "219 episode , 128 step , -4.50 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 10.02 Total reward\n",
            "220 episode , 135 step , -4.53 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.23 Top reward, 10.65 Total reward\n",
            "221 episode , 103 step , -4.54 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 8.52 Total reward\n",
            "222 episode , 97 step , -4.56 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.43 Top reward, 10.00 Total reward\n",
            "223 episode , 119 step , -4.54 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.56 Top reward, 15.17 Total reward\n",
            "224 episode , 141 step , -4.57 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 14.18 Total reward\n",
            "225 episode , 116 step , -4.56 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.25 Top reward, 7.74 Total reward\n",
            "226 episode , 127 step , -4.58 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.31 Top reward, 10.22 Total reward\n",
            "227 episode , 110 step , -4.59 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.50 Top reward, 13.36 Total reward\n",
            "228 episode , 77 step , -4.65 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.39 Top reward, 7.29 Total reward\n",
            "229 episode , 82 step , -4.54 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.30 Top reward, -1.34 Total reward\n",
            "230 episode , 96 step , -4.56 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.39 Total reward\n",
            "231 episode , 97 step , -4.61 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.33 Top reward, 11.51 Total reward\n",
            "232 episode , 128 step , -4.61 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 12.23 Total reward\n",
            "233 episode , 84 step , -4.60 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 4.19 Total reward\n",
            "234 episode , 100 step , -4.60 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.25 Top reward, 8.27 Total reward\n",
            "235 episode , 108 step , -4.61 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.33 Top reward, 12.57 Total reward\n",
            "236 episode , 140 step , -4.63 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 8.18 Total reward\n",
            "237 episode , 105 step , -4.64 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.35 Top reward, 1.49 Total reward\n",
            "238 episode , 111 step , -4.68 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.35 Top reward, 8.18 Total reward\n",
            "239 episode , 97 step , -4.68 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.30 Top reward, 3.88 Total reward\n",
            "240 episode , 119 step , -4.68 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 14.40 Total reward\n",
            "241 episode , 149 step , -4.71 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.32 Top reward, 6.36 Total reward\n",
            "242 episode , 165 step , -4.69 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.28 Top reward, 11.67 Total reward\n",
            "243 episode , 85 step , -4.71 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 11.85 Total reward\n",
            "244 episode , 109 step , -4.66 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.55 Top reward, 12.71 Total reward\n",
            "245 episode , 106 step , -4.72 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.65 Top reward, 15.12 Total reward\n",
            "246 episode , 115 step , -4.72 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.36 Top reward, 11.99 Total reward\n",
            "247 episode , 98 step , -4.72 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.33 Top reward, 8.00 Total reward\n",
            "248 episode , 136 step , -4.77 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 9.76 Total reward\n",
            "249 episode , 102 step , -4.77 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.31 Top reward, 5.74 Total reward\n",
            "250 episode , 109 step , -4.75 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.28 Top reward, 7.09 Total reward\n",
            "251 episode , 120 step , -4.78 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.36 Top reward, 8.10 Total reward\n",
            "252 episode , 125 step , -4.83 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.37 Top reward, 10.83 Total reward\n",
            "253 episode , 143 step , -4.86 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.32 Top reward, 10.31 Total reward\n",
            "254 episode , 122 step , -4.84 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 15.05 Total reward\n",
            "255 episode , 199 step , -4.90 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.35 Top reward, 9.49 Total reward\n",
            "256 episode , 127 step , -4.92 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 8.42 Total reward\n",
            "257 episode , 115 step , -4.92 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.37 Top reward, 10.48 Total reward\n",
            "258 episode , 183 step , -4.93 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 14.24 Total reward\n",
            "259 episode , 89 step , -4.92 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.32 Top reward, 5.64 Total reward\n",
            "260 episode , 193 step , -4.93 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.28 Top reward, 10.76 Total reward\n",
            "261 episode , 98 step , -4.96 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.40 Top reward, 8.37 Total reward\n",
            "262 episode , 150 step , -4.92 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.30 Top reward, 8.72 Total reward\n",
            "263 episode , 120 step , -4.97 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.33 Top reward, 8.76 Total reward\n",
            "264 episode , 113 step , -4.96 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 9.15 Total reward\n",
            "265 episode , 111 step , -5.00 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.40 Top reward, 9.75 Total reward\n",
            "266 episode , 157 step , -4.97 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.27 Top reward, 13.53 Total reward\n",
            "267 episode , 120 step , -4.98 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.32 Top reward, 13.81 Total reward\n",
            "268 episode , 120 step , -4.98 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.43 Top reward, 8.72 Total reward\n",
            "269 episode , 166 step , -4.99 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.39 Top reward, 7.63 Total reward\n",
            "270 episode , 210 step , -5.01 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 12.59 Total reward\n",
            "271 episode , 100 step , -4.96 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.34 Top reward, 9.76 Total reward\n",
            "272 episode , 105 step , -5.01 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.35 Top reward, 9.63 Total reward\n",
            "273 episode , 118 step , -5.04 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.41 Top reward, 15.10 Total reward\n",
            "274 episode , 149 step , -5.02 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.32 Top reward, 12.57 Total reward\n",
            "275 episode , 132 step , -5.04 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.32 Top reward, 13.54 Total reward\n",
            "276 episode , 95 step , -5.03 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.34 Top reward, 11.16 Total reward\n",
            "277 episode , 97 step , -5.09 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.41 Total reward\n",
            "278 episode , 114 step , -5.02 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.31 Top reward, 10.80 Total reward\n",
            "279 episode , 120 step , -5.08 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.29 Top reward, 11.13 Total reward\n",
            "280 episode , 99 step , -5.17 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.29 Top reward, 7.66 Total reward\n",
            "281 episode , 147 step , -5.12 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.32 Top reward, 13.62 Total reward\n",
            "282 episode , 149 step , -5.16 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.39 Top reward, 16.26 Total reward\n",
            "283 episode , 116 step , -5.17 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.37 Top reward, 16.70 Total reward\n",
            "284 episode , 214 step , -5.20 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.44 Top reward, 15.34 Total reward\n",
            "285 episode , 166 step , -5.18 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.46 Top reward, 20.23 Total reward\n",
            "286 episode , 152 step , -5.21 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.36 Top reward, 18.48 Total reward\n",
            "287 episode , 210 step , -5.27 Actor Loss, 0.11 Critic Loss,  0.01 Threshold , 0.42 Top reward, 30.06 Total reward\n",
            "288 episode , 172 step , -5.30 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.41 Top reward, 24.71 Total reward\n",
            "289 episode , 133 step , -5.40 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.38 Top reward, 22.12 Total reward\n",
            "290 episode , 119 step , -5.46 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.39 Top reward, 17.51 Total reward\n",
            "291 episode , 106 step , -5.43 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.37 Top reward, 11.10 Total reward\n",
            "292 episode , 108 step , -5.52 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.53 Top reward, 13.28 Total reward\n",
            "293 episode , 180 step , -5.52 Actor Loss, 0.12 Critic Loss,  0.01 Threshold , 0.49 Top reward, 24.82 Total reward\n",
            "294 episode , 616 step , -5.67 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.51 Top reward, 89.02 Total reward\n",
            "295 episode , 113 step , -5.74 Actor Loss, 0.13 Critic Loss,  0.01 Threshold , 0.38 Top reward, 16.19 Total reward\n",
            "296 episode , 305 step , -5.82 Actor Loss, 0.14 Critic Loss,  0.01 Threshold , 0.45 Top reward, 42.33 Total reward\n",
            "297 episode , 345 step , -5.88 Actor Loss, 0.14 Critic Loss,  0.01 Threshold , 0.40 Top reward, 56.83 Total reward\n",
            "298 episode , 140 step , -6.10 Actor Loss, 0.15 Critic Loss,  0.01 Threshold , 0.57 Top reward, 21.84 Total reward\n",
            "299 episode , 276 step , -6.15 Actor Loss, 0.15 Critic Loss,  0.01 Threshold , 0.44 Top reward, 42.60 Total reward\n",
            "300 episode , 284 step , -6.26 Actor Loss, 0.16 Critic Loss,  0.01 Threshold , 0.46 Top reward, 50.92 Total reward\n",
            "301 episode , 207 step , -6.35 Actor Loss, 0.16 Critic Loss,  0.01 Threshold , 0.42 Top reward, 31.75 Total reward\n",
            "302 episode , 569 step , -6.49 Actor Loss, 0.17 Critic Loss,  0.01 Threshold , 0.35 Top reward, 85.25 Total reward\n",
            "303 episode , 225 step , -6.67 Actor Loss, 0.18 Critic Loss,  0.01 Threshold , 0.42 Top reward, 41.34 Total reward\n",
            "304 episode , 108 step , -6.74 Actor Loss, 0.18 Critic Loss,  0.01 Threshold , 0.37 Top reward, 13.96 Total reward\n",
            "305 episode , 502 step , -6.84 Actor Loss, 0.19 Critic Loss,  0.01 Threshold , 0.45 Top reward, 109.30 Total reward\n",
            "306 episode , 379 step , -7.05 Actor Loss, 0.19 Critic Loss,  0.01 Threshold , 0.51 Top reward, 86.27 Total reward\n",
            "307 episode , 90 step , -7.18 Actor Loss, 0.20 Critic Loss,  0.01 Threshold , 0.41 Top reward, 13.74 Total reward\n",
            "308 episode , 78 step , -7.18 Actor Loss, 0.20 Critic Loss,  0.01 Threshold , 0.29 Top reward, 3.81 Total reward\n",
            "309 episode , 62 step , -7.20 Actor Loss, 0.21 Critic Loss,  0.01 Threshold , -0.01 Top reward, -17.10 Total reward\n",
            "310 episode , 96 step , -7.20 Actor Loss, 0.20 Critic Loss,  0.01 Threshold , 0.03 Top reward, -21.18 Total reward\n",
            "311 episode , 62 step , -7.19 Actor Loss, 0.21 Critic Loss,  0.01 Threshold , -0.01 Top reward, -15.52 Total reward\n",
            "312 episode , 79 step , -7.21 Actor Loss, 0.20 Critic Loss,  0.01 Threshold , 0.33 Top reward, 3.77 Total reward\n",
            "313 episode , 187 step , -7.25 Actor Loss, 0.20 Critic Loss,  0.01 Threshold , 0.38 Top reward, 28.07 Total reward\n",
            "314 episode , 1415 step , -7.63 Actor Loss, 0.23 Critic Loss,  0.01 Threshold , 0.56 Top reward, 280.16 Total reward\n",
            "315 episode , 1394 step , -8.29 Actor Loss, 0.26 Critic Loss,  0.01 Threshold , 0.50 Top reward, 284.37 Total reward\n",
            "316 episode , 88 step , -8.61 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.35 Top reward, 7.60 Total reward\n",
            "317 episode , 196 step , -8.65 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.44 Top reward, 31.67 Total reward\n",
            "318 episode , 88 step , -8.73 Actor Loss, 0.28 Critic Loss,  0.01 Threshold , 0.38 Top reward, 9.49 Total reward\n",
            "319 episode , 865 step , -8.88 Actor Loss, 0.29 Critic Loss,  0.01 Threshold , 0.52 Top reward, 187.96 Total reward\n",
            "320 episode , 314 step , -9.12 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.54 Top reward, 72.63 Total reward\n",
            "321 episode , 1171 step , -9.44 Actor Loss, 0.32 Critic Loss,  0.01 Threshold , 0.57 Top reward, 293.36 Total reward\n",
            "322 episode , 354 step , -9.75 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.49 Top reward, 78.82 Total reward\n",
            "323 episode , 206 step , -9.84 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.53 Top reward, 34.83 Total reward\n",
            "324 episode , 253 step , -9.90 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.44 Top reward, 44.86 Total reward\n",
            "325 episode , 1248 step , -10.21 Actor Loss, 0.36 Critic Loss,  0.01 Threshold , 0.56 Top reward, 292.37 Total reward\n",
            "326 episode , 130 step , -10.52 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.43 Top reward, 14.05 Total reward\n",
            "327 episode , 48 step , -10.63 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , -0.00 Top reward, -13.35 Total reward\n",
            "328 episode , 47 step , -10.47 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.04 Top reward, -12.99 Total reward\n",
            "329 episode , 122 step , -10.46 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.02 Top reward, -21.57 Total reward\n",
            "330 episode , 81 step , -10.41 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , -0.00 Top reward, -19.19 Total reward\n",
            "331 episode , 115 step , -10.34 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.16 Top reward, -5.21 Total reward\n",
            "332 episode , 153 step , -10.39 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.52 Top reward, 34.90 Total reward\n",
            "333 episode , 920 step , -10.55 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.56 Top reward, 243.75 Total reward\n",
            "334 episode , 98 step , -10.71 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.36 Top reward, 10.32 Total reward\n",
            "335 episode , 1020 step , -11.04 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.47 Top reward, 234.63 Total reward\n",
            "336 episode , 102 step , -11.22 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.41 Top reward, 17.59 Total reward\n",
            "337 episode , 578 step , -11.30 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.56 Top reward, 152.54 Total reward\n",
            "338 episode , 398 step , -11.42 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.61 Top reward, 124.48 Total reward\n",
            "339 episode , 329 step , -11.58 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.53 Top reward, 91.60 Total reward\n",
            "340 episode , 81 step , -11.57 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.31 Top reward, 8.89 Total reward\n",
            "341 episode , 80 step , -11.56 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.40 Top reward, 10.55 Total reward\n",
            "342 episode , 266 step , -11.60 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.54 Top reward, 74.26 Total reward\n",
            "343 episode , 98 step , -11.64 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.44 Top reward, 19.05 Total reward\n",
            "344 episode , 105 step , -11.72 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.52 Top reward, 21.02 Total reward\n",
            "345 episode , 324 step , -11.69 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.51 Top reward, 89.97 Total reward\n",
            "346 episode , 150 step , -11.72 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.56 Top reward, 35.27 Total reward\n",
            "347 episode , 277 step , -11.78 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.52 Top reward, 64.74 Total reward\n",
            "348 episode , 353 step , -11.93 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.52 Top reward, 91.89 Total reward\n",
            "349 episode , 88 step , -11.91 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.49 Top reward, 15.01 Total reward\n",
            "350 episode , 173 step , -12.01 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.48 Top reward, 37.83 Total reward\n",
            "351 episode , 1003 step , -12.09 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.56 Top reward, 306.93 Total reward\n",
            "352 episode , 638 step , -12.37 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.56 Top reward, 166.87 Total reward\n",
            "353 episode , 475 step , -12.45 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.52 Top reward, 132.98 Total reward\n",
            "354 episode , 45 step , -12.48 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , -0.07 Top reward, -17.21 Total reward\n",
            "355 episode , 420 step , -12.58 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.54 Top reward, 114.29 Total reward\n",
            "356 episode , 527 step , -12.71 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.50 Top reward, 142.78 Total reward\n",
            "357 episode , 605 step , -12.80 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.52 Top reward, 178.34 Total reward\n",
            "358 episode , 253 step , -12.88 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.58 Top reward, 63.55 Total reward\n",
            "359 episode , 271 step , -13.03 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.61 Top reward, 74.35 Total reward\n",
            "360 episode , 440 step , -13.28 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.50 Top reward, 119.16 Total reward\n",
            "361 episode , 172 step , -13.37 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.55 Top reward, 36.01 Total reward\n",
            "362 episode , 221 step , -13.39 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.54 Top reward, 61.98 Total reward\n",
            "363 episode , 77 step , -13.37 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.43 Top reward, 2.21 Total reward\n",
            "364 episode , 206 step , -13.33 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.52 Top reward, 54.01 Total reward\n",
            "365 episode , 295 step , -13.37 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.51 Top reward, 83.73 Total reward\n",
            "366 episode , 326 step , -13.48 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.55 Top reward, 98.22 Total reward\n",
            "367 episode , 1058 step , -13.65 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.60 Top reward, 300.50 Total reward\n",
            "368 episode , 80 step , -13.84 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.48 Top reward, 8.07 Total reward\n",
            "369 episode , 82 step , -13.77 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.36 Top reward, 2.83 Total reward\n",
            "370 episode , 257 step , -13.71 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.57 Top reward, 73.44 Total reward\n",
            "371 episode , 164 step , -13.72 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.52 Top reward, 45.62 Total reward\n",
            "372 episode , 86 step , -13.77 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.46 Top reward, 8.69 Total reward\n",
            "373 episode , 98 step , -13.73 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.49 Total reward\n",
            "374 episode , 160 step , -13.66 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.51 Top reward, 33.67 Total reward\n",
            "375 episode , 204 step , -13.68 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.55 Top reward, 53.91 Total reward\n",
            "376 episode , 208 step , -13.82 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.57 Top reward, 57.40 Total reward\n",
            "377 episode , 570 step , -13.92 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.48 Top reward, 162.20 Total reward\n",
            "378 episode , 989 step , -14.17 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.55 Top reward, 305.16 Total reward\n",
            "379 episode , 515 step , -14.57 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.54 Top reward, 141.42 Total reward\n",
            "380 episode , 369 step , -14.56 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.55 Top reward, 105.54 Total reward\n",
            "381 episode , 348 step , -14.60 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.51 Top reward, 96.33 Total reward\n",
            "382 episode , 313 step , -14.63 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.53 Top reward, 90.38 Total reward\n",
            "383 episode , 508 step , -14.63 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.53 Top reward, 152.73 Total reward\n",
            "384 episode , 413 step , -14.72 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.54 Top reward, 132.92 Total reward\n",
            "385 episode , 419 step , -14.85 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.55 Top reward, 133.48 Total reward\n",
            "386 episode , 288 step , -14.94 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.54 Top reward, 93.75 Total reward\n",
            "387 episode , 193 step , -15.04 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.58 Top reward, 56.88 Total reward\n",
            "388 episode , 362 step , -15.04 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.59 Top reward, 109.47 Total reward\n",
            "389 episode , 258 step , -15.14 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.57 Top reward, 78.95 Total reward\n",
            "390 episode , 475 step , -15.15 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.54 Top reward, 136.51 Total reward\n",
            "391 episode , 982 step , -15.40 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.63 Top reward, 301.18 Total reward\n",
            "392 episode , 280 step , -15.70 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.58 Top reward, 91.55 Total reward\n",
            "393 episode , 626 step , -15.84 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.59 Top reward, 216.92 Total reward\n",
            "394 episode , 226 step , -15.99 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.53 Top reward, 64.47 Total reward\n",
            "395 episode , 691 step , -16.00 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.56 Top reward, 222.89 Total reward\n",
            "396 episode , 775 step , -16.19 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.53 Top reward, 106.00 Total reward\n",
            "397 episode , 321 step , -16.42 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.54 Top reward, 96.97 Total reward\n",
            "398 episode , 135 step , -16.50 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.50 Top reward, 31.47 Total reward\n",
            "399 episode , 592 step , -16.56 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.59 Top reward, 208.65 Total reward\n",
            "400 episode , 190 step , -16.59 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.54 Top reward, 53.78 Total reward\n",
            "401 episode , 489 step , -16.65 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.59 Top reward, 155.20 Total reward\n",
            "402 episode , 154 step , -16.70 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.57 Top reward, 43.71 Total reward\n",
            "403 episode , 110 step , -16.65 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.48 Top reward, 24.53 Total reward\n",
            "404 episode , 84 step , -16.66 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.01 Top reward, -15.56 Total reward\n",
            "405 episode , 235 step , -16.61 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.61 Top reward, 71.59 Total reward\n",
            "406 episode , 276 step , -16.54 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.56 Top reward, 84.57 Total reward\n",
            "407 episode , 801 step , -16.72 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.56 Top reward, 250.94 Total reward\n",
            "408 episode , 220 step , -16.90 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.52 Top reward, 63.35 Total reward\n",
            "409 episode , 532 step , -16.92 Actor Loss, 0.61 Critic Loss,  0.01 Threshold , 0.58 Top reward, 136.79 Total reward\n",
            "410 episode , 456 step , -16.98 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.52 Top reward, 146.95 Total reward\n",
            "411 episode , 110 step , -17.01 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.50 Top reward, 21.96 Total reward\n",
            "412 episode , 735 step , -17.07 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.59 Top reward, 223.61 Total reward\n",
            "413 episode , 1036 step , -17.35 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.61 Top reward, 302.85 Total reward\n",
            "414 episode , 520 step , -17.55 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.54 Top reward, 168.90 Total reward\n",
            "415 episode , 108 step , -17.64 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.45 Top reward, 22.31 Total reward\n",
            "416 episode , 264 step , -17.56 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.57 Top reward, 72.03 Total reward\n",
            "417 episode , 392 step , -17.62 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.54 Top reward, 126.84 Total reward\n",
            "418 episode , 175 step , -17.63 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.51 Top reward, 43.11 Total reward\n",
            "419 episode , 93 step , -17.53 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.43 Top reward, 12.39 Total reward\n",
            "420 episode , 169 step , -17.56 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.52 Top reward, 44.78 Total reward\n",
            "421 episode , 79 step , -17.47 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.29 Top reward, 8.42 Total reward\n",
            "422 episode , 73 step , -17.38 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.21 Top reward, 2.87 Total reward\n",
            "423 episode , 467 step , -17.53 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.57 Top reward, 159.87 Total reward\n",
            "424 episode , 459 step , -17.53 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.53 Top reward, 142.41 Total reward\n",
            "425 episode , 363 step , -17.60 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.54 Top reward, 115.60 Total reward\n",
            "426 episode , 930 step , -17.78 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.56 Top reward, 308.51 Total reward\n",
            "427 episode , 79 step , -17.95 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.02 Top reward, -17.34 Total reward\n",
            "428 episode , 989 step , -18.03 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.61 Top reward, 305.69 Total reward\n",
            "429 episode , 930 step , -18.36 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.58 Top reward, 307.77 Total reward\n",
            "430 episode , 917 step , -18.70 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.57 Top reward, 309.73 Total reward\n",
            "431 episode , 886 step , -19.04 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.60 Top reward, 310.62 Total reward\n",
            "432 episode , 84 step , -19.16 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.33 Top reward, 6.94 Total reward\n",
            "433 episode , 601 step , -19.27 Actor Loss, 0.60 Critic Loss,  0.01 Threshold , 0.56 Top reward, 188.70 Total reward\n",
            "434 episode , 132 step , -19.29 Actor Loss, 0.58 Critic Loss,  0.01 Threshold , 0.50 Top reward, 31.46 Total reward\n",
            "435 episode , 168 step , -19.23 Actor Loss, 0.59 Critic Loss,  0.01 Threshold , 0.51 Top reward, 44.97 Total reward\n",
            "436 episode , 258 step , -19.24 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.49 Top reward, 66.72 Total reward\n",
            "437 episode , 973 step , -19.26 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.54 Top reward, 305.13 Total reward\n",
            "438 episode , 1109 step , -19.50 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.60 Top reward, 301.61 Total reward\n",
            "439 episode , 492 step , -19.71 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.59 Top reward, 148.06 Total reward\n",
            "440 episode , 332 step , -19.96 Actor Loss, 0.56 Critic Loss,  0.01 Threshold , 0.56 Top reward, 102.65 Total reward\n",
            "441 episode , 91 step , -19.92 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.28 Top reward, 4.04 Total reward\n",
            "442 episode , 78 step , -19.84 Actor Loss, 0.55 Critic Loss,  0.01 Threshold , 0.06 Top reward, -16.19 Total reward\n",
            "443 episode , 78 step , -19.83 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.33 Top reward, 2.22 Total reward\n",
            "444 episode , 222 step , -19.79 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.60 Top reward, 56.79 Total reward\n",
            "445 episode , 221 step , -19.82 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.53 Top reward, 59.42 Total reward\n",
            "446 episode , 84 step , -19.82 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.34 Top reward, 4.87 Total reward\n",
            "447 episode , 67 step , -19.74 Actor Loss, 0.57 Critic Loss,  0.01 Threshold , 0.32 Top reward, 3.38 Total reward\n",
            "448 episode , 122 step , -19.69 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.47 Top reward, 20.88 Total reward\n",
            "449 episode , 176 step , -19.62 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.33 Top reward, 14.15 Total reward\n",
            "450 episode , 131 step , -19.55 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.36 Top reward, 8.27 Total reward\n",
            "451 episode , 73 step , -19.52 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.36 Top reward, 9.47 Total reward\n",
            "452 episode , 101 step , -19.50 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.37 Top reward, 14.76 Total reward\n",
            "453 episode , 144 step , -19.46 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.50 Top reward, 26.64 Total reward\n",
            "454 episode , 75 step , -19.40 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.34 Top reward, 6.66 Total reward\n",
            "455 episode , 110 step , -19.39 Actor Loss, 0.54 Critic Loss,  0.01 Threshold , 0.41 Top reward, 18.95 Total reward\n",
            "456 episode , 881 step , -19.45 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.61 Top reward, 312.23 Total reward\n",
            "457 episode , 429 step , -19.56 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.59 Top reward, 130.40 Total reward\n",
            "458 episode , 124 step , -19.47 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , 0.47 Top reward, 27.87 Total reward\n",
            "459 episode , 726 step , -19.54 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.55 Top reward, 223.81 Total reward\n",
            "460 episode , 686 step , -19.59 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.56 Top reward, 223.82 Total reward\n",
            "461 episode , 263 step , -19.56 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.53 Top reward, 79.22 Total reward\n",
            "462 episode , 88 step , -19.53 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.39 Top reward, 9.62 Total reward\n",
            "463 episode , 987 step , -19.49 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.59 Top reward, 305.62 Total reward\n",
            "464 episode , 497 step , -19.58 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.53 Top reward, 157.81 Total reward\n",
            "465 episode , 994 step , -19.82 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.63 Top reward, 306.46 Total reward\n",
            "466 episode , 913 step , -20.03 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.58 Top reward, 311.72 Total reward\n",
            "467 episode , 89 step , -20.07 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.34 Top reward, 0.73 Total reward\n",
            "468 episode , 508 step , -20.02 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.57 Top reward, 144.44 Total reward\n",
            "469 episode , 62 step , -19.98 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.15 Top reward, -8.69 Total reward\n",
            "470 episode , 71 step , -20.01 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.31 Top reward, -12.01 Total reward\n",
            "471 episode , 43 step , -20.00 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , -0.06 Top reward, -13.56 Total reward\n",
            "472 episode , 54 step , -19.99 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.17 Top reward, -7.32 Total reward\n",
            "473 episode , 69 step , -19.86 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.15 Top reward, -7.34 Total reward\n",
            "474 episode , 51 step , -19.84 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , -0.08 Top reward, -12.82 Total reward\n",
            "475 episode , 44 step , -19.77 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , -0.07 Top reward, -12.58 Total reward\n",
            "476 episode , 39 step , -19.68 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , -0.03 Top reward, -14.54 Total reward\n",
            "477 episode , 996 step , -19.77 Actor Loss, 0.47 Critic Loss,  0.01 Threshold , 0.62 Top reward, 303.25 Total reward\n",
            "478 episode , 102 step , -20.01 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.42 Top reward, 7.27 Total reward\n",
            "479 episode , 49 step , -20.08 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , -0.06 Top reward, -13.31 Total reward\n",
            "480 episode , 102 step , -19.95 Actor Loss, 0.49 Critic Loss,  0.01 Threshold , 0.16 Top reward, -11.57 Total reward\n",
            "481 episode , 915 step , -20.34 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.61 Top reward, 308.58 Total reward\n",
            "482 episode , 39 step , -20.64 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , -0.06 Top reward, -7.55 Total reward\n",
            "483 episode , 41 step , -20.59 Actor Loss, 0.53 Critic Loss,  0.01 Threshold , -0.04 Top reward, -7.04 Total reward\n",
            "484 episode , 421 step , -20.77 Actor Loss, 0.52 Critic Loss,  0.01 Threshold , 0.65 Top reward, 142.76 Total reward\n",
            "485 episode , 45 step , -20.73 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , -0.00 Top reward, -5.71 Total reward\n",
            "486 episode , 282 step , -20.73 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.62 Top reward, 79.67 Total reward\n",
            "487 episode , 62 step , -20.61 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , -0.05 Top reward, -12.68 Total reward\n",
            "488 episode , 397 step , -20.56 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.57 Top reward, 116.25 Total reward\n",
            "489 episode , 738 step , -20.66 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.67 Top reward, 242.14 Total reward\n",
            "490 episode , 276 step , -20.73 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.55 Top reward, 57.18 Total reward\n",
            "491 episode , 339 step , -20.75 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.67 Top reward, 102.83 Total reward\n",
            "492 episode , 188 step , -20.72 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.55 Top reward, 43.06 Total reward\n",
            "493 episode , 351 step , -20.67 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.57 Top reward, 89.14 Total reward\n",
            "494 episode , 257 step , -20.78 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.37 Top reward, 31.95 Total reward\n",
            "495 episode , 183 step , -20.77 Actor Loss, 0.51 Critic Loss,  0.01 Threshold , 0.35 Top reward, 13.64 Total reward\n",
            "496 episode , 251 step , -20.70 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.41 Top reward, 30.80 Total reward\n",
            "497 episode , 255 step , -20.60 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.47 Top reward, 56.91 Total reward\n",
            "498 episode , 153 step , -20.56 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.51 Top reward, 28.82 Total reward\n",
            "499 episode , 219 step , -20.52 Actor Loss, 0.50 Critic Loss,  0.01 Threshold , 0.54 Top reward, 46.58 Total reward\n",
            "500 episode , 488 step , -20.37 Actor Loss, 0.48 Critic Loss,  0.01 Threshold , 0.54 Top reward, 73.83 Total reward\n",
            "501 episode , 1600 step , -19.96 Actor Loss, 0.46 Critic Loss,  0.01 Threshold , 0.12 Top reward, -90.86 Total reward\n",
            "502 episode , 959 step , -19.31 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.44 Top reward, 61.26 Total reward\n",
            "503 episode , 257 step , -19.40 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.50 Top reward, 25.30 Total reward\n",
            "504 episode , 198 step , -19.52 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.43 Top reward, 27.55 Total reward\n",
            "505 episode , 237 step , -19.42 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.51 Top reward, 49.07 Total reward\n",
            "506 episode , 150 step , -19.26 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.46 Top reward, 28.38 Total reward\n",
            "507 episode , 185 step , -19.24 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.55 Top reward, 42.70 Total reward\n",
            "508 episode , 205 step , -19.05 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.54 Top reward, 43.94 Total reward\n",
            "509 episode , 182 step , -19.04 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.51 Top reward, 38.55 Total reward\n",
            "510 episode , 234 step , -19.17 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.53 Top reward, 54.02 Total reward\n",
            "511 episode , 195 step , -19.14 Actor Loss, 0.37 Critic Loss,  0.01 Threshold , 0.50 Top reward, 47.24 Total reward\n",
            "512 episode , 251 step , -19.14 Actor Loss, 0.37 Critic Loss,  0.01 Threshold , 0.50 Top reward, 67.80 Total reward\n",
            "513 episode , 367 step , -19.08 Actor Loss, 0.37 Critic Loss,  0.01 Threshold , 0.56 Top reward, 87.38 Total reward\n",
            "514 episode , 209 step , -19.13 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.51 Top reward, 55.82 Total reward\n",
            "515 episode , 172 step , -19.17 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.57 Top reward, 43.87 Total reward\n",
            "516 episode , 238 step , -19.10 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.56 Top reward, 63.62 Total reward\n",
            "517 episode , 185 step , -19.13 Actor Loss, 0.34 Critic Loss,  0.01 Threshold , 0.59 Top reward, 49.31 Total reward\n",
            "518 episode , 129 step , -19.25 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.50 Top reward, 27.28 Total reward\n",
            "519 episode , 451 step , -19.28 Actor Loss, 0.34 Critic Loss,  0.01 Threshold , 0.55 Top reward, 143.90 Total reward\n",
            "520 episode , 636 step , -19.30 Actor Loss, 0.33 Critic Loss,  0.01 Threshold , 0.59 Top reward, 228.23 Total reward\n",
            "521 episode , 575 step , -19.37 Actor Loss, 0.32 Critic Loss,  0.01 Threshold , 0.56 Top reward, 194.81 Total reward\n",
            "522 episode , 238 step , -19.42 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.52 Top reward, 67.65 Total reward\n",
            "523 episode , 592 step , -19.62 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.55 Top reward, 150.08 Total reward\n",
            "524 episode , 282 step , -19.72 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.45 Top reward, 38.09 Total reward\n",
            "525 episode , 538 step , -19.90 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.51 Top reward, 145.29 Total reward\n",
            "526 episode , 418 step , -20.13 Actor Loss, 0.30 Critic Loss,  0.01 Threshold , 0.57 Top reward, 125.34 Total reward\n",
            "527 episode , 371 step , -20.32 Actor Loss, 0.31 Critic Loss,  0.01 Threshold , 0.57 Top reward, 94.43 Total reward\n",
            "528 episode , 759 step , -20.57 Actor Loss, 0.32 Critic Loss,  0.01 Threshold , 0.55 Top reward, 209.31 Total reward\n",
            "529 episode , 531 step , -20.89 Actor Loss, 0.34 Critic Loss,  0.01 Threshold , 0.56 Top reward, 175.00 Total reward\n",
            "530 episode , 338 step , -21.07 Actor Loss, 0.35 Critic Loss,  0.01 Threshold , 0.57 Top reward, 110.50 Total reward\n",
            "531 episode , 859 step , -21.42 Actor Loss, 0.38 Critic Loss,  0.01 Threshold , 0.67 Top reward, 316.30 Total reward\n",
            "532 episode , 1145 step , -22.07 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.51 Top reward, 300.99 Total reward\n",
            "533 episode , 392 step , -22.40 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.52 Top reward, 94.13 Total reward\n",
            "534 episode , 582 step , -22.55 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.53 Top reward, 197.05 Total reward\n",
            "535 episode , 455 step , -22.70 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.62 Top reward, 168.36 Total reward\n",
            "536 episode , 210 step , -22.84 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.60 Top reward, 66.40 Total reward\n",
            "537 episode , 290 step , -22.91 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.58 Top reward, 97.95 Total reward\n",
            "538 episode , 469 step , -23.09 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.59 Top reward, 175.44 Total reward\n",
            "539 episode , 585 step , -23.26 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.60 Top reward, 222.96 Total reward\n",
            "540 episode , 351 step , -23.28 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.58 Top reward, 127.33 Total reward\n",
            "541 episode , 170 step , -23.37 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.59 Top reward, 53.50 Total reward\n",
            "542 episode , 121 step , -23.36 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.54 Top reward, 32.64 Total reward\n",
            "543 episode , 90 step , -23.24 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.45 Top reward, 16.60 Total reward\n",
            "544 episode , 173 step , -23.15 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.51 Top reward, 42.18 Total reward\n",
            "545 episode , 271 step , -23.14 Actor Loss, 0.39 Critic Loss,  0.01 Threshold , 0.55 Top reward, 76.91 Total reward\n",
            "546 episode , 220 step , -23.24 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.56 Top reward, 55.25 Total reward\n",
            "547 episode , 469 step , -23.18 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.54 Top reward, 149.49 Total reward\n",
            "548 episode , 244 step , -23.35 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.56 Top reward, 66.99 Total reward\n",
            "549 episode , 154 step , -23.34 Actor Loss, 0.40 Critic Loss,  0.01 Threshold , 0.46 Top reward, 27.15 Total reward\n",
            "550 episode , 202 step , -23.37 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.51 Top reward, 53.21 Total reward\n",
            "551 episode , 277 step , -23.34 Actor Loss, 0.41 Critic Loss,  0.01 Threshold , 0.56 Top reward, 89.98 Total reward\n",
            "552 episode , 203 step , -23.36 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.59 Top reward, 59.70 Total reward\n",
            "553 episode , 282 step , -23.39 Actor Loss, 0.42 Critic Loss,  0.01 Threshold , 0.55 Top reward, 87.08 Total reward\n",
            "554 episode , 506 step , -23.53 Actor Loss, 0.43 Critic Loss,  0.01 Threshold , 0.57 Top reward, 190.82 Total reward\n",
            "555 episode , 363 step , -23.62 Actor Loss, 0.44 Critic Loss,  0.01 Threshold , 0.59 Top reward, 119.82 Total reward\n",
            "556 episode , 440 step , -23.69 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.61 Top reward, 162.75 Total reward\n",
            "557 episode , 344 step , -23.74 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.60 Top reward, 104.47 Total reward\n",
            "558 episode , 212 step , -23.80 Actor Loss, 0.45 Critic Loss,  0.01 Threshold , 0.62 Top reward, 59.43 Total reward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lg-4834irdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}