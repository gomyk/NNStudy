{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLH2cGLmXj4P5NSTf/T5xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomyk/NNStudy/blob/moonwon/%5BRL%5D%5BDDPG%5D%5BMW%5D%20BipedalWalker-v2%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO9p_LliP05R",
        "colab_type": "text"
      },
      "source": [
        "#Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9duZfSLhJ8X",
        "colab_type": "code",
        "outputId": "cac53623-d8e9-4eb1-fe78-efd7c06c1b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils\n",
        "!pip install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 66%\r\rReading package lists... 66%\r\rReading package lists... 71%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DE8ejMqcTWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "display = Display(visible=0, size=(400,600),)\n",
        "display.start()\n",
        "env = gym.make(\"BipedalWalker-v2\")\n",
        "\n",
        "#env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), \"video\", force=True, video_callable=lambda c:c%100 ==0)\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3exp-qAP7jv",
        "colab_type": "text"
      },
      "source": [
        "##Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmc6Jfr2d8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, action_dim:int , obs_dim: int, size: int, batch_size: int):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size, action_dim], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrDEGlnQAeH",
        "colab_type": "text"
      },
      "source": [
        "##Define Noise Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j021icUCet_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAZSWC2QGDx",
        "colab_type": "text"
      },
      "source": [
        "##Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1hagvrqKTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, outputs, init_w: float = 3e-3,):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, outputs)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.linear(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x).tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5GwnzbQJ4o",
        "colab_type": "text"
      },
      "source": [
        "##Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_lqf372OXYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, action_size, init_w: float = 3e-3,):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear = nn.Linear(obs_size + action_size, HIDDEN_SIZE)\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.linear3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "        self.head = nn.Linear(HIDDEN_SIZE, 1)\n",
        "\n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.linear(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxzbD_-QPWZ",
        "colab_type": "text"
      },
      "source": [
        "###Environment Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVgLn9rKgS79",
        "colab_type": "code",
        "outputId": "226e2c52-7dbc-4815-8b1e-b02572138a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0DaDLxQVkZ",
        "colab_type": "text"
      },
      "source": [
        "##Prepare to learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y5GkgVljenU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 2000\n",
        "TARGET_UPDATE = 10\n",
        "ACTOR_LR = 0.001\n",
        "CRITIC_LR = 0.001\n",
        "MEMORY_SIZE = 10000\n",
        "EPISODE_SIZE = 1000\n",
        "TAU = 0.005\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "\n",
        "RECORD_INTERVAL = 100\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "#n_actions = env.action_space.n\n",
        "n_actions = 4\n",
        "n_obvs = 24\n",
        "\n",
        "actor = Actor(n_obvs, n_actions).to(device)\n",
        "actor.eval()\n",
        "actor_target = Actor(n_obvs, n_actions).to(device)\n",
        "actor_target.load_state_dict(actor.state_dict())\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(n_obvs, n_actions).to(device)\n",
        "critic.eval()\n",
        "critic_target = Critic(n_obvs, n_actions).to(device)\n",
        "critic_target.load_state_dict(critic.state_dict())\n",
        "critic_target.eval()\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "memory = ReplayMemory(n_actions,n_obvs,MEMORY_SIZE,BATCH_SIZE)\n",
        "\n",
        "noise = OUNoise(\n",
        "            n_actions,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    if sample < eps_threshold:\n",
        "            #selected_action = [np.random.uniform(0,1),np.random.uniform(0,1),np.random.uniform(0,1)]\n",
        "            selected_action = [np.random.uniform(-1,1),\n",
        "                               np.random.uniform(-1,1),\n",
        "                               np.random.uniform(-1,1),\n",
        "                               np.random.uniform(-1,1)]\n",
        "    else:\n",
        "        selected_action = actor(\n",
        "             torch.FloatTensor(state).to(device)\n",
        "         ).detach().cpu().numpy()\n",
        "    _noise = noise.sample()\n",
        "    for action in selected_action:\n",
        "      action = np.clip(action + _noise, -1.0, 1.0)\n",
        "    return selected_action\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_xKtOnUR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_soft_update():\n",
        "        #Soft-update: target = tau*local + (1-tau)*target\n",
        "        tau = TAU\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            actor_target.parameters(), actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            critic_target.parameters(), critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7qP2ZuQf-s",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Vbb4tzjnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return -1 , -1\n",
        "    samples = memory.sample_batch()\n",
        "    state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "    next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "    action = torch.FloatTensor(samples[\"acts\"]).to(device)\n",
        "    reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "    \n",
        "    masks = 1 - done\n",
        "    next_action = actor_target(next_state)\n",
        "    next_value = critic_target(next_state, next_action)\n",
        "    curr_return = reward + GAMMA * next_value * masks\n",
        "\n",
        "    # train critic\n",
        "    values = critic(state, action)\n",
        "    critic_loss = F.smooth_l1_loss(values, curr_return)\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()     \n",
        "    # train actor\n",
        "    loss = critic(state, actor(state))\n",
        "    actor_loss = -loss.mean()\n",
        "        \n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "        \n",
        "    # target update\n",
        "    target_soft_update()\n",
        "\n",
        "    return actor_loss.data, critic_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UN3NpFQiLJ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2dnWNDjp7y",
        "colab_type": "code",
        "outputId": "76cc5f21-8aa9-4db5-f92c-262c032277ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "frames = []\n",
        "for i_episode in range(EPISODE_SIZE):\n",
        "    # 환경과 상태 초기화\n",
        "    obv = env.reset()\n",
        "    total_actor_loss = 0\n",
        "    total_critic_loss = 0\n",
        "    total_reward = 0\n",
        "    global steps_done\n",
        "    top_reward = -1\n",
        "    total_action_count = [0,0,0]\n",
        "    for t in count():\n",
        "        if i_episode % RECORD_INTERVAL == 0:\n",
        "          frames.append(env.render(mode=\"rgb_array\"))\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(obv)\n",
        "        next_obv, reward, done, _ = env.step(action)\n",
        "\n",
        "        if reward > top_reward:\n",
        "          top_reward = reward\n",
        "        total_reward += reward\n",
        "        # 메모리에 변이 저장\n",
        "        assert obv is not None\n",
        "        memory.store(obv, action, reward, next_obv, done)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        obv = next_obv\n",
        "\n",
        "        # 최적화 한단계 수행(목표 네트워크에서)\n",
        "        actor_loss, critic_loss = optimize_model()\n",
        "        total_actor_loss += actor_loss\n",
        "        total_critic_loss += critic_loss\n",
        "        if done:\n",
        "            E = eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "            math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            episode_durations.append(t + 1)\n",
        "            print('%d episode , %d step , %.2f Actor Loss, %.2f Critic Loss,  %.2f Threshold , %.2f Top reward, %.2f Avg reward'\\\n",
        "                  %(i_episode,t+1,total_actor_loss/(t+1), total_critic_loss/(t+1) ,E, top_reward, total_reward/(t+1)))\n",
        "            print(total_action_count)\n",
        "            plot_durations()\n",
        "            total_actor_loss = 0\n",
        "            total_critic_loss = 0\n",
        "            top_reward = 0\n",
        "            total_reward = 0\n",
        "            total_action_count = [0,0,0]\n",
        "            break\n",
        "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        actor_target.load_state_dict(actor.state_dict())\n",
        "        critic_target.load_state_dict(critic.state_dict())\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 episode , 76 step , -1.00 Actor Loss, -1.00 Critic Loss,  0.87 Threshold , 0.08 Top reward, -1.40 Avg reward\n",
            "[0, 0, 0]\n",
            "1 episode , 82 step , -1.00 Actor Loss, -1.00 Critic Loss,  0.84 Threshold , 0.43 Top reward, -1.20 Avg reward\n",
            "[0, 0, 0]\n",
            "2 episode , 1600 step , -0.27 Actor Loss, -0.01 Critic Loss,  0.40 Threshold , 0.22 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "3 episode , 94 step , -0.46 Actor Loss, 0.01 Critic Loss,  0.39 Threshold , 0.28 Top reward, -1.17 Avg reward\n",
            "[0, 0, 0]\n",
            "4 episode , 64 step , -0.50 Actor Loss, 0.05 Critic Loss,  0.38 Threshold , 0.05 Top reward, -1.78 Avg reward\n",
            "[0, 0, 0]\n",
            "5 episode , 58 step , -0.53 Actor Loss, 0.08 Critic Loss,  0.37 Threshold , 0.16 Top reward, -1.81 Avg reward\n",
            "[0, 0, 0]\n",
            "6 episode , 105 step , -0.58 Actor Loss, 0.15 Critic Loss,  0.35 Threshold , 0.32 Top reward, -0.97 Avg reward\n",
            "[0, 0, 0]\n",
            "7 episode , 47 step , -0.65 Actor Loss, 0.19 Critic Loss,  0.34 Threshold , -0.01 Top reward, -2.33 Avg reward\n",
            "[0, 0, 0]\n",
            "8 episode , 38 step , -0.67 Actor Loss, 0.32 Critic Loss,  0.34 Threshold , -0.03 Top reward, -2.86 Avg reward\n",
            "[0, 0, 0]\n",
            "9 episode , 103 step , -0.69 Actor Loss, 0.25 Critic Loss,  0.32 Threshold , 0.21 Top reward, -1.11 Avg reward\n",
            "[0, 0, 0]\n",
            "10 episode , 1600 step , -1.00 Actor Loss, 0.09 Critic Loss,  0.17 Threshold , 0.27 Top reward, -0.05 Avg reward\n",
            "[0, 0, 0]\n",
            "11 episode , 66 step , -1.46 Actor Loss, 0.07 Critic Loss,  0.17 Threshold , 0.31 Top reward, -1.52 Avg reward\n",
            "[0, 0, 0]\n",
            "12 episode , 73 step , -1.54 Actor Loss, 0.06 Critic Loss,  0.16 Threshold , 0.29 Top reward, -1.45 Avg reward\n",
            "[0, 0, 0]\n",
            "13 episode , 140 step , -1.68 Actor Loss, 0.08 Critic Loss,  0.16 Threshold , 0.07 Top reward, -0.94 Avg reward\n",
            "[0, 0, 0]\n",
            "14 episode , 45 step , -1.75 Actor Loss, 0.06 Critic Loss,  0.15 Threshold , 0.20 Top reward, -2.51 Avg reward\n",
            "[0, 0, 0]\n",
            "15 episode , 63 step , -1.81 Actor Loss, 0.14 Critic Loss,  0.15 Threshold , 0.23 Top reward, -1.86 Avg reward\n",
            "[0, 0, 0]\n",
            "16 episode , 62 step , -1.89 Actor Loss, 0.12 Critic Loss,  0.15 Threshold , 0.29 Top reward, -1.83 Avg reward\n",
            "[0, 0, 0]\n",
            "17 episode , 63 step , -1.91 Actor Loss, 0.18 Critic Loss,  0.15 Threshold , 0.11 Top reward, -1.82 Avg reward\n",
            "[0, 0, 0]\n",
            "18 episode , 72 step , -1.88 Actor Loss, 0.12 Critic Loss,  0.14 Threshold , 0.25 Top reward, -1.57 Avg reward\n",
            "[0, 0, 0]\n",
            "19 episode , 57 step , -1.93 Actor Loss, 0.14 Critic Loss,  0.14 Threshold , 0.22 Top reward, -1.82 Avg reward\n",
            "[0, 0, 0]\n",
            "20 episode , 1060 step , -2.50 Actor Loss, 0.10 Critic Loss,  0.10 Threshold , 0.27 Top reward, -0.18 Avg reward\n",
            "[0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWWG7HcMD3j",
        "colab_type": "text"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHCq9xymdUgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install JSAnimation\n",
        "from matplotlib import animation, rc\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2NMRr68tmra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('./*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    print(mp4list)\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaeim4ulL0y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports specifically so we can render outputs in Colab.\n",
        "fig = plt.figure()\n",
        "def display_frames_as_gif(frame):\n",
        "    \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "    patch = plt.imshow(frame[0].astype(int))\n",
        "    def animate(i):\n",
        "        patch.set_data(frame[i].astype(int))\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=30, blit=False\n",
        "    )\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    # Set up formatting for the movie files\n",
        "    display(HTML(data=anim.to_html5_video()))\n",
        "    #FFwriter = animation.FFMpegWriter()\n",
        "    #anim.save('basic_animation.mp4', writer = FFwriter)\n",
        "    #show_video()\n",
        "# display \n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}